{"meta":{"title":"Hareric","subtitle":"柠檬茶🍋 永远的神","description":"人生苦短，我用python","author":"Eric Chen","url":"https://hareric.com"},"posts":[{"title":"json解析工具","slug":"json解析工具","date":"2021-05-29T08:48:30.000Z","updated":"2021-05-29T08:46:48.202Z","comments":true,"path":"2021/05/29/json解析工具/","link":"","permalink":"https://hareric.com/2021/05/29/json%E8%A7%A3%E6%9E%90%E5%B7%A5%E5%85%B7/","excerpt":"","text":"","raw":null,"content":null,"categories":[],"tags":[{"name":"开发工具","slug":"开发工具","permalink":"https://hareric.com/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}]},{"title":"哈希&编码工具","slug":"哈希&编码工具","date":"2021-05-23T03:47:30.000Z","updated":"2021-05-29T08:46:43.065Z","comments":true,"path":"2021/05/23/哈希&编码工具/","link":"","permalink":"https://hareric.com/2021/05/23/%E5%93%88%E5%B8%8C&%E7%BC%96%E7%A0%81%E5%B7%A5%E5%85%B7/","excerpt":"","text":"","raw":null,"content":null,"categories":[],"tags":[{"name":"开发工具","slug":"开发工具","permalink":"https://hareric.com/tags/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/"}]},{"title":"Hive 2.3.4 安裝及配置","slug":"Hive 2.3.4安裝及配置","date":"2019-03-04T12:18:01.000Z","updated":"2021-05-23T13:21:37.192Z","comments":true,"path":"2019/03/04/Hive 2.3.4安裝及配置/","link":"","permalink":"https://hareric.com/2019/03/04/Hive%202.3.4%E5%AE%89%E8%A3%9D%E5%8F%8A%E9%85%8D%E7%BD%AE/","excerpt":"Hive是一个数据仓库基础工具在Hadoop中用来处理结构化数据。它架构在Hadoop之上，总归为大数据，并使得查询和分析方便。并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。","text":"Hive是一个数据仓库基础工具在Hadoop中用来处理结构化数据。它架构在Hadoop之上，总归为大数据，并使得查询和分析方便。并提供简单的sql查询功能，可以将sql语句转换为MapReduce任务进行运行。 前期工作 安装JDK 安装Hadoop 安装MySQL 下载并安装Hive下载Hive下载地址及官网。 选择apache-hive-2.3.4-bin.tar.gz进行下载。 安装将压缩包复制至想要的安装位置，我的做法是复制到跟hadoop安装的同一个目录下。123sudo mv apache-hive-2.1.1-bin.tar.gz /usr/local/Cellar/hadoopcd /usr/local/Cellar/hadoopsudo tar -xzvf apache-hive-2.3.4-bin.tar.gz ##解压 设置系统环境变量编辑bash_profile文件 vi .bash_profile。添加以下内容。123# Hiveexport HIVE_HOME=/usr/local/Cellar/hadoop/apache-hive-2.3.4-binexport PATH=$PATH:$HIVE_HOME/bin使环境变量生效: source .bash_profile 配置文件hive.site.xml进入Hive/conf文件夹 复制示例配置文件并改名为hive.site.xml1cp hive-default.xml.template hive-site.xml在开头添加如下配置。12345678&lt;property&gt; &lt;name&gt;system:java.io.tmpdir&lt;/name&gt; &lt;value&gt;/tmp/hive/java&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;system:user.name&lt;/name&gt; &lt;value&gt;$&#123;user.name&#125;&lt;/value&gt;&lt;/property&gt; 创建及配置HDFS文件夹创建在 Hive 中创建表之前需要创建以下 HDFS 目录并给它们赋相应的权限。 123456hdfs dfs -mkdir -p /user/hive/warehousehdfs dfs -mkdir -p /user/hive/tmphdfs dfs -mkdir -p /user/hive/loghdfs dfs -chmod 777 /user/hive/warehousehdfs dfs -chmod 777 /usr/hive/tmphdfs dfs -chmod 777 /usr/hive/log 创建完成后可以使用hdfs dfs -ls /hive查看是否已经成功新建文件夹。 配置打开刚刚准备好的hive-site.xml配置文件，把刚刚新建的HDFS文件夹的路径添加到配置文件中去。123456789101112131415&lt;property&gt; &lt;name&gt;hive.exec.scratchdir&lt;/name&gt; &lt;value&gt;hive/tmp&lt;/value&gt; &lt;description&gt;HDFS root scratch dir for Hive jobs which gets created with write all (733) permission. For each connecting user, an HDFS scratch dir: $&#123;hive.exec.scratchdir&#125;/&amp;lt;username&amp;gt; is created, with $&#123;hive.scratch.dir.permission&#125;.&lt;/description&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hive.metastore.warehouse.dir&lt;/name&gt; &lt;value&gt;/hive/warehouse&lt;/value&gt; &lt;description&gt;location of default database for the warehouse&lt;/description&gt; &lt;/property&gt;&lt;property&gt; &lt;name&gt;hive.querylog.location&lt;/name&gt; &lt;value&gt;/hive/log&lt;/value&gt; &lt;description&gt;Location of Hive run time structured log file&lt;/description&gt; &lt;/property&gt; 创建及配置Mysql创建Hive数据库假定你已经安装好MySQL。下面需要创建一个 hive 数据库用来存储 Hive 元数据 打开Mysql服务，并以root用户登入。12mysql.server startmysql -u root -p # 回车后输入密码 若无密码则使用 mysql -u root 创建用户名进入mysql后，创建一个 hive 数据库用来存储 Hive 元数据，且数据库访问的用户名和密码都为hadoop。1234567CREATE DATABASE hive; USE hive; CREATE USER &#x27;hadoop&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;hadoop&#x27;;GRANT ALL ON hive.* TO &#x27;hive&#x27;@&#x27;localhost&#x27; IDENTIFIED BY &#x27;hadoop&#x27;; GRANT ALL ON hive.* TO &#x27;hive&#x27;@&#x27;%&#x27; IDENTIFIED BY &#x27;hadoop&#x27;; FLUSH PRIVILEGES; quit; 配置将刚刚创建的数据库及用户名和密码写入配置文件。12345678910111213141516&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt; &lt;value&gt;jdbc:mysql://localhost:3306/hive?createDatabaseIfNotExist=true&amp;amp;characterEncoding=UTF-8&amp;amp;useSSL=false&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt; &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt; &lt;value&gt;hadoop&lt;/value&gt;&lt;/property&gt;&lt;property&gt; &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt; &lt;value&gt;hadoop&lt;/value&gt;&lt;/property&gt; 配置JDBC由于需要使用Java连接Mysql数据库，所以需要配置MySQL Connector。我这里使用的是mysql-connector-java-5.1.39-bin版本下载地址。并将它复制进 $HIVE_HOME/lib路径下 启动及测试启动HDFS使用Hive之前，请先确保HDFS已经启动。可以使用start-dfs.sh脚本来启动 HDFS。 Mysql初始化从 Hive 2.1 版本开始, 我们需要先运行 schematool 命令来执行初始化操作。1schematool -dbType mysql -initSchema运行后终端将会显示如下信息 使用Hive CLI要使用 Hive CLI（Hive command line interface）, 可以在终端输入Hive，便可以进入。启动信息如下：创建一个table1CREATE TABLE pokes (foo INT, bar STRING);show tables和desc pokes 显示信息如下在HDFS中显示如下","raw":null,"content":null,"categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://hareric.com/tags/Hadoop/"}]},{"title":"使用Pig对数据进行处理和分析","slug":"使用Pig对数据进行处理和分析","date":"2019-02-25T08:54:09.000Z","updated":"2021-05-23T14:08:57.122Z","comments":true,"path":"2019/02/25/使用Pig对数据进行处理和分析/","link":"","permalink":"https://hareric.com/2019/02/25/%E4%BD%BF%E7%94%A8Pig%E5%AF%B9%E6%95%B0%E6%8D%AE%E8%BF%9B%E8%A1%8C%E5%A4%84%E7%90%86%E5%92%8C%E5%88%86%E6%9E%90/","excerpt":"本文将针对一份数据提出多个数据处理的相关问题，并介绍如何使用Apache Pig来解决相关的问题，建议读者根据具体问题具体实践后再查看我分享的解题Pig代码。","text":"本文将针对一份数据提出多个数据处理的相关问题，并介绍如何使用Apache Pig来解决相关的问题，建议读者根据具体问题具体实践后再查看我分享的解题Pig代码。 数据数据下载本文用到的数据是1999年至2004年的The Daily Show嘉宾的历史数据。下载地址 数据描述YEAR – The year the episode aired. GoogleKnowlege_Occupation -Their occupation or office, according to Google’s Knowledge Graph. On the other hand, if they are not in there, how Stewart introduced them on the program. Show – Air date of the episode. Not unique, as some shows had more than one guest Group – A larger group designation for the occupation. For instance, U.S senators, U.S presidents, and former presidents are all under “politicians” Raw_Guest_List – The person or list of people who appeared on the show, according to Wikipedia. The GoogleKnowlege_Occupation only refers to one of them in a given row. 问题问题1在指定的时间范围内，根据GoogleKnowlege_Occupation的不同，找出最多的5种类型。 pig代码123456789101112A = load &#x27;dialy_show_guests&#x27; using PigStorage(&#x27;,&#x27;) AS (year:chararray,occupation:chararray,date:chararray, groupName:chararray,gusetlist:chararray);C = foreach A generate occupation,ToDate(date,&#x27;MM/dd/yy&#x27;) as date;D = filter C by ((date&gt; ToDate(&#x27;1/11/99&#x27;,&#x27;MM/dd/yy&#x27;)) AND (date&lt;ToDate(&#x27;6/11/99&#x27;,&#x27;MM/dd/yy&#x27;)));E = group D by occupation;F = foreach E generate group, COUNT(D) as cnt;describe F;G = order F by cnt desc;H = limit G 5;DUMP H; 输出结果 (actor,28)(actress,20)(comedian,4)(television actress,3)(singer,2) 问题2根据Group,找出每年参加该show的politician的人数。 pig 代码123456789A = load &#x27;dialy_show_guests&#x27; using PigStorage(&#x27;,&#x27;) AS (year:chararray,occupation:chararray,date:chararray, groupName:chararray,gusetlist:chararray);B = FILTER A by groupName==&#x27;Politician&#x27;;C = foreach B generate ToDate(date,&#x27;MM/dd/yy&#x27;) as date;D = FOREACH C GENERATE GetYear(date) as y;E = GROUP D by y;F = FOREACH E GENERATE group, COUNT(D) as cnt;DUMP F; 在做这题的时候，没有发现数据已经提供了year数据域，于是我使用date数据域并提取了其中的年份，有些多此一举。 输出结果 (1999,2)(2000,13)(2001,3)(2002,8)(2003,14)(2004,32)(2005,22)(2006,25)(2007,21)(2008,27)… 问题3在数据描述中，Group数据类型与GoogleKnowledge occupation数据类型，是包含关系，本题是要统计每个group中有多少种不同的GoogleKnowledge occupation。 pig 代码12345678A = load &#x27;dialy_show_guests&#x27; using PigStorage(&#x27;,&#x27;) AS (year:chararray,occupation:chararray,date:chararray, groupName:chararray,gusetlist:chararray);B = FOREACH A GENERATE occupation, groupName;C = GROUP B by groupName;D = FOREACH C &#123;o = DISTINCT B.occupation; GENERATE group, COUNT(o) as cnt;&#125;;E = ORDER D BY cnt DESC ; DUMP E; 输出结果 (Politician,105)(Media,80)(Athletics,29)(Government,27)(Musician,25)(Academic,24)(Misc,16)… 问题4找出各个Group与GoogleKnowledge occupation组合类后的参加show的人数。 pig 代码1234567A = load &#x27;dialy_show_guests&#x27; using PigStorage(&#x27;,&#x27;) AS (year:chararray,occupation:chararray,date:chararray, groupName:chararray,gusetlist:chararray);B = GROUP A BY (groupName, occupation);C = FOREACH B GENERATE group, COUNT(A) as cnt;D = ORDER C BY cnt desc;DUMP D; 输出结果 ((Acting,actor),596)((Acting,actress),271)((Media,journalist),180)((Media,author),102)((Media,Journalist),72)((Comedy,comedian),64)((Politician,us senator),50)((Media,Author),48)((Media,television host),39)((Comedy,Comedian),39)((Media,writer),30)…","raw":null,"content":null,"categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://hareric.com/tags/Hadoop/"}]},{"title":"使用Eclipse+Java配置Pig开发环境","slug":"使用Eclipse-Java配置Pig开发环境","date":"2019-02-22T14:54:25.000Z","updated":"2021-05-23T13:21:37.191Z","comments":true,"path":"2019/02/22/使用Eclipse-Java配置Pig开发环境/","link":"","permalink":"https://hareric.com/2019/02/22/%E4%BD%BF%E7%94%A8Eclipse-Java%E9%85%8D%E7%BD%AEPig%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83/","excerpt":"自从开始使用Eclipse+Maven来配置Java的开发环境并体验到了Maven的强大和方便以后。在之后学习Hadoop的过程中，我都尽量避免使用终端配置运行环境。\n本文将介绍如何配置Eclipse在Pig中的开发环境。若还未使用过Maven配置Hadoop开发环境，可以参考我的另一篇博客","text":"自从开始使用Eclipse+Maven来配置Java的开发环境并体验到了Maven的强大和方便以后。在之后学习Hadoop的过程中，我都尽量避免使用终端配置运行环境。 本文将介绍如何配置Eclipse在Pig中的开发环境。若还未使用过Maven配置Hadoop开发环境，可以参考我的另一篇博客 开发环境 系统：MacOS 10.14.1 Hadoop：2.7.0 Java：1.8.0 Eclipse：4.6.2 Maven: 3.3.9 Pig下载地址Apache Pig 的官方下载地址 由于本文介绍的是用Eclipse开发，Maven配置，因此不需要从此网站下载。 确定版本在下载前需要确定版本。 由于不同版本的Pig支持不同的版本的Hadoop，如下图所示。因此在开始配置前，需要确定自己的Hadoop版本并选择合适的Pig版本。 我使用的是Hadoop 2.7.0 只有Pig 0.17.0 可以支持。所以我选择0.17版本。 其余版本介绍 配置jar包使用Java在Eclipse运行Pig代码，需要antlr，pig和pigunit包，因此需要在Maven Project的pom.xml添加以下依赖。 由于许多不可抗力，使用Apache原生的pig版本时一直无法配置成功，因此我选择Cloudera公司的发行版本，所以需要添加cloudera的资源库。 123456789101112131415161718192021222324252627&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;cloudera&lt;/id&gt; &lt;url&gt;https://repository.cloudera.com/artifactory/cloudera-repos/&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;dependencies&gt; &lt;!-- https://mvnrepository.com/artifact/org.antlr/antlr4 --&gt; &lt;dependency&gt; &lt;groupId&gt;org.antlr&lt;/groupId&gt; &lt;artifactId&gt;antlr4&lt;/artifactId&gt; &lt;version&gt;4.7.2&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.pig&lt;/groupId&gt; &lt;artifactId&gt;pig&lt;/artifactId&gt; &lt;version&gt;0.17.0-cdh6.0.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.pig&lt;/groupId&gt; &lt;artifactId&gt;pigunit&lt;/artifactId&gt; &lt;version&gt;0.17.0-cdh6.0.0&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 创建脚本Apache Pig 提供了后缀名为脚本方式来处理数据。脚本是以后缀名为.pig的文本文件进行保存。 以下是一个pig脚本代码，统计单词的数量，将该脚本保存在src文件夹内wordcount.pig。12345A = load &#x27;src/main/resources/pigData/sample.data&#x27;;B = foreach A generate flatten(TOKENIZE((chararray)$0)) as word;C = group B by word;D = foreach C generate COUNT(B), group;dump D;sample.data数据文件12345678Johny, Johny!Yes, PapaEating sugar?No, PapaTelling lies?No, PapaOpen your mouth!Ha! Ha! Ha! 使用Java运行Pig 有两种运行模式： Local 模式和 MapReduce 模式。在Java中，可以通过org.apache.pig.ExecType进行设置 PigServerPigRunner.java1234567891011121314151617181920import java.util.Properties;import org.apache.pig.ExecType;import org.apache.pig.PigServer;public class PigRunner&#123; public static void main(String[] args) throws Exception &#123; // MapReduce 模式 Properties props = new Properties(); props.setProperty(&quot;fs.default.name&quot;, &quot;hdfs://localhost:9000&quot;);// props.setProperty(&quot;mapred.job.tracker&quot;, &quot;http://localhost:50070&quot;); PigServer pigServer = new PigServer(ExecType.MAPREDUCE, props); pigServer.registerScript(&quot;src/main/java/pig/wordcount.pig&quot;); // local 模式 PigServer pigServer1 = new PigServer(ExecType.LOCAL); pigServer1.registerScript(&quot;src/main/java/pig/wordcount.pig&quot;); &#125;&#125; PigTestPigTest提供了对Pig脚本所得的结果进行单元测试。AppTest.java1234567891011121314import org.apache.pig.pigunit.PigTest;import junit.framework.*;public class AppTest extends TestCase&#123; public void testStudentsPigScript() throws Exception &#123; PigTest pigTest = new PigTest(&quot;src/main/java/pig/wordcount.pig&quot;); pigTest.assertOutput(&quot;D&quot;, new String[] &#123; &quot;(2,No)&quot;, &quot;(3,Ha!)&quot;, &quot;(1,Yes)&quot;, &quot;(1,Open)&quot;, &quot;(3,Papa)&quot;, &quot;(1,your)&quot;, &quot;(1,Johny)&quot;, &quot;(1,lies?)&quot;, &quot;(1,Eating)&quot;, &quot;(1,Johny!)&quot;, &quot;(1,mouth!)&quot;, &quot;(1,sugar?)&quot;, &quot;(1,Telling)&quot;&#125;); &#125;&#125; 下图为上述文件在Java Project中的路径。可供读者进行参考","raw":null,"content":null,"categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://hareric.com/tags/Hadoop/"}]},{"title":"MapReduce实践 Uber数据分析","slug":"MapReduce实践 Uber数据分析","date":"2019-02-21T05:02:57.000Z","updated":"2021-05-23T14:10:24.527Z","comments":true,"path":"2019/02/21/MapReduce实践 Uber数据分析/","link":"","permalink":"https://hareric.com/2019/02/21/MapReduce%E5%AE%9E%E8%B7%B5%20Uber%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/","excerpt":"介绍这篇博客是关于如何在Hadoop MapReduce中进行Uber数据分析的。","text":"介绍这篇博客是关于如何在Hadoop MapReduce中进行Uber数据分析的。 数据数据下载 数据展示 数据说明 该数据有4列： dispatching_base_number date active_vehicles trips 问题描述计算每个Basement每个周几总共有多少trips MapReduce实现Mapper在Mapper中 使用java.time.LocalDate来获取每个年月日具体是星期几，并将Basement_number+dayofweek作为keys，tripNum作为value。 1234567891011121314151617181920212223242526public static class ExtractTripMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;&#123; private IntWritable tripNum; // trip 值 String specifyDate = &quot;MM/DD/YYYY&quot;; DateTimeFormatter formatter = DateTimeFormatter.ofPattern(&quot;M/d/y&quot;); // date转化格式 LocalDate date; String dayOfWeek; public void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; String[] splitArray = value.toString().split(&quot;,&quot;); // 对字符串进行切分 specifyDate = splitArray[1]; // 使用try来处理不和谐的数据 try&#123; date = LocalDate.parse(specifyDate,formatter); dayOfWeek = date.getDayOfWeek().toString(); tripNum = new IntWritable(new Integer(splitArray[3])); &#125; catch (DateTimeParseException e)&#123; e.printStackTrace(); return; &#125; context.write(new Text(splitArray[0] + &quot;+&quot; + dayOfWeek), tripNum); &#125;&#125; Combiner&amp;Reducer之后就与WordCont相同，进行简单的统计和合并。12345678910111213public static class SumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val : values) &#123; sum += val.get(); &#125; context.write(key, new IntWritable(sum)); &#125;&#125; 输出 完整代码","raw":null,"content":null,"categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://hareric.com/tags/Hadoop/"}]},{"title":"MapReduce实践 Youtube数据分析","slug":"MapReduce实践-Youtube数据分析","date":"2019-02-19T14:46:21.000Z","updated":"2021-05-23T13:21:37.192Z","comments":true,"path":"2019/02/19/MapReduce实践-Youtube数据分析/","link":"","permalink":"https://hareric.com/2019/02/19/MapReduce%E5%AE%9E%E8%B7%B5-Youtube%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90/","excerpt":"介绍这篇博客是关于如何在Hadoop MapReduce中进行YouTube数据分析的。使用该数据集执行一些分析，并将提取一些有用的信息，例如YouTube上排名前10位的视频，他们上传了最多的视频。","text":"介绍这篇博客是关于如何在Hadoop MapReduce中进行YouTube数据分析的。使用该数据集执行一些分析，并将提取一些有用的信息，例如YouTube上排名前10位的视频，他们上传了最多的视频。 数据数据下载 数据展示 数据说明Column 1: Video id of 11 characters.Column 2: uploader of the videoColumn 3: Interval between the day of establishment of Youtube and the date of uploading of the video.Column 4: Category of the video.Column 5: Length of the video.Column 6: Number of views for the video.Column 7: Rating on the video.Column 8: Number of ratings given for the videoColumn 9: Number of comments done on the videos.Column 10: Related video ids with the uploaded video. 问题描述寻找Top N个最多视频的类别 MapReduce实现Mapper对每一行数据进行划分，统计各个视频类别的数量（Column 4）。数据集中部分数据缺失，因此忽略了划分后少于5个属性的数据。123456789101112131415public static class CategoryMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt;&#123; private final static IntWritable one = new IntWritable(1); // 值为1 private Text category = new Text(); public void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; String[] attributeArray = value.toString().split(&quot;\\t&quot;);// 对字符串进行切分 if (attributeArray.length &gt; 5) // 忽略属性值少于5的错误数据 &#123; category.set(attributeArray[3]); context.write(category, one); &#125; &#125;&#125; CombinerCombiner的作用就是对map端的输出先做一次合并，以减少在map和reduce节点之间的数据传输量，以提高网络IO性能，是MapReduce的一种优化手段之一。Combiner实质就是在本地端先运行的一次Reducer。 1234567891011121314public static class SumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val : values) &#123; sum += val.get(); &#125; context.write(key, new IntWritable(sum)); &#125;&#125; Reducer因为需要对，[视频类别, 视频数]数组进行排序比较，因此首先定义一个二元组类，包含视频类别和视频数，分别对应first和second。并定义了Comparable接口，用于后面排序的需要。12345678910111213141516171819202122public static class TwoTuple implements Comparable&lt;TwoTuple&gt;&#123; public String first; public int second; public TwoTuple(String a, int b) &#123; first = a; second = b; &#125; public String toString() &#123; return &quot;(&quot; + first + &quot;, &quot; + second + &quot;)&quot;; &#125; @Override public int compareTo(TwoTuple tt) &#123; return second - tt.second; &#125;&#125;使用Reducer实现提取Top N值的算法。 首先需要介绍setup()函数和cleanup()函数，与reduce()函数不同，不会根据key的数目多次执行，只会执行1次。 setup() 此方法被MapReduce框架仅且执行一次，在执行Map任务前，进行相关变量或者资源的集中初始化工作。若是将资源初始化工作放在方法map()中，导致Mapper任务在解析每一行输入时都会进行资源初始化工作，导致重复，程序运行效率不高。 cleanup() 此方法被MapReduce框架仅且执行一次，在执行完毕Map任务后，进行相关变量或资源的释放工作。若是将释放资源工作放入方法map()中，也会导致Mapper任务在解析、处理每一行文本后释放资源，而且在下一行文本解析前还要重复初始化，导致反复重复，程序运行效率不高。 算法介绍在setup()函数中，主要用来从配置中获取需要提取Top N的N值，并初始化top[]数组；在reduce()函数中，计算出每个Category的视频总数后覆盖放入top[0]数组并进行排序；在cleanup()函数中，将覆盖排序多次后的top数组写入output。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546public static class TopNReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123; int len; TwoTuple[] top; @Override protected void setup(Context context) throws IOException, InterruptedException &#123; len =context.getConfiguration().getInt(&quot;N&quot;, 10); // 从配置中获取top N的N值，若无则默认为10 top = new TwoTuple[len + 1]; for (int i=0; i&lt;=len; i++) &#123; top[i] = new TwoTuple(&quot;null&quot;, 0); &#125; &#125; @Override protected void cleanup(Context context) throws IOException, InterruptedException &#123; for (int i = len; i &gt; 0; i--) &#123; context.write(new Text(top[i].first), new IntWritable(top[i].second)); &#125; &#125; @Override protected void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val : values) &#123; sum += val.get(); &#125; add(key.toString(), sum); &#125; private void add(String key, int val) &#123; top[0].first = key; top[0].second = val; // 替换掉最小值 Arrays.sort(top); // 排序，从小到大顺序 &#125;&#125; main&amp;conf12345678910111213141516171819202122232425262728public static void main(String[] args) throws Exception&#123; Configuration conf = new Configuration(); conf.addResource(&quot;classpath:/hadoop/core-site.xml&quot;); conf.addResource(&quot;classpath:/hadoop/hdfs-site.xml&quot;); conf.addResource(&quot;classpath:/hadoop/mapred-site.xml&quot;); conf.setInt(&quot;N&quot;, 5); // String[] otherArgs = new GenericOptionsParser(conf, // args).getRemainingArgs(); String[] otherArgs = &#123; &quot;/youtube&quot;, &quot;/youtube_category_Top5&quot; &#125;; if (otherArgs.length != 2) &#123; System.err.println(&quot;Usage: wordcount &lt;in&gt; &lt;out&gt;&quot;); System.exit(2); &#125; Job job = new Job(conf, &quot;youtube&quot;); job.setJarByClass(FindMaxCategory.class); job.setMapperClass(CategoryMapper.class); job.setCombinerClass(SumReducer.class);// job.setReducerClass(SumReducer.class); // 统计每个类别的总量 job.setReducerClass(TopNReducer.class); // 统计TopN的类别的总量 job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.setInputDirRecursive(job, true); FileInputFormat.addInputPath(job, new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); System.exit(job.waitForCompletion(true) ? 0 : 1);&#125; 输出 完整代码","raw":null,"content":null,"categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://hareric.com/tags/Hadoop/"}]},{"title":"Hadoop排序","slug":"Hadoop排序","date":"2019-02-18T08:14:51.000Z","updated":"2021-05-22T08:01:05.399Z","comments":true,"path":"2019/02/18/Hadoop排序/","link":"","permalink":"https://hareric.com/2019/02/18/Hadoop%E6%8E%92%E5%BA%8F/","excerpt":"Hadoop排序转载来源\nHadoop排序，从大的范围来说有两种排序，一种是按照key排序，一种是按照value排序。如果按照value排序，只需在map函数中将key和value对调，然后在reduce函数中在对调回去。从小范围来说排序又分成部分排序，全局排序，辅助排序，二次排序等。本文介绍如何在Hadoop中实现全局排序。","text":"Hadoop排序转载来源 Hadoop排序，从大的范围来说有两种排序，一种是按照key排序，一种是按照value排序。如果按照value排序，只需在map函数中将key和value对调，然后在reduce函数中在对调回去。从小范围来说排序又分成部分排序，全局排序，辅助排序，二次排序等。本文介绍如何在Hadoop中实现全局排序。 全局排序，就是说在一个MapReduce程序产生的输出文件中，所有的结果都是按照某个策略进行排序的，例如降序还是升序。MapReduce只能保证一个分区内的数据是key有序的，一个分区对应一个reduce，因此只有一个reduce就保证了数据全局有序，但是这样又不能用到Hadoop集群的优势。 对于多个reduce如何保证数据的全局排序呢？通常的做法是按照key值分区，通过MapReduce的默认分区函数HashPartition将不同范围的key发送到不同的reduce处理，例如一个文件中有key值从1到10000的数据，我们使用两个分区，将1到5000的key发送到partition1，然后由reduce1处理，5001到10000的key发动到partition2然后由reduce2处理，reduce1中的key是按照1到5000的升序排序，reduce2中的key是按照5001到10000的升序排序，这样就保证了整个MapReduce程序的全局排序。但是这样做有两个缺点： 1、当数据量大时会出现OOM。 2、会出现数据倾斜。 Hadoop提供TotalOrderPartitioner类用于实现全局排序的功能，并且解决了OOM和数据倾斜的问题。 TotalOrderPartitioner类提供了数据采样器，对key值进行部分采样，然后按照采样结果寻找key值的最佳分割点，将key值均匀的分配到不同的分区中。 TotalOrderPartitioner 类提供了三个采样器，分别是： SplitSampler 分片采样器，从数据分片中采样数据，该采样器不适合已经排好序的数据 RandomSampler随机采样器，按照设置好的采样率从一个数据集中采样 IntervalSampler间隔采样机，以固定的间隔从分片中采样数据，对于已经排好序的数据效果非常好。 三个采样器都实现了K[] getSample(InputFormat&lt;K,V&gt; inf, Job job)方法，该方法返回的是K[]数组，数组中存放的是根据采样结果返回的key值，即分隔点，MapRdeuce就是根据K[]数组的长度N生成N-1个分区partition数量，然后按照分割点的范围将对应的数据发送到对应的分区中。","raw":null,"content":null,"categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://hareric.com/tags/Hadoop/"}]},{"title":"MapReduce的类型与格式","slug":"MapReduce的类型与格式","date":"2019-02-18T08:14:40.000Z","updated":"2021-05-23T13:21:37.191Z","comments":true,"path":"2019/02/18/MapReduce的类型与格式/","link":"","permalink":"https://hareric.com/2019/02/18/MapReduce%E7%9A%84%E7%B1%BB%E5%9E%8B%E4%B8%8E%E6%A0%BC%E5%BC%8F/","excerpt":"MapReduce的类型与格式转载来源\nMapReduce的类型默认的MR作业\n默认的mapper是Ｍapper类，它将输入的键和值原封不动地写到输出中\n默认的partitioner是HashPartitioner，它对每条记录的键进行哈希操作以决定该记录应该属于哪个分区（每个分区对应于一个reduce任务）\n默认的reducer是Reducer类，它将所有的输入写到输出中\nmap任务的数量等于输入文件被划分成的块数\nreduce任务的个数的选择： 一个经验法则是目标reducer保持在每个运行5分钟左右且产生至少一个HDFS块的输出比较合适\n默认的输入格式是TexInputFormat，输出是TextOutpFormat","text":"MapReduce的类型与格式转载来源 MapReduce的类型默认的MR作业 默认的mapper是Ｍapper类，它将输入的键和值原封不动地写到输出中 默认的partitioner是HashPartitioner，它对每条记录的键进行哈希操作以决定该记录应该属于哪个分区（每个分区对应于一个reduce任务） 默认的reducer是Reducer类，它将所有的输入写到输出中 map任务的数量等于输入文件被划分成的块数 reduce任务的个数的选择： 一个经验法则是目标reducer保持在每个运行5分钟左右且产生至少一个HDFS块的输出比较合适 默认的输入格式是TexInputFormat，输出是TextOutpFormat 输入格式输入分片与记录 一个输入分片就是由单个map操作来处理的数据块，并且每一个map只处理一个分片、 每个输入分片分为若干个记录，每条记录就是 一个键值对，map将一个接一个地处理记录 输入分片和记录都是逻辑概念，不一定对应着文件，也可能对应其他数据形式，如对于数据库，输入分片就是对应于一个表上的若干行，一条记录对应着其中的一行 输入分片只是指向数据的引用，不包含数据本身 InputSpilt接口（Java中的实现），包含 以字节为单位的长度，表示分片的大小，用以排序分片，以便优先处理最大的分片，从而最小化作业运行时间 一组存储位置，供MR系统使用一边将map任务尽可能放在分片数据附近 该接口由InputFormat创建 InputFormat 运行作业的客户端使用getSplits方法计算分片，并将结果告知application master，后者使用其存储信息来调度map任务从而在集群集群上处理这些分片数据 map任务将输入分片传给createRecordReader方法来获取这个分片的RecordReader（就像是记录上的迭代器），map任务用这个RecordReader来生成记录的键值对，然后再将键值对传递给map函数（参见run方法） 1234567891011121314/////InputFormat接口public abstract class InputFormat&lt;K, V&gt; &#123; public abstract List&lt;InputSplit&gt; getSplits(JobContext context) throws IOException, InterruptedException; public abstract RecordReader&lt;K, V&gt; createRecordReader(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException;&#125;/////Mapper的run方法public void run(Context context) throws IOException, InterruptedException &#123; setup(context); while (context.nextKeyValue()/*委托给RecorReader的同名方法，为mapper产生键值对*/) &#123; map(context.getCurrentKey(), context.getCurrentValue(), context);//从RecordReader中检索出并传递给map方法 &#125; cleanup(context);&#125; 文件输入–FileInputFormat FileInputFormat类提供两个功能： 指出作业的输入文件位置 实现了为输入文件产生输入分片的功能（把分片切割成记录的功能由其子类完成） 输入路径public static void addInputPath(Job job, Path path)public static void addInputPaths(Job job, String commaSeparatedPaths)public static void setInputPaths(Job job, Path... inputPaths)public static void setInputPaths(Job job, String commaSeparatedPaths) 前两者用于加入一个或多个路径到路径列表中，后两者一次设定完整的路径列表（replacing any paths set on the Job in previous calls） 一条路径可以是文件、目录或者glob（文件和目录的结合），但是目录在默认情况下不会进行递归处理，如果目录下存在子目录，则要么采用glob的形式，要么设置过滤器过滤子目录（因为子目录会被当作文件而报错），或者更改属性设置让其可以递归处理 FileInputFormat有默认过滤器，用以过滤隐藏文件（自定义的过滤器会和这个默认的一起工作） 输入分片： FileInputFormat只分割大文件（超过块的大小） 分片计算公式 max(minimumSize, min(maximumSize, blockSize)) 默认情况 minimumSize &lt; blockSize &lt; maximumSize 小文件与CombineFileInputFormat CombineFileInputFormat类可以把多个文件打包到一个分片中（在决定将哪些块放到同一分片时，会考虑节点和机架的因素） 避免切分 设置最小分片大小以避免切分、 重写isSplitable方法 mapper中文件信息 调用Mapper类中Context对象的getInputSplit方法来获得InputSplit，对于FileInputFormat，它会被转成FileSplit 注意此处的getInputSplit方法和InputFormat中的getSplit方法，后者是用于为整个输入计算分片，而前者是为某个mapper获取该输入分片的相关信息 把整个文件作为一条记录处理 即便不分割文件，仍然需要一个RecordReader来读取文件内容作为record的值 WholeFileRecordReader,java 文本输入–TextInputFormatHadoop非常擅长处理非结构化文本数据 TextInputFormat是默认的InputFormat 每条记录是一条输入，键是LongWritable，存储该行在整个文件中的字节偏移量，值是该行的内容（不包括任何行终止符） 由于此处的逻辑记录是以行为单位的，因而可能出现某一行会跨文件块存放，从未会为‘本地化’的map任务带来远程读操作的开销（这是因为分片是和行对齐的而不是hdfs块，参考图示） 控制一行最大的长度 目的是应对损坏的文件，文件的损坏可能对应一个超长行，从而导致内存溢出 长度通过属性mapreduce.input.linerecordreader.line.maxlength设置 关于KeyValueTextInputFFormat 目的是应对那些每行内容是一个键值对的文件（之所以是键值对，是因为它经过了一些操作，比如TextOutputFormat的输出就会将键值对写入文件，两者之间使用分隔符分开） 所以使用时要指定键值对之间的分隔符，默认是制表符（属性mapreduce.input.keyvaluelinere cordreader.key.value.separator），且保持原来的键而不是使用偏移量作为键 关于NLineInputFormat 一般每个mapper收到的行数不同（行数取决于分片大小和行长度），通过该类可是使每个mapper收到的行数相同 键是文件中行的字节偏移量，值是行本身 应用场景 仿真 用Hadoop引导从多个数据源（如数据库）加载数据，每行一个数据源 关于xml StreamXmlReccordReader","raw":null,"content":null,"categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://hareric.com/tags/Hadoop/"}]},{"title":"Parquet学习笔记","slug":"Parquet学习笔记","date":"2019-02-15T11:56:20.000Z","updated":"2021-05-23T13:21:37.192Z","comments":true,"path":"2019/02/15/Parquet学习笔记/","link":"","permalink":"https://hareric.com/2019/02/15/Parquet%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/","excerpt":"简介Apache Parquet是Hadoop生态圈中一种新型列式存储格式，它可以兼容Hadoop生态圈中大多数计算框架(Hadoop、Spark等)，被多种查询引擎支持(Hive、Impala、Drill等)，并且它是语言和平台无关的。","text":"简介Apache Parquet是Hadoop生态圈中一种新型列式存储格式，它可以兼容Hadoop生态圈中大多数计算框架(Hadoop、Spark等)，被多种查询引擎支持(Hive、Impala、Drill等)，并且它是语言和平台无关的。 Definition level &amp; Repetition level定义definition LevelDefinition level指明该列的路径上多少个可选field被定义了。 definition Level是该路径上有定义的repeated field 和 optional field的个数，不包括required field，因为required field是必须有定义的。 Repetition levelsRepetition level指明该值在路径中哪个repeated field重复。 DL和RL的计算 我们用深度0表示一个纪录的开头（虚拟的根节点），深度的计算忽略非重复字段（标签不是repeated的字段都不算在深度里）。所以在Name.Language.Code这个路径中，包含两个重复字段，Name和Language，如果在Name处重复，重复深度为1（虚拟的根节点是0，下一级就是1），在Language处重复就是2，不可能在Code处重复，它是required类型，表示有且仅有一个；同样的，在路径Links.Forward中，Links是optional的，不参与深度计算（不可能重复），Forward是repeated的，因此只有在Forward处重复时重复深度为1。 Parquet Java example来源pom文件123456789101112131415161718&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.parquet&lt;/groupId&gt; &lt;artifactId&gt;parquet-avro&lt;/artifactId&gt; &lt;version&gt;1.10.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt; &lt;version&gt;0.23.1&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.1&lt;/version&gt; &lt;/dependency&gt;&lt;/dependencies&gt;WriteParquet.java1234567891011121314151617181920212223242526272829303132333435363738import java.io.IOException;import java.util.ArrayList;import java.util.List;import org.apache.avro.Schema;import org.apache.avro.Schema.Field;import org.apache.avro.Schema.Type;import org.apache.avro.generic.GenericData;import org.apache.avro.generic.GenericData.Record;import org.apache.hadoop.fs.Path;import org.apache.parquet.avro.AvroParquetWriter;import org.apache.parquet.hadoop.ParquetWriter;import org.apache.parquet.hadoop.metadata.CompressionCodecName;public class WriteParquet &#123; public static void main(String[] args) throws IllegalArgumentException, IOException &#123; List&lt;Field&gt; fields = new ArrayList&lt;Field&gt;(); Object defaultValue = null; fields.add(new Field(&quot;x&quot;, Schema.create(Type.INT), &quot;x&quot;, defaultValue)); fields.add(new Field(&quot;y&quot;, Schema.create(Type.INT), &quot;y&quot;, defaultValue)); Schema schema = Schema.createRecord(&quot;name&quot;, &quot;doc&quot;, &quot;namespace&quot;, false, fields); try (ParquetWriter&lt;GenericData.Record&gt; writer = AvroParquetWriter.&lt;GenericData.Record&gt;builder( new Path(&quot;my-file.parquet&quot;)).withSchema(schema).withCompressionCodec(CompressionCodecName.SNAPPY) .build()) &#123; // 模拟10000行数据 for (int r = 0; r &lt; 10000; ++r) &#123; Record record = new Record(schema); record.put(0, r); record.put(1, r * 3); writer.write(record); &#125; &#125; &#125;&#125;ReadParquet.java12345678910111213141516171819import java.io.IOException;import org.apache.avro.generic.GenericRecord;import org.apache.hadoop.fs.Path;import org.apache.parquet.avro.AvroParquetReader;import org.apache.parquet.hadoop.ParquetReader;public class ReadParquet &#123; public static void main(String[] args) throws IllegalArgumentException, IOException &#123; ParquetReader&lt;GenericRecord&gt; reader = AvroParquetReader.&lt;GenericRecord&gt;builder(new Path(&quot;my-file.parquet&quot;)) .build(); GenericRecord record; while ((record = reader.read()) != null) &#123; System.out.println(record); &#125; &#125;&#125;","raw":null,"content":null,"categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://hareric.com/tags/Hadoop/"}]},{"title":"Avro简介及Java运用","slug":"Avro简介及Java运用","date":"2019-02-15T04:36:05.000Z","updated":"2021-05-22T08:01:05.398Z","comments":true,"path":"2019/02/15/Avro简介及Java运用/","link":"","permalink":"https://hareric.com/2019/02/15/Avro%E7%AE%80%E4%BB%8B%E5%8F%8AJava%E8%BF%90%E7%94%A8/","excerpt":"简介Avro是一种远程过程调用和数据序列化框架，是在Apache的Hadoop项目之内开发的。它使用JSON来定义数据类型和通讯协议，使用压缩二进制格式来序列化数据。它主要用于Hadoop，它可以为持久化数据提供一种序列化格式，Avro是一个数据序列化的系统。可以将数据结构或对象转化成便于存储或传输的格式。Avro设计之初就用来支持数据密集型应用，适合于远程或本地大规模数据的存储和交换。","text":"简介Avro是一种远程过程调用和数据序列化框架，是在Apache的Hadoop项目之内开发的。它使用JSON来定义数据类型和通讯协议，使用压缩二进制格式来序列化数据。它主要用于Hadoop，它可以为持久化数据提供一种序列化格式，Avro是一个数据序列化的系统。可以将数据结构或对象转化成便于存储或传输的格式。Avro设计之初就用来支持数据密集型应用，适合于远程或本地大规模数据的存储和交换。 序列化及反序列化概念 把对象转换为字节序列的过程称为对象的序列化。 把字节序列恢复为对象的过程称为对象的反序列化。 对象的序列化主要有两种用途： 把对象的字节序列永久地保存到硬盘上，通常存放在一个文件中 在网络上传送对象的字节序列。 在很多应用中，需要对某些对象进行序列化，让它们离开内存空间，入住物理硬盘，以便长期保存。比如最常见的是Web服务器中的Session对象，当有 10万用户并发访问，就有可能出现10万个Session对象，内存可能吃不消，于是Web容器就会把一些seesion先序列化到硬盘中，等要用了，再把保存在硬盘中的对象还原到内存中。 当两个进程在进行远程通信时，彼此可以发送各种类型的数据。无论是何种类型的数据，都会以二进制序列的形式在网络上传送。发送方需要把这个Java对象转换为字节序列，才能在网络上传送；接收方则需要把字节序列再恢复为Java对象。 Avro特点 丰富的数据结构类型； 快速可压缩的二进制数据形式，对数据二进制序列化后可以节约数据存储空间和网络传输带宽； 存储持久数据的文件容器； 可以实现远程过程调用RPC； 简单的动态语言结合功能。 数据类型基础数据类型 Type Description Schema null the absence of a value “null” boolean a binary value “boolean” int 32-bit signed integer “int” long 64-bit signed integer “long” float Single-precision (32-bit) IEEE 754 floating-point number “float” double Double-precision (64-bit) IEEE 754 floating-point number “double” bytes Sequence of 8-bit unsigned bytes “bytes” string Sequence of Unicode characters “string” 复杂数据类型Avro提供了6种复杂类型。分别是Record，Enum，Array，Map，Union和Fixed。 RecordRecord是一个任意类型的命名字段的集合 支持的属性设置： name：record类型的名字(必填) namespace：命名空间(可选) doc：这个类型的文档说明(可选) aliases：record类型的别名，是个字符串数组(可选) fields：record类型中的字段，是个对象数组(必填)。每个字段需要以下属性： name：字段名字(必填) doc：字段说明文档(可选) type：一个schema的json对象或者一个类型名字(必填) default：默认值(可选) order：排序(可选)，只有3个值ascending(默认)，descending或ignore aliases：别名，字符串数组(可选) 一个Record类型例子，定义一个元素类型是Long的链表：123456789&#123; &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;LongList&quot;, &quot;aliases&quot;: [&quot;LinkedLongs&quot;], // old name for this &quot;fields&quot; : [ &#123;&quot;name&quot;: &quot;value&quot;, &quot;type&quot;: &quot;long&quot;&#125;, // each element has a long &#123;&quot;name&quot;: &quot;next&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;LongList&quot;]&#125; // optional next element ]&#125; Enum枚举类型的类型名字是”enum”，还支持其它属性的设置： name：枚举类型的名字(必填) namespace：命名空间(可选) aliases：字符串数组，别名(可选) doc：说明文档(可选) symbols：字符串数组，所有的枚举值(必填)，不允许重复数据。 一个枚举类型的例子：1234&#123; &quot;type&quot;: &quot;enum&quot;, &quot;name&quot;: &quot;Suit&quot;, &quot;symbols&quot; : [&quot;SPADES&quot;, &quot;HEARTS&quot;, &quot;DIAMONDS&quot;, &quot;CLUBS&quot;]&#125; Array数组类型的类型名字是”array”并且只支持一个属性： items：数组元素的schema 一个数组例子：1234&#123; &quot;type&quot;: &quot;array&quot;, &quot;items&quot;: &quot;string&quot;&#125; MapMap类型的类型名字是”map”并且只支持一个属性： values：map值的schema Map的key必须是字符串。 一个Map例子：1234&#123; &quot;type&quot;: &quot;map&quot;, &quot;values&quot;: &quot;long&quot;&#125; Union组合类型，表示各种类型的组合，使用数组进行组合。比如[“null”, “string”]表示类型可以为null或者string。 如以下例子12345678910&#123; &quot;type&quot;: &quot;record&quot;, &quot;namespace&quot;: &quot;com.example&quot;, &quot;name&quot;: &quot;FullName&quot;, &quot;fields&quot;: [ &#123; &quot;name&quot;: &quot;first&quot;, &quot;type&quot;: [&quot;null&quot;, &quot;string&quot;] &#125;, &#123; &quot;name&quot;: &quot;last&quot;, &quot;type&quot;: &quot;string&quot;, &quot;default&quot; : &quot;Doe&quot; &#125; ]&#125; 组合类型的默认值是看组合类型的第一个元素，因此如果一个组合类型包括null类型，那么null类型一般都会放在第一个位置，这样子的话这个组合类型的默认值就是null。 组合类型中不允许同一种类型的元素的个数不会超过1个，除了record，fixed和enum。比如组合类中有2个array类型或者2个map类型，这是不允许的。 组合类型不允许嵌套组合类型。 Fixed混合类型的类型名字是fixed，支持以下属性： name：名字(必填) namespace：命名空间(可选) aliases：字符串数组，别名(可选) size：一个整数，表示每个值的字节数(必填) 比如16个字节数的fixed类型例子如下：12345&#123; &quot;type&quot;: &quot;fixed&quot;, &quot;size&quot;: 16, &quot;name&quot;: &quot;md5&quot;&#125; java实践配置引入pom.xml12345&lt;dependency&gt; &lt;groupId&gt;org.apache.avro&lt;/groupId&gt; &lt;artifactId&gt;avro&lt;/artifactId&gt; &lt;version&gt;1.8.2&lt;/version&gt;&lt;/dependency&gt; 下载avro-toolavro-tools-1.8.2.jar 该jar包主要用户将定义好的schema文件生成对应的java文件下载地址 schemaAvro依赖模式(Schema)来实现数据结构定义。可以把模式理解为Java的类，它定义每个实例的结构，可以包含哪些属性。可以根据类来产生任意多个实例对象。对实例序列化操作时必须需要知道它的基本结构，也就需要参考类的信息。这里，根据模式产生的Avro对象类似于类的实例对象。每次序列化/反序列化时都需要知道模式的具体结构。所以，在Avro可用的一些场景下，如文件存储或是网络通信，都需要模式与数据同时存在。Avro数据以模式来读和写(文件或是网络)，并且写入的数据都不需要加入其它标识，这样序列化时速度快且结果内容少。由于程序可以直接根据模式来处理数据，所以Avro更适合于脚本语言的发挥。 定义schemaavro的schema是json格式，支持前文所描述的类型。下面是一个schema例子 User.avsc12345678910111213141516171819202122232425&#123; &quot;type&quot;: &quot;record&quot;, &quot;name&quot;: &quot;User&quot;, &quot;fields&quot;: [ &#123; &quot;name&quot;: &quot;name&quot;, &quot;type&quot;: &quot;string&quot;, &quot;doc&quot;:&quot;姓名&quot; &#125;, &#123; &quot;name&quot;: &quot;favorite_number&quot;, &quot;type&quot;: [ &quot;int&quot;, &quot;null&quot; ] &#125;, &#123; &quot;name&quot;: &quot;favorite_color&quot;, &quot;type&quot;: [ &quot;string&quot;, &quot;null&quot; ] &#125; ]&#125; 编译schema使用avro-tools-1.8.2.jar 编译编译命令格式如下： java -jar avro-tools-1.8.2.jar compile schema &lt;filename.avsc&gt; &lt;outputpath&gt; 序列化与反序列化avro的序列化和反序列化，有两种方法。第一种是需要利用schema生成实体类，另外一种只需要指定schema并使用GenericRecord作为实体。 生成实体类User编译schema至java项目src目录 序列化AvroSerialize.java1234567891011121314151617181920212223242526272829303132import java.io.*;import org.apache.avro.file.*;import org.apache.avro.io.*;import org.apache.avro.specific.*; public class AvroSerialize &#123; public static void main(String args[]) throws IOException &#123; // 新建3个实体类 User user1 = new User(); user1.setName(&quot;Alyssa&quot;); user1.setFavoriteNumber(256); User user2 = new User(&quot;Ben&quot;, 7, &quot;red&quot;); User user3 = User.newBuilder() .setName(&quot;Charlie&quot;) .setFavoriteColor(&quot;blue&quot;) .setFavoriteNumber(null) .build(); DatumWriter&lt;User&gt; userDatumWriter = new SpecificDatumWriter&lt;User&gt;(User.class); DataFileWriter&lt;User&gt; dataFileWriter = new DataFileWriter&lt;User&gt;(userDatumWriter); dataFileWriter.create(user1.getSchema(), new File(&quot;./src/main/avro/user.avro&quot;)); dataFileWriter.append(user1); dataFileWriter.append(user2); dataFileWriter.append(user3); dataFileWriter.close(); &#125;&#125; 反序列化123456789101112131415161718192021import java.io.*;import org.apache.avro.file.*;import org.apache.avro.io.*;import org.apache.avro.specific.*;public class AvroDeserialize &#123; public static void main(String args[]) &#123; DatumReader&lt;User&gt; reader = new SpecificDatumReader&lt;&gt;(User.class); DataFileReader&lt;User&gt; fileReader = null; try &#123; fileReader = new DataFileReader&lt;User&gt;(new File(&quot;./src/main/avro/user.avro&quot;), reader); User user = null; while (fileReader.hasNext()) &#123; user = fileReader.next(user); System.out.println(user); &#125; &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; &#125;&#125; 输出 {“name”: “Alyssa”, “favorite_number”: 256, “favorite_color”: null}{“name”: “Ben”, “favorite_number”: 7, “favorite_color”: “red”}{“name”: “Charlie”, “favorite_number”: null, “favorite_color”: “blue”} 使用GenericRecord序列化AvroGenericDeserializa.java123456789101112131415161718192021222324252627282930package avro;import java.io.*;import org.apache.avro.Schema;import org.apache.avro.file.*;import org.apache.avro.generic.*;import org.apache.avro.io.*;public class AvroGenericDeserializa &#123; public static void main(String args[]) throws IOException &#123; Schema schema = new Schema.Parser().parse(new File(&quot;./src/main/avro/User.avsc&quot;)); GenericRecord user1 = new GenericData.Record(schema); user1.put(&quot;name&quot;, &quot;Alyssa&quot;); user1.put(&quot;favorite_number&quot;, 256); GenericRecord user2 = new GenericData.Record(schema); user2.put(&quot;name&quot;, &quot;Ben&quot;); user2.put(&quot;favorite_number&quot;, 7); user2.put(&quot;favorite_color&quot;, &quot;red&quot;); File file = new File(&quot;./src/main/avro/user_generic.avro&quot;); DatumWriter&lt;GenericRecord&gt; datumWriter = new GenericDatumWriter&lt;GenericRecord&gt;(schema); DataFileWriter&lt;GenericRecord&gt; dataFileWriter = new DataFileWriter&lt;GenericRecord&gt;(datumWriter); dataFileWriter.create(schema, file); dataFileWriter.append(user1); dataFileWriter.append(user2); dataFileWriter.close(); &#125;&#125; 反序列化AvroGenericSerialize.java1234567891011121314151617181920import java.io.*;import org.apache.avro.Schema;import org.apache.avro.file.*;import org.apache.avro.generic.*;import org.apache.avro.io.*;public class AvroGenericSerialize &#123; public static void main(String args[]) throws IOException &#123; Schema schema = new Schema.Parser().parse(new File(&quot;./src/main/avro/User.avsc&quot;)); File file = new File(&quot;./src/main/avro/user_generic.avro&quot;); DatumReader&lt;GenericRecord&gt; datumReader = new GenericDatumReader&lt;GenericRecord&gt;(schema); DataFileReader&lt;GenericRecord&gt; dataFileReader = new DataFileReader&lt;GenericRecord&gt;(file, datumReader); GenericRecord user = null; while (dataFileReader.hasNext()) &#123; user = dataFileReader.next(user); System.out.println(user); &#125; &#125;&#125;输出 {“name”: “Alyssa”, “favorite_number”: 256, “favorite_color”: null}{“name”: “Ben”, “favorite_number”: 7, “favorite_color”: “red”}","raw":null,"content":null,"categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://hareric.com/tags/Hadoop/"}]},{"title":"Eclipse+Maven构建Hadoop项目","slug":"Eclipse+Maven构建Hadoop项目","date":"2019-01-31T16:00:00.000Z","updated":"2021-05-23T13:21:37.192Z","comments":true,"path":"2019/02/01/Eclipse+Maven构建Hadoop项目/","link":"","permalink":"https://hareric.com/2019/02/01/Eclipse+Maven%E6%9E%84%E5%BB%BAHadoop%E9%A1%B9%E7%9B%AE/","excerpt":"Maven翻译为”专家”、”内行”，是 Apache 下的一个纯 Java 开发的开源项目。基于项目对象模型(Project Object Model 缩写：POM)概念，Maven利用一个中央信息片断能管理一个项目的构建、报告和文档等步骤。Maven 是一个项目管理工具，可以对 Java 项目进行构建、依赖管理。\n在开发一些大型项目的时候，需要用到各种各样的开源包jar，为了方便管理及加载jar，使用maven开发项目可以节省大量时间且方便项目移动至新的开发环境。","text":"Maven翻译为”专家”、”内行”，是 Apache 下的一个纯 Java 开发的开源项目。基于项目对象模型(Project Object Model 缩写：POM)概念，Maven利用一个中央信息片断能管理一个项目的构建、报告和文档等步骤。Maven 是一个项目管理工具，可以对 Java 项目进行构建、依赖管理。 在开发一些大型项目的时候，需要用到各种各样的开源包jar，为了方便管理及加载jar，使用maven开发项目可以节省大量时间且方便项目移动至新的开发环境。 开发环境 系统：MacOS 10.14.1 Hadoop：2.7.0 Java：1.8.0 Eclipse：4.6.2 Maven: 3.3.9 Maven安装我使用的这个版本的Eclipse已经自带了Maven插件，不需要在自行安装，因此我也没有实际操作，本文就不介绍如何配置。 至于怎么知道自己使用的Eclipse是否自带有Maven，可以在Eclipse-&gt;Preference-&gt;Maven-&gt;Installations查看是否有Maven及版本号。或者直接新建项目查看是否有Maven选项。 构建Hadoop环境创建Maven项目打开Eclipse，File-&gt;new-&gt;project，选择Maven，然后下一步next 选择Creat a simple project，然后下一步next 输入Group id和artifact id。然后finish。 groupid和artifactId被统称为“坐标”是为了保证项目唯一性而提出的，如果你要把你项目弄到maven本地仓库去，你想要找到你的项目就必须根据这两个id去查找。 groupId一般分为多个段，这里我只说两段，第一段为域，第二段为公司名称。域又分为org、com、cn等等许多，其中org为非营利组织，com为商业组织。举个apache公司的tomcat项目例子：这个项目的groupId是org.apache，它的域是org（因为tomcat是非营利项目），公司名称是apache，artigactId是tomcat。 比如我创建一个项目，我一般会将groupId设置为cn.snowin，cn表示域为中国，snowin是我个人姓名缩写，artifactId设置为testProj，表示你这个项目的名称是testProj，依照这个设置，你的包结构最后是cn.snowin.testProj打头。(引自链接) 完成上述步骤后，就可以在Project Explorer中看到刚刚创建的Maven项目。 增加Hadoop依赖我使用的Hadoop 2.7版本，以下是我的POM配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;practice.hadoop&lt;/groupId&gt; &lt;artifactId&gt;simple-examples&lt;/artifactId&gt; &lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt; &lt;packaging&gt;jar&lt;/packaging&gt; &lt;name&gt;simple-examples&lt;/name&gt; &lt;url&gt;http://maven.apache.org&lt;/url&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-common --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-client --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.mrunit&lt;/groupId&gt; &lt;artifactId&gt;mrunit&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;classifier&gt;hadoop2&lt;/classifier&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-core --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-core&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-yarn-api --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-yarn-api&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-auth --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-auth&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-minicluster --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-minicluster&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-mapreduce-client-jobclient --&gt; &lt;dependency&gt; &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt; &lt;artifactId&gt;hadoop-mapreduce-client-jobclient&lt;/artifactId&gt; &lt;version&gt;2.7.0&lt;/version&gt; &lt;scope&gt;provided&lt;/scope&gt; &lt;/dependency&gt; &lt;/dependencies&gt;&lt;/project&gt; 在Project Explorer中右键该项目，选择build project，Maven就会根据POM.xml配置文件下载所需要的jar包。 稍等一段时间后，就可以看到Maven Dependencies中已经下载好的jar包。 hadoop配置文件运行 MapReduce 程序前，务必将 /usr/local/Cellar/hadoop/2.7.0/libexec/etc/hadoop 中将有修改过的配置文件（如伪分布式需要core-site.xml 和 hdfs-site.xml），以及log4j.properties复制到src/main/resources/ MapReduce实例—WordCount在src/main/java/路径下，创建java文件，代码如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475import java.io.IOException;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;public class WordCount &#123; public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; &#123; /** * LongWritable, IntWritable, Text 均是 Hadoop 中实现的用于封装 Java * 数据类型的类，这些类实现了WritableComparable接口， * 都能够被串行化从而便于在分布式环境中进行数据交换，你可以将它们分别视为long,int,String 的替代品。 */ private final static IntWritable one = new IntWritable(1); // 值为1 private Text word = new Text(); public void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; StringTokenizer itr = new StringTokenizer(value.toString()); // 对字符串进行切分 while (itr.hasMoreTokens()) &#123; word.set(itr.nextToken()); context.write(word, one); &#125; &#125; &#125; public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val : values) &#123; sum += val.get(); &#125; result.set(sum); context.write(key, result); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration(); conf.addResource(&quot;classpath:/hadoop/core-site.xml&quot;); conf.addResource(&quot;classpath:/hadoop/hdfs-site.xml&quot;); conf.addResource(&quot;classpath:/hadoop/mapred-site.xml&quot;);// String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); String[] otherArgs = &#123;&quot;/input&quot;, &quot;/output&quot;&#125;; if (otherArgs.length != 2) &#123; System.err.println(&quot;Usage: wordcount &lt;in&gt; &lt;out&gt;&quot;); System.exit(2); &#125; Job job = new Job(conf, &quot;word count&quot;); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.setInputDirRecursive(job, true); FileInputFormat.addInputPath(job, new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125;","raw":null,"content":null,"categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://hareric.com/tags/Hadoop/"}]},{"title":"《Hadoop权威指南》ch6 MapReduce配置","slug":"《Hadoop权威指南》ch6 MapReduce配置","date":"2019-01-28T16:00:00.000Z","updated":"2021-05-23T13:21:37.191Z","comments":true,"path":"2019/01/29/《Hadoop权威指南》ch6 MapReduce配置/","link":"","permalink":"https://hareric.com/2019/01/29/%E3%80%8AHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E3%80%8Bch6%20MapReduce%E9%85%8D%E7%BD%AE/","excerpt":"Hadoop Configuration APIHadoop中的组件Components是通过Hadoop自带的API进行配置的。一个Configuration类（org.apache.hadoop.conf）的实例代表配置属性及其取值的一个集合。每个属性有一个String进行命名并作为一个key，而值的类型包括Java的基本类型如（boolean、int、long、float等）。Configuration实例可以从XML文件中读取属性值。","text":"Hadoop Configuration APIHadoop中的组件Components是通过Hadoop自带的API进行配置的。一个Configuration类（org.apache.hadoop.conf）的实例代表配置属性及其取值的一个集合。每个属性有一个String进行命名并作为一个key，而值的类型包括Java的基本类型如（boolean、int、long、float等）。Configuration实例可以从XML文件中读取属性值。 XML配置文件的创建及使用 一个简单的配置文件configuration-1.xml123456789101112131415161718192021&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;color&lt;/name&gt; &lt;value&gt;yellow&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;size&lt;/name&gt; &lt;value&gt;10&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;weight&lt;/name&gt; &lt;value&gt;heavy&lt;/value&gt; &lt;final&gt;true&lt;/final&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;size-weight&lt;/name&gt; &lt;value&gt;$&#123;size&#125;,$&#123;weight&#125;&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 配置文件需要放置在MapReduce项目的src目录下。实例化Configuration，并使用get()方法获取指定属性的值。另外，get()方法允许为XML文件中没有定义的属性指定默认值。 Java代码如下12345678910111213import org.apache.hadoop.conf.*;public class TestConfiguration &#123; public static void main(String[] args) throws Exception&#123; Configuration conf = new Configuration(); conf.addResource(&quot;configuration-1.xml&quot;); System.out.println(conf.get(&quot;color&quot;)); // 输出为“yello” System.out.println(conf.getInt(&quot;size&quot;, 0)); // 输出为10 System.out.println(conf.get(&quot;breadth&quot;, &quot;wide&quot;)); //输出为“wide” &#125;&#125; 资源合并（Combining Resource）Configuration类是添加多个配置文件的，后来添加到资源文件的属性会覆盖之前的定义属性。但若属性的final值为true的话，意味该属性值不能被后面的定义所覆盖。若尝试着覆盖，将会弹出警告消息帮助进行故障诊断。 第二个配置文件configuration-2.xml123456789101112&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;configuration&gt; &lt;property&gt; &lt;name&gt;size&lt;/name&gt; &lt;value&gt;12&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;weight&lt;/name&gt; &lt;value&gt;light&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; Java代码如下12345678910111213import org.apache.hadoop.conf.*;public class TestConfiguration &#123; public static void main(String[] args) throws Exception&#123; Configuration conf = new Configuration(); conf.addResource(&quot;configuration-1.xml&quot;); conf.addResource(&quot;configuration-2.xml&quot;); System.out.println(conf.getInt(&quot;size&quot;, 0)); // 输出为12，size属性值被第二个配置文件覆盖 System.out.println(conf.get(&quot;weight&quot;)); // 输出为&quot;light&quot;，weight属性的final值为true，无法被覆盖输出不变 &#125;&#125; 因尝试着override weight属性值，报以下警告 变量扩展（Variable Expansion）配置文件可以定义配置属性，如第一个配置文件中，size-weight订了${size}和${weight}。此外，在程序中可以另外定义系统属性，系统属性的优先级高于资源文件中定义的配置属性。1234567891011121314151617181920import org.apache.hadoop.conf.*;public class TestConfiguration &#123; public static void main(String[] args) throws Exception&#123; Configuration conf = new Configuration(); conf.addResource(&quot;configuration-1.xml&quot;); conf.addResource(&quot;configuration-2.xml&quot;); System.setProperty(&quot;size&quot;, &quot;14&quot;); // 配置属性可以用系统属性进行定义 System.setProperty(&quot;weight&quot;, &quot;super heavy&quot;); System.out.println(&quot;系统属性size为 &quot; + System.getProperty(&quot;size&quot;)); // 输出为14 System.out.println(&quot;系统属性weight为 &quot; + System.getProperty(&quot;weight&quot;)); // 输出为super heavy System.out.println(&quot;配置属性size为 &quot; + conf.get(&quot;size&quot;)); // 输出值仍为12 System.out.println(&quot;配置属性weight为 &quot; + conf.get(&quot;weight&quot;)); // 输出值仍为light &#125; &#125; 辅助类GenericOptionsParser, Tool和ToolRunnerHadoop提供了辅助类，GenericOptionsParser：用来解释常用的Hadoop命令选项，但是一般更常用的方式:实现Tool接口，通过ToolsRunner来运行程序，ToolRunner内部调用GenericOptionsParser123456789101112131415161718192021222324252627282930package configuration;import java.util.Map.Entry;import org.apache.hadoop.conf.*;import org.apache.hadoop.util.*;public class ConfigurationPrinter extends Configured implements Tool &#123; // 将配置文件添加进资源，若配置无需修改（即src文件夹无配置文件），则无需执行 // static &#123;// Configuration.addDefaultResource(&quot;hdfs-default.xml&quot;);// Configuration.addDefaultResource(&quot;hdfs-site.xml&quot;);// Configuration.addDefaultResource(&quot;mapred-default.xml&quot;);// Configuration.addDefaultResource(&quot;mapred-site.xml&quot;);// &#125; @Override public int run(String[] args) throws Exception &#123; Configuration conf = getConf(); for (Entry&lt;String, String&gt; entry : conf) &#123; System.out.printf(&quot;%s=%s\\n&quot;, entry.getKey(), entry.getValue()); &#125; return 0; &#125; public static void main(String[] args) throws Exception &#123; int exitCode = ToolRunner.run(new ConfigurationPrinter(), args); System.exit(exitCode); &#125;&#125;ConfigurationPrinter的main()没有直接调用自身的run()，而是调用了ToolRunner的静态run()方法，该方法在调用自身的run()之前，为Tool建立一个Configuration对象。","raw":null,"content":null,"categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://hareric.com/tags/Hadoop/"}]},{"title":"Hadoop开发实例 使用Eclipse实现WordCount","slug":"Hadoop开发实例 使用Eclipse实现WordCount","date":"2019-01-28T03:45:48.000Z","updated":"2021-05-23T13:21:37.192Z","comments":true,"path":"2019/01/28/Hadoop开发实例 使用Eclipse实现WordCount/","link":"","permalink":"https://hareric.com/2019/01/28/Hadoop%E5%BC%80%E5%8F%91%E5%AE%9E%E4%BE%8B%20%E4%BD%BF%E7%94%A8Eclipse%E5%AE%9E%E7%8E%B0WordCount/","excerpt":"在学习Hadoop中，WordCount是许多人接触的第一个MapReduce程序，我也不例外。通过在网络上学习找到了不少帖子，但大多数是使用终端运行hadoop jar WordCount.jar实现的。本文将介绍如何使用Eclipse开发并运行WordCount。","text":"在学习Hadoop中，WordCount是许多人接触的第一个MapReduce程序，我也不例外。通过在网络上学习找到了不少帖子，但大多数是使用终端运行hadoop jar WordCount.jar实现的。本文将介绍如何使用Eclipse开发并运行WordCount。 开发环境 系统：MacOS 10.14.1 Hadoop：2.7.0 Java：1.8.0 Eclipse：4.6.2 配置Eclipse添加hadoop-eclipse-plugin插件新安装的Eclipse通常是无法直接新建MapReduce程序，需要添加hadoop-eclipse-plugin插件，下载地址。此处以macOS为例。 先下载hadoop_eclipse_plugin插件 进入到eclipse的dropins目录在Finder中找到应用程序，然后找到eclipse.这时eclipse显示的是一个app，而不是一个目录。右键点击后，打开选择显示包内容，就可以找到eclipse的应用程序目录了 把hadoop_eclipse_plugin.jar放进dropins文件夹内。 重启eclipse，打开window-&gt; show view-&gt; other-&gt; MapReduce Tools,选择Map/Reduce Locations 与Hadoop集群建立连接点击Eclipse的Map/Reduce Locations面板在面板中单击右键，选择New Hadoop Location。在弹出来的General选项面板中，General 的设置要与 Hadoop 的配置一致。一般两个 Host值是一样的。如果是在本机搭建hadoop伪分布式，填写 localhost 即可，这里使用的是Hadoop伪分布，DFS Master 的 Port 改为 9000。Map/Reduce(V2) Master 的 Port 用默认的即可，Location Name 随意填写。 操作HDFS文件配置好后，点击Project Explorer 中的DFS Location就能直接查看 HDFS 中的文件列表，双击可以查看内容，右键点击可以上传、下载、删除 HDFS 中的文件。当程序运行完成后，会生成一个output文件夹，通常运行完成后不能马上看到，此时可以尝试，右键点击DFS Locations后Reconnect或重启Eclipse。 使用Eclipse实现WordCountCreate Project点击File菜单,选择New-&gt;Project选择MapReduce wizard，后点击next。Project Name随意填写，然后点finish。 Create WordCount.java如普通Project一样，创建一个java程序后，复制如下代码。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071import java.io.IOException;import java.util.StringTokenizer;import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IntWritable;import org.apache.hadoop.io.Text;import org.apache.hadoop.mapreduce.Job;import org.apache.hadoop.mapreduce.Mapper;import org.apache.hadoop.mapreduce.Reducer;import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;import org.apache.hadoop.util.GenericOptionsParser;public class WordCount &#123; public static class TokenizerMapper extends Mapper&lt;Object, Text, Text, IntWritable&gt; &#123; /** * LongWritable, IntWritable, Text 均是 Hadoop 中实现的用于封装 Java * 数据类型的类，这些类实现了WritableComparable接口， * 都能够被串行化从而便于在分布式环境中进行数据交换，你可以将它们分别视为long,int,String 的替代品。 */ private final static IntWritable one = new IntWritable(1); // 值为1 private Text word = new Text(); public void map(Object key, Text value, Context context) throws IOException, InterruptedException &#123; StringTokenizer itr = new StringTokenizer(value.toString()); // 对字符串进行切分 while (itr.hasMoreTokens()) &#123; word.set(itr.nextToken()); context.write(word, one); &#125; &#125; &#125; public static class IntSumReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt; &#123; private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable&lt;IntWritable&gt; values, Context context) throws IOException, InterruptedException &#123; int sum = 0; for (IntWritable val : values) &#123; sum += val.get(); &#125; result.set(sum); context.write(key, result); &#125; &#125; public static void main(String[] args) throws Exception &#123; Configuration conf = new Configuration();// String[] otherArgs = new GenericOptionsParser(conf, args).getRemainingArgs(); String[] otherArgs = &#123;&quot;/input&quot;, &quot;/output&quot;&#125;; if (otherArgs.length != 2) &#123; System.err.println(&quot;Usage: wordcount &lt;in&gt; &lt;out&gt;&quot;); System.exit(2); &#125; Job job = new Job(conf, &quot;word count&quot;); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.setInputDirRecursive(job, true); FileInputFormat.addInputPath(job, new Path(otherArgs[0])); FileOutputFormat.setOutputPath(job, new Path(otherArgs[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); &#125;&#125; 在Eclipse运行WordCount.java如前文所说，若要在hadoop上运行此project需要封装成jar包。若需要直接在Eclipse右键运行的话还需要做一下配置。运行 MapReduce 程序前，务必将 /usr/local/Cellar/hadoop/2.7.0/libexec/etc/hadoop 中将有修改过的配置文件（如伪分布式需要core-site.xml 和 hdfs-site.xml），以及log4j.properties复制到WordCount 项目下的src文件夹（~/workspace/WordCount/src）中。如下图 这是因为在使用 Eclipse 运行 MapReduce 程序时，会读取 Hadoop-Eclipse-Plugin 的Advanced parameters作为 Hadoop 运行参数，如果未进行修改，则默认的参数其实就是单机（非分布式）参数，因此程序运行时是读取本地目录而不是HDFS目录，就会提示Input路径不存在。报如下图错误配置好以上文件后，就可以运行普通java程序一样右键run了，不同的是所输出的结果会出现的HDFS上，而输出路径及输入数据集的路径在在源代码中String[] otherArgs指定。","raw":null,"content":null,"categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://hareric.com/tags/Hadoop/"}]},{"title":"《Hadoop权威指南》ch5 Data Integrity","slug":"《Hadoop权威指南》ch5 Data Integrity","date":"2019-01-24T17:53:05.000Z","updated":"2021-05-22T08:01:05.400Z","comments":true,"path":"2019/01/25/《Hadoop权威指南》ch5 Data Integrity/","link":"","permalink":"https://hareric.com/2019/01/25/%E3%80%8AHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E3%80%8Bch5%20Data%20Integrity/","excerpt":"Data Integrity(数据完整性)尽管磁盘和网络的每个I/O操作，对数据造成损坏的可能性都很低，但是如果系统中需要处理的数据量大到Hadoop的处理极限时，数据被损坏的概率还是很高的。","text":"Data Integrity(数据完整性)尽管磁盘和网络的每个I/O操作，对数据造成损坏的可能性都很低，但是如果系统中需要处理的数据量大到Hadoop的处理极限时，数据被损坏的概率还是很高的。 Checksum (校验和)在数据第一次引入的时候计算checksum，当数据在进行传输后再计算一次checksum，若不同则代表数据已损坏。其中，校验和作为数据也是有可能损坏的，但是相比于普通数据要小的多，所以损坏的可能性也非常小。 常用的错误检测码为CRC-32（32-bit cyclic redundancy check 32位循环冗余校验），任何大小的数据输入均计算得出一个32位的整数校验和。 Data Integrity in HDFS dfs.bytes-per-checksum指定计算校验和字节的长度，默认为512个字节，CRC-32校验和是4个字节，因此存储数据的额外开销低于1%。 datanode从客户端或者其它datanode收到数据后对数据进行验证。 每个datanode会保存检验和日志(persistent log of checksum verification)，用来保存每次验证的验证时间。 每个datanode也会在后台线程运行DataBlockScanner,定期验证该datanode上的数据块，该措施可用来解决数据的物理损坏。 每个HDFS存储着每个数据块的复本(replica),因此可以通过数据复本修复损坏的数据块。 在open()方法读取文件之前，可以将FileSystem实例的setVerifyChecksum(false)禁用校验和验证。该方法可以用来处理一些已损坏的数据，比如尝试是否能恢复部分数据。 123456789101112131415161718192021222324252627/** * Tests read/seek/getPos/skipped opeation for input stream. */private void testChecker(FileSystem fileSys, boolean readCS)throws Exception &#123; Path file = new Path(&quot;try.dat&quot;); writeFile(fileSys, file); try &#123; if (!readCS) &#123; fileSys.setVerifyChecksum(false); &#125; stm = fileSys.open(file); checkReadAndGetPos(); checkSeek(); checkSkip(); //checkMark assertFalse(stm.markSupported()); stm.close(); &#125; finally &#123; if (!readCS) &#123; fileSys.setVerifyChecksum(true); &#125; cleanupFile(fileSys, file); &#125;&#125;","raw":null,"content":null,"categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://hareric.com/tags/Hadoop/"}]},{"title":"《Hadoop权威指南》ch5 Compression","slug":"《Hadoop权威指南》ch5 Compression","date":"2019-01-22T17:53:05.000Z","updated":"2021-05-23T13:21:37.192Z","comments":true,"path":"2019/01/23/《Hadoop权威指南》ch5 Compression/","link":"","permalink":"https://hareric.com/2019/01/23/%E3%80%8AHadoop%E6%9D%83%E5%A8%81%E6%8C%87%E5%8D%97%E3%80%8Bch5%20Compression/","excerpt":"Compression (压缩)压缩的好处\n\n减少文件所需空间\n加速数据的在网络和磁盘上的传输","text":"Compression (压缩)压缩的好处 减少文件所需空间 加速数据的在网络和磁盘上的传输 Codec (Code-Decode)Codec是Hadoop中的压缩-解压算法的实现。在hadoop中，实现CompressionCodec接口的类代表一个codec，下表列举了Hadoop实现的codec。 CompressionCodecCompressionCode包含两个函数，用于压缩和解压缩数据。 压缩：通过createOutputStream(OutputStream out)方法获得CompressionOutputStream对象 解压：通过createInputStream(InputStream in)方法获得CompressionInputStream对象压缩的示例代码1234567891011121314151617181920package com.sweetop.styhadoop; import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.compress.CompressionCodec;import org.apache.hadoop.io.compress.CompressionOutputStream;import org.apache.hadoop.util.ReflectionUtils; public class StreamCompressor &#123; public static void main(String[] args) throws Exception &#123; String codecClassName = args[0]; Class&lt;?&gt; codecClass = Class.forName(codecClassName); Configuration conf = new Configuration(); CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, conf); CompressionOutputStream out = codec.createOutputStream(System.out); IOUtils.copyBytes(System.in, out, 4096, false); out.finish(); &#125;&#125; 从命令行接受一个CompressionCodec实现类的参数，然后通过ReflectionUtils把实例化这个类，调用CompressionCodec的接口方法对标准输出流进行封装，封装成一个压缩流，通过IOUtils类的copyBytes方法把标准输入流拷贝到压缩流中，最后调用CompressionCodec的finish方法，完成压缩。 CompressionCodecFactory如果你想读取一个被压缩的文件的话，首先你得先通过扩展名判断该用哪种codec。如前面的表可得，若文件名后缀为.gz，则需要用GzipCodec来读取。 也可以使用CompressionCodecFactory的getCodec()方法，通过传入一个Path,即可获得相应得codec。123456789101112131415161718192021222324252627282930313233343536373839404142package com.sweetop.styhadoop; import org.apache.hadoop.conf.Configuration;import org.apache.hadoop.fs.FileSystem;import org.apache.hadoop.fs.Path;import org.apache.hadoop.io.IOUtils;import org.apache.hadoop.io.compress.CompressionCodec;import org.apache.hadoop.io.compress.CompressionCodecFactory; import java.io.IOException;import java.io.InputStream;import java.io.OutputStream;import java.net.URI; public class FileDecompressor &#123; public static void main(String[] args) throws Exception &#123; String uri = args[0]; Configuration conf = new Configuration(); FileSystem fs = FileSystem.get(URI.create(uri), conf); Path inputPath = new Path(uri); CompressionCodecFactory factory = new CompressionCodecFactory(conf); CompressionCodec codec = factory.getCodec(inputPath); if (codec == null) &#123; System.out.println(&quot;No codec found for &quot; + uri); System.exit(1); &#125; String outputUri = CompressionCodecFactory.removeSuffix(uri, codec.getDefaultExtension()); InputStream in = null; OutputStream out = null; try &#123; in = codec.createInputStream(fs.open(inputPath)); out = fs.create(new Path(outputUri)); IOUtils.copyBytes(in,out,conf); &#125; finally &#123; IOUtils.closeStream(in); IOUtils.closeStream(out); &#125; &#125;&#125;注意看下removeSuffix方法，这是一个静态方法，它可以将文件的后缀去掉，然后我们将这个路径做为解压的输出路径。 CompressionCodecFactory能找到的code默认只有三种 org.apache.hadoop.io.compress.GzipCodec org.apache.hadoop.io.compress.BZip2Codec org.apache.hadoop.io.compress.DefaultCodec 如果想添加其他的codec你需要更改io.compression.codecs属性，并注册codec。 Native libraries(原生类库)通常情况下，使用Native libraries来实现解压缩会获得更高的性能。所有的解压缩格式都有原生类库，但并非都有java实现。如下表所示。 默认情况下，Hadoop会根据自身运行的平台搜索原生代码库并自动加载，无需要做配置。当需要禁用原生代码库时，需要将io.native.lib.available设置为false。 CodePool如果使用原生代码库执行大量解压缩，可以使用CodePool，支持反复使用，从而减少了创建对象的开销。123456789101112131415161718public class PooledStreamCompressor &#123; public static void main(String[] args) throws Exception &#123; String codecClassname = args[0]; Class&lt;?&gt; codecClass = Class.forName(codecClassname); Configuration conf = new Configuration(); CompressionCodec codec = (CompressionCodec) ReflectionUtils.newInstance(codecClass, conf); Compressor compressor = null; try &#123; compressor = CodecPool.getCompressor(codec); CompressionOutputStream out = codec.createOutputStream(System.out, compressor); IOUtils.copyBytes(System.in, out, 4096, false); out.finish(); &#125; finally &#123; CodecPool.returnCompressor(compressor); &#125; &#125; &#125;","raw":null,"content":null,"categories":[],"tags":[{"name":"Hadoop","slug":"Hadoop","permalink":"https://hareric.com/tags/Hadoop/"}]},{"title":"基于图的关键词抽取(HITS算法和PageRank算法)","slug":"基于图的关键词抽取","date":"2017-01-23T11:16:50.000Z","updated":"2021-05-22T08:23:47.377Z","comments":true,"path":"2017/01/23/基于图的关键词抽取/","link":"","permalink":"https://hareric.com/2017/01/23/%E5%9F%BA%E4%BA%8E%E5%9B%BE%E7%9A%84%E5%85%B3%E9%94%AE%E8%AF%8D%E6%8A%BD%E5%8F%96/","excerpt":"项目研究背景：　　在关键词抽取研究中，最常用的一种方法就是通过计算一篇文档中词语的TF-IDF值（term frequency-inverse document frequency）,并对它们进行排序选取TopK个作为关键词，这是一种无监督的方法。另外一种方法是通过有监督的方法，通过训练学习一个分类器，将关键词抽取问题转化为对每个词语的二分类问题，从而选择出合适的关键词。","text":"项目研究背景： 在关键词抽取研究中，最常用的一种方法就是通过计算一篇文档中词语的TF-IDF值（term frequency-inverse document frequency）,并对它们进行排序选取TopK个作为关键词，这是一种无监督的方法。另外一种方法是通过有监督的方法，通过训练学习一个分类器，将关键词抽取问题转化为对每个词语的二分类问题，从而选择出合适的关键词。 无监督和有监督各有各的优势和缺点：无监督学习，不需要人工标注，训练集合的过程，因此更加方便和快捷；然而监督学习的方法，在进行了机器学习后，分类器更具有判断多种信息对关键词影响的能力，效果更优。但在信息爆炸的网络时代，人工标注训练集是非常耗时耗力的事情，更何况网络文档的主题常常随着时间变化剧烈，为了使分类器随时能够适应网络就必须随时进行训练和标注，显然这是不现实的。因此关键词抽取的研究主要还是集中在无监督的方法。 在信息检索领域，HITS算法和PageRank算法是搜索引擎中两个最基础和最重要的算法。其基本思想都是首先利用网页之间的链接关系构建网页的网络图，通过相邻节点之间的网页的重要性，计算出某个网页的重要性。 受PageRank的启发，提出了一种基于图的排序算法TextRank，通过将文档看作一个词的网络，该网络中的链接表示词与词之间的语义关系，然后利用PageRank计算网络图中词的重要性，选取重要性TopK个词作为关键词。同理也可将最初用于网页排序的HITS算法用来抽取关键词。 项目具体思路：input: 一篇文档 output: k个关键词一．构建文档的词典 1. 去除停用词。 2. 消重。 3. 获得词典 vocab。 二．构建词-词之间的关联矩阵 matrix[i][j]: 表示i词与j词之间的权值。初始值为0 1. 文章已分好段落。 2. 在一段新闻中，若w1和w2共现，则matrix[w1][w2]++，matrix[w2][w1]++ 三．利用词-词之间的关联矩阵构建词语之间的无向拓扑图 四．评价节点重要性 计算拓扑图中每一个节点的PageRank值和Authority值用来评价节点的重要性，最后取TopK个节点作为本篇文章的关键词。 PageRank 算法算法流程 PageRank的思想在于将一个网页级别/重要性的排序问题转化成了一个公共参与、以群体民主投票的方式求解的问题，网页之间的链接即被认为是投票行为。同时，各个站点投票的权重不同，重要的网站投票具有较大的分量，而该网站是否重要的标准还需要依照其PageRank值。 现在假设有4个网站A、B、C、D，则它们的初始PageRank都是0.25。对于B而言的话，它把自己的总价值分散投给了A和C，各占一半的PageRank，即0.125，C和D的情况同理。即一个网站投票给其它网站PageRank的值，需要除以它所链接到的网站总数。此时PageRank的计算公式为： PR(A) = PR(B) / 2 + PR(C) / 1 + PR(D) / 3 PR(B) = PR(D) / 3 PR(C) = PR(B) / 2 + PR(D) / 3 PR(D) = 0 PageRank PR(A) PR(B) PR(C) PD(D) 初始值 0.25 0.25 0.25 0.25 第一次迭代后 0.4583 0.0833 0.2083 0 第二次迭代后 0.25 0 0.0417 0 第三次迭代后 0.0417 0 0 0 第四次迭代后 0 0 0 0 PageRank值计算过程的一般步骤可以概括如下：（1）为每个网站设置一个初始的PageRank值。（2）第一次迭代：每个网站得到一个新的PageRank。（3）第二次迭代：用这组新的PageRank再按上述公式形成另一组新的PageRank。（4）重复迭代直至PageRank值收敛 黑洞效应 上图中，顶点Ａ由于无出度像黑洞一样吸收所有PR值，最后导致所有PR值为0。为了防止这种情况的发生，有人给出了一种解决办法：即如果一个网站没有外链，那么就假定该连通图内其余所有的网点都是它的外链，这样我们就避免了整体PageRank值被吸收的现象。 马尔可夫链(马氏链， Markov Chain)针对以上该图，建立概率转移矩阵P，P[i][j]表示从顶点i到达顶点j的概率，那么矩阵表示就是： 0 0.5 0 0.5 0 0 1 0 0 0 0 1 0 1 0 0 我们所给定的初始向量是：(0.25 0.25 0.25 0.25)，做第一次迭代，就相当于用初始向量乘以上面的矩阵。第二次迭代就相当于第一次迭代的结果再乘以上面的矩阵……设转移概率矩阵为P，若存在正整数N，使得P^N&gt;0（每个元素大于0），这种链被称作正则链，它存在唯一的极限状态概率，并且与初始状态无关。 PageRank算法的马尔科夫过程分析假设{A, B, C}为马氏链，其转移概率矩阵如下所示： 0.7 0.1 0.2 0.1 0.8 0.1 0.05 0.05 0.9 因为该马氏链是不可约的非周期的有限状态，平稳分布存在，则我们要求其平衡分布为： X = 0.7X + 0.1Y + 0.05Z Y = 0.1X + 0.8Y + 0.05Z Z = 0.2X + 0.1Y + 0.9Z X + Y + Z = 1 解得上述方程组的平稳分布为：X = 0.1765，Y = 0.2353，Z = 0.5882。把PageRank收敛性问题转化为了求马尔可夫链的平稳分布的问题，那么我们就可以从马氏链的角度来分析问题。因此，对于PageRank的收敛性问题的证明也就迎刃而解了，只需要证明马氏链在什么情况下才会出现平稳分布即可。 HITS算法基本概念 所谓“Authority”页面，是指与某个领域或者某个话题相关的高质量网页，比如搜索引擎领域，Google和百度首页即该领域的高质量网页，比如视频领域，优酷和土豆首页即该领域的高质量网页。 所谓“Hub”页面，指的是包含了很多指向高质量“Authority”页面链接的网页，比如hao123首页可以认为是一个典型的高质量“Hub”网页。 基本假设 假设1：一个好的“Authority”页面会被很多好的“Hub”页面指向； 假设2：一个好的“Hub”页面会指向很多好的“Authority”页面； 算法流程 具体算法：可利用上面提到的两个基本假设，以及相互增强关系等原则进行多轮迭代计算，每轮迭代计算更新每个页面的两个权值，直到权值稳定不再发生明显的变化为止。 在网页排名中，HITS算法是与用户输入的查询请求密切相关的，即HITS算法在使用之前必须构建一个根集合，由于本次项目集中研究HITS算法在文档网络的运用价值，所以在此不详细说明。具体流程 一、a(i), h(i)分别表示结点i的Authority Score 和 Hub值（中心度） 二、在初始情况下，在没有更多可利用信息前，每个页面的这两个权值都是相同的，可以都设置为1，即：a(i)=1, h(i)=1 三、每次迭代计算Hub权值和Authority权值： 网页 a (i)在此轮迭代中的Authority权值即为所有指向网页 a (i)页面的Hub权值之和： a (i) = Σ h (i) ; 网页 a (i)的Hub分值即为所指向的页面的Authority权值之和： h (i) = Σ a (i) 。 对a (i)、h (i)进行规范化处理: 将所有网页的中心度都除以最高中心度以将其标准化： a (i) = a (i)/|a(i)| ； 将所有网页的权威度都除以最高权威度以将其标准化： h (i) = h (i)/ |h(i)| ： 四、收敛 上一轮迭代计算中的权值和本轮迭代之后权值的差异，如果发现总体来说权值没有明显变化，说明系统已进入稳定状态，则可以结束计算，即a ( u),h(v)收敛 。 实验实验一：新闻报道： 原标题：陈君文一行到长沙县调研两型住宅产业化工作 红网长沙县站3月17日讯(星沙时报记者廖真怡)昨日，湖南省人大常委会副主任陈君文率队来到长沙县，就两型住宅产业化工作开展实地调研。长沙县人大常委会主任李建章，副主任彭军其，副县长李开兴陪同调研。 在福临镇泉源村集中安置房项目现场，调研组一行参观了该示范点去年底新建完成的轻钢结构房屋。据介绍，轻钢结构房屋是以冷弯薄壁型钢为结构体系的绿色建筑，具备抗震防火、保温隔热、节能环保等诸多特点。一栋250平方米的轻钢结构房屋建成耗时不到两个月，村民只需花费约六十万元即可入住。 近年来，长沙县加快推进两型住宅产业化工作，逐步将保障性住房、棚户区改造、学校、医院等政府投资类建设项目纳入两型住宅产业化实施。目前，已启动福临镇泉源村、影珠山村住宅产业化的试点工作，建成4栋冷弯薄壁轻钢房屋，均选择了农民集中居住点进行统一规划设计，集中运用住宅产业化技术建设。 陈君文强调，轻钢结构房屋集环保、节能、高效等优点于一体，符合绿色发展理念，在农村住房改造中值得大力推广。政府要出台相关政策加以扶持，示范点则要积极发挥带头引领作用，通过政府、市场两手抓，推动项目又好又快发展。 利用textRank抽取关键词 轻钢 0.01902440798 房屋 0.01902440798 长沙县 0.0139585118277 两型 0.0139585118277 住宅 0.0139585118277 产业化 0.0139585118277 工作 0.0139585118277 福临 0.0136226056716 泉源 0.0136226056716 薄壁 0.0136226056716耗时：0.0591039657593ｓ 利用jieba包中内置基于tf-idf提取关键词 轻钢 0.386129934778 长沙县 0.309113581381 产业化 0.267338155519 两型 0.265661500064 住宅 0.234284985021 陈君文 0.199246125048 房屋 0.183773798317 泉源 0.136569330439 结构 0.13419387805 示范点 0.124362527232耗时：0.00664782524109ｓ 实验二：新闻报道： 原标题：美国宣布对朝鲜新的制裁措施 据新华社电 美国政府16日宣布对朝鲜实施新的制裁措施。 美国白宫的一份新闻公告说，总统奥巴马当天发布行政令，旨在冻结朝鲜政府和朝鲜劳动党资产，禁止与朝鲜的特定交易。 美国财政部随后发布新闻公告，对朝鲜17个政府官员和机构以及20艘船只实施制裁。 新制裁针对朝鲜的能源、矿业、金融服务和交通业，禁止对朝鲜的商品、服务、技术出口以及在朝鲜进行新的投资。 白宫的新闻公告说，美国将继续对朝鲜施压，令其付出“代价”，直至朝鲜最终履行国际责任和义务。 联合国安理会3月2日一致通过第2270号决议，针对朝鲜核、导计划规定一系列制裁措施，重申支持重启六方会谈及通过和平方式实现半岛无核化。 利用textRank抽取关键词 朝鲜 0.0324132510978 制裁 0.0239090726699 公告 0.0206482233926 新闻 0.0165485844427 禁止 0.0164814054247 措施 0.0158486641078 实施 0.0138236030274 美国 0.0133183833419 朝鲜劳动党 0.012182643101 资产 0.012182643101耗时: 0.0261380672455ｓ 利用jieba包中内置基于tf-idf提取关键词 朝鲜 0.645829143617 制裁 0.427122149321 公告 0.161725613834 措施 0.161613619075 禁止 0.133688805542 针对 0.118448597479 20 0.11720360297 17 0.11720360297 16 0.11720360297 2270 0.11720360297耗时: 0.00232791900635ｓ 实验总结 本次实验主要针对新浪网中近100条新闻作为实验的测试集，通过人工统计，对比评价基于图的关键词提取法（textRank）以及基于tf-idf的关键词的抽取法。其中选取了最具代表性2篇新闻作为本次的实验的实验样本。 实验一的关键词提取效果中，textRank和tf-idf均能较好提取出关键词，二个算法之间的关键词提取效果也并没有太大的差异。然而二者之间的耗时却有着明显的差异。由于textRank算法中涉及到网络图的构建以及多轮迭代，所以性能上普遍比tf-idf要低上一截。（jieba包中已包含了常见词语的idf词语，因此不需要大量文档进行学习） 实验二的关键词的提取效果中就有着明显差异，由于jieba包内置的tf-idf关键词提取算法，并不存在实时标注和学习的过程，在面对不同的主题的新闻时，关键词的提取效果就有些差异。如实验二中，textRank所抓取的10个关键词都较符合新闻的主题，而tf-idf所提取的就有少些无实际意义的词作为关键词。 总结，基于图的关键词的提取中算法，较于传统的tf-idf关键词提取效果有一定的提高。然而在本次实验室中的100多个测试集中，粗略统计，大多数新闻的关键词的提取中，tf-idf的关键词提取效果与textRank相比，并无多大差异。因此我觉得在实际应用中，还需要综合各种情况选取合适的算法。","raw":null,"content":null,"categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://hareric.com/tags/python/"},{"name":"算法","slug":"算法","permalink":"https://hareric.com/tags/%E7%AE%97%E6%B3%95/"}]},{"title":"python实用装饰器代码","slug":"python实用装饰器代码","date":"2016-11-20T10:09:18.000Z","updated":"2021-05-22T08:01:05.400Z","comments":true,"path":"2016/11/20/python实用装饰器代码/","link":"","permalink":"https://hareric.com/2016/11/20/python%E5%AE%9E%E7%94%A8%E8%A3%85%E9%A5%B0%E5%99%A8%E4%BB%A3%E7%A0%81/","excerpt":"　　python装饰器是一种可以用来对函数进行装饰的面向对象的方法。　　当想要对一个已有的模块做一些“修饰工作”，所谓修饰工作就是想给现有的模块加上一些小装饰（一些小功能，这些小功能可能好多模块都会用到），但又不让这个小装饰（小功能）侵入到原有的模块中的代码里去。我们便可以写成一个装饰器，哪些函数需要哪些修饰功能就可以直接在函数前套用装饰器。","text":"python装饰器是一种可以用来对函数进行装饰的面向对象的方法。 当想要对一个已有的模块做一些“修饰工作”，所谓修饰工作就是想给现有的模块加上一些小装饰（一些小功能，这些小功能可能好多模块都会用到），但又不让这个小装饰（小功能）侵入到原有的模块中的代码里去。我们便可以写成一个装饰器，哪些函数需要哪些修饰功能就可以直接在函数前套用装饰器。 type_limit用途 限制函数的参数和返回值的类型装饰器代码1234567891011121314151617181920212223242526class LimitError(Exception): pass def type_limit(*input_type, **return_type): def test_value_type(func): def wrapper(*args, **kwargs): length = len(input_type) if length != len(args): raise LimitError(&quot;The list of typeLimit and param &quot; &quot;must have the same length&quot;) for index in range(length): t = input_type[index] p = args[index] if not isinstance(p, t): raise LimitError(&quot;The param %s should be %s,&quot; &quot;but it&#x27;s %s now!&quot; % (str(p), t, type(p))) res = func(*args, **kwargs) if &quot;return_type&quot; in return_type: limit = return_type[&quot;return_type&quot;] if not isinstance(res, limit): raise LimitError( &quot;This function must return a value that is %s,&quot; &quot;but now it&#x27;s %s&quot; % (limit, type(res))) return res return wrapper return test_value_type使用方法123@type_limit(int, int, return_type=int)def add(a, b): return a + b exe_time用途 输出函数运行时间装饰器代码12345678910111213141516171819import timedef exe_time(func): def wrapper(*args, **kwargs): t0 = time.time() sys.stdout.write(&#x27;\\033[0;32;0m&#x27;) print &quot;-----------------------------------&quot; print &quot;%s,function \\033[4;32;0m%s()\\033[0m\\033[0;32;0m start&quot; \\ % (time.strftime(&quot;%X&quot;, time.localtime()), func.__name__) sys.stdout.write(&#x27;\\033[0m&#x27;) res = func(*args, **kwargs) sys.stdout.write(&#x27;\\033[0;32;0m&#x27;) print &quot;%s,function \\033[4;32;0m%s()\\033[0m\\033[0;32;0m end&quot; \\ % (time.strftime(&quot;%X&quot;, time.localtime()), func.__name__) print &quot;%.3fs taken for function \\033[4;32;0m%s()\\033[0m\\033[0;32;0m&quot; \\ % (time.time() - t0, func.__name__) print &quot;-----------------------------------&quot; sys.stdout.write(&#x27;\\033[0m&#x27;) return res return wrapper使用方法1234@exe_timedef func(): for i in range(100000): pass","raw":null,"content":null,"categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://hareric.com/tags/python/"}]},{"title":"HMM+Vetebi算法实现词性标注 python实现","slug":"HMM-Vetebi算法实现词性标注-python实现","date":"2016-10-30T14:01:52.000Z","updated":"2021-05-23T13:21:37.047Z","comments":true,"path":"2016/10/30/HMM-Vetebi算法实现词性标注-python实现/","link":"","permalink":"https://hareric.com/2016/10/30/HMM-Vetebi%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E8%AF%8D%E6%80%A7%E6%A0%87%E6%B3%A8-python%E5%AE%9E%E7%8E%B0/","excerpt":"　　隐马尔可夫模型（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。　　在正常的马尔可夫模型中，状态对于观察者来说是直接可见的。这样状态的转换概率便是全部的参数。而在隐马尔可夫模型中,状态并不是直接可见的，但受状态影响的某些变量则是可见的。每一个状态在可能输出的符号上都有一概率分布。因此输出符号的序列能够透露出状态序列的一些信息。","text":"隐马尔可夫模型（Hidden Markov Model，HMM）是统计模型，它用来描述一个含有隐含未知参数的马尔可夫过程。其难点是从可观察的参数中确定该过程的隐含参数。然后利用这些参数来作进一步的分析，例如模式识别。 在正常的马尔可夫模型中，状态对于观察者来说是直接可见的。这样状态的转换概率便是全部的参数。而在隐马尔可夫模型中,状态并不是直接可见的，但受状态影响的某些变量则是可见的。每一个状态在可能输出的符号上都有一概率分布。因此输出符号的序列能够透露出状态序列的一些信息。 马尔科夫模型 （Markov Model，HMM） 在考虑隐马尔科夫模型之前，我们首先要了解马尔可夫模型。马尔可夫模型描述了一类重要的随机过程，这个随机过程是随时间而随机变化的过程。我们常会考虑一个并不互相独立的随机变量组成的序列，序列中每个变量的值依赖于它前面的元素。简单来说： 即在特定条件下，系统在时间t的状态只与其在时间t-1的状态相关该随机过程称为一阶马尔可夫过程。 隐马尔科夫模型（Hidden Markov Model，HMM） 如果知道某个事件的观察序列，是可以使用一个马尔可夫模型来计算；但是，有时候有些事件是不可以直接观测到的。例如，本篇博文要讲的词性标注这个例子： 字串(可观察序列)： 结合 / 成 / 分子 / 时 字的词性(隐序列)： vn,v / v,nr,q,a,an,j / n / Ng,nr,Dg / HMM就是估算隐藏于表面事件背后的事件的概率。 转移概率矩阵 转移概率指的是估计的事件的序列之间的概率关系，上诉例子中，我们所需要的转移概率为P(v|vn)p(v|v)…P(v|n)表示前一个词的词性是名词，则这个词为动词的概率。通过训练集的统计我们可以统计得出转移概率矩阵。即词性转移矩阵： v n s ｖ 0.5 0.375 0.125 ｎ 0.25 0.125 0.625 ｓ 0.25 0.375 0.375 发射概率矩阵 发射概率指的是，隐藏序列和可观察序列之间的概率关系，上述例子中，我们需要的发射概率为P(结合｜v)…表示“结合”这个词为动词的概率。通过训练集的统计，我们同样也能够得出发射概率矩阵： v n s 结合 0.5 0.375 0.125 成 0.25 0.125 0.625 分子 0.25 0.375 0.375 时 0.25 0.375 0.375 ps 上面矩阵的数据都是乱写的 计算观测序列的概率 有了转移矩阵和概率矩阵，我们终于可以来预测句子中每个词的词性。最直接易懂的方法便是穷举法，我们通过求每个词每个词性每种情况的概率，取最大的概率作为我们预测的词性标注。 例如上述例子： 字串： 结合 / 成 / 分子 / 时 字的词性： vn,v / v,nr,q,a,an,j / n / Ng,nr,Dg / 第一种可能的词性序列是：vn,v,n,Ng 即: P(结合,成,分子,时,vn,v,n,Ng) =p(结合|vn)×p(成|v)×p(分子|n)×p(时|Ng)×p(vn|start)×p(v|vn)×p(n|v)×p(Ng|n) Vetebi算法 如果直接使用上述的穷举法去寻找最优的概率，毋庸置疑该算法的复杂度是相当复杂的，例如上述例子仅有４个词却总共有36种情况需要考虑，也就是说我们需要分别计算36种情况的概率，然后取最大值作为我们的预测结果。那么我们有没有什么可以简化的方法吗？ 因此在算法优化上，我们可以引用维特比算法(Vetebi)。维特比算法是现代数字通信中使用最频繁的算法，同时也是很多自然语言处理的解码算法。 算法描述：依据最后一个时刻中概率最高的状态，逆向通过找其路径中的上一个最大部分最优路径，从而找到整个最优路径。 vt(j)是所有序列中在t时刻以状态j终止的最大概率,所对应的路径为部分最优路径。 实例 还是以“结合 成 分子 时”为例子，在计算概率的途中，我们可以理解为不断寻找最优解的过程。 我们可以首先考虑“结合 成”这2个词的词性计算词性概率，对于“成”的6个词性分别都有一个最大的概率和对应的前置词性如图所示： 接着考虑“分子”的词性为n，它的最优前置词性为an，如图所示： 最后考虑“时”的词性Ng,nr,Dg，分别计算得最后的概率为0.1，0.2，0.3；如图所示： 取最大概率Dg为0.3，从后往前将全局最优路径导出，如图所示： 最后我们可以得出最终结果: 结合/v; 成/an; 分子/n; 时/Dg python代码实现核心代码123456789101112131415161718192021222324252627282930313233343536def hmm(self, sentence_list): &quot;&quot;&quot; :param sentence_list: 已分好词的句子列表 :return: 对应每个词的词性列表 &quot;&quot;&quot; sentence_list = list(sentence_list) sentence_len = sentence_list.__len__() # 句子长度 cixin_len = self.cixin_list.__len__() # 词性个数 # 概率分布表 .[i, j, 0]表示第i个词为第j个词性的最优概率;.[i, j, 1]表示该最优概率的前一个词的词性索引,若为-1表示该词为第一个词无前词 pro_table = np.zeros((sentence_len, cixin_len, 2)) try: pro_table[0, :, 0] = self.emitter_pro_matrix[self.vocab_map[sentence_list[0]]] pro_table[0, :, 1] = -1 for i in range(sentence_len)[1:]: for j in range(cixin_len): if self.emitter_pro_matrix[self.vocab_map[sentence_list[i]], j] == 0: continue pre_cixin_pro = pro_table[i-1, :, 0] pre_cixin_pro *= self.trans_pro_matrix[j] pre_cixin_pro *= self.emitter_pro_matrix[self.vocab_map[sentence_list[i]], j] pro_table[i, j, 0] = np.max(pre_cixin_pro) pro_table[i, j, 1] = np.where(pre_cixin_pro == np.max(pre_cixin_pro))[0][0] result_cixin_map = [] sy = int(np.where(pro_table[-1, :, 0] == np.max(pro_table[-1, :, 0]))[0][0]) t = -1 except KeyError: return &quot;无法正常运行 有词语不存在词库之中&quot; while sy != -1: result_cixin_map.append(sy) sy = int(pro_table[t, sy, 1]) t -= 1 result_cixin = [] for s in result_cixin_map[::-1]: result_cixin.append(self.cixin_list[s]) return result_cixin 完整代码及数据下载","raw":null,"content":null,"categories":[],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://hareric.com/tags/NLP/"},{"name":"python","slug":"python","permalink":"https://hareric.com/tags/python/"}]},{"title":"Java DBhelper类","slug":"Java-DBhelper类","date":"2016-10-11T13:55:54.000Z","updated":"2021-05-22T08:01:05.399Z","comments":true,"path":"2016/10/11/Java-DBhelper类/","link":"","permalink":"https://hareric.com/2016/10/11/Java-DBhelper%E7%B1%BB/","excerpt":"　　简单用Java写的连接mysql的DBhealper类，包含了基本的数据库连接，增删查改等功能。","text":"简单用Java写的连接mysql的DBhealper类，包含了基本的数据库连接，增删查改等功能。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355356357358359360361362363364365366367368369370371372373374375376377378379380381382import java.sql.*;/*** @author Eric_Chan* @version 2016.10.11*/public class DBhelper &#123; private String ipAddress = &quot;127.0.0.1&quot;; private int port = 3306; private String user = &quot;&quot;; private String pwd = &quot;&quot;; private String dbName = &quot;&quot;; private Connection conn; private static DBhealper instance = null; // 单例 /** * 检查是否导入 mysql-connector-java.jar */ private DBhealper() &#123; try &#123; Class.forName(&quot;com.mysql.jdbc.Driver&quot;); &#125; catch (ClassNotFoundException e) &#123; System.out.println(&quot;没有正确导入 connector.jar\\n 下载地址http://dev.mysql.com/downloads/connector/j/&quot;); &#125; &#125; /** * 获得唯一连接器 * @return DbConnection */ public static DBhealper getInstance() &#123; if (DBhealper.instance == null) &#123; instance = new DBhealper(); return DBhealper.instance; &#125; else &#123; return DBhealper.instance; &#125; &#125; /** * 连接数据库 */ private void connect() throws SQLException &#123; String url = String.format(&quot;jdbc:mysql://%s:%s/%s&quot;,this.ipAddress, this.port, this.dbName); try &#123; this.conn = DriverManager.getConnection(url, this.user, this.pwd); &#125; catch (SQLException e) &#123; System.out.println(&quot;连接失败\\n&quot;); e.printStackTrace(); &#125; &#125; /** * 提供参数 并连接数据库 默认为本地数据库，端口为3306 * @param user 用户名 * @param pwd 密码 * @param dbName 数据库名 */ public void connSQL(String user, String pwd, String dbName) throws SQLException &#123; this.user = user; this.pwd = pwd; this.dbName = dbName; this.connect(); System.out.println(&quot;连接成功\\nconn-------------&quot; + conn + &#x27;\\n&#x27;); this.free(this.conn, null, null); &#125; public void connSQL(String user, String pwd, String dbName, int port) throws SQLException &#123; this.port = port; this.connSQL(user, pwd, dbName); &#125; public void connSQL(String user, String pwd, String dbName, String ipAddress) throws SQLException &#123; this.ipAddress = ipAddress; this.connSQL(user, pwd, dbName); &#125; public void connSQL(String user, String pwd, String dbName,String ipAddress, int port) throws SQLException &#123; this.ipAddress = ipAddress; this.port = port; this.connSQL(user, pwd, dbName); &#125; /** * 将表内数据输出至控制台 * @param tableName 表名 * @throws Exception */ public void showTable(String tableName) throws SQLException &#123; this.connect(); Statement stmt = null; ResultSet rs = null; try &#123; String sql = &quot;SELECT * FROM &quot; + tableName; stmt = conn.createStatement(); rs = stmt.executeQuery(sql); DBhealper.showResultSet(rs); &#125; catch(SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; this.free(this.conn, stmt, rs); &#125; &#125; /** * 查【Query】 * 无参查找 * @param sql * @return ResultSet */ public ResultSet executeQuery(String sql) throws SQLException &#123; this.connect(); Statement stmt = null; ResultSet rs = null; try &#123; stmt = conn.createStatement(); rs = stmt.executeQuery(sql); &#125; catch (SQLException e) &#123; e.printStackTrace(); this.free(conn, stmt, rs); &#125; return rs; &#125; /** * 查【Query】 * 有参查找 * @param sql * @param obj * @return ResultSet */ public ResultSet executeQuery(String sql, Object... obj) throws SQLException &#123; this.connect(); PreparedStatement pstmt = null; ResultSet rs = null; try &#123; pstmt = conn.prepareStatement(sql); for (int i = 0; i &lt; obj.length; i++) &#123; pstmt.setObject(i + 1, obj[i]); &#125; rs = pstmt.executeQuery(); &#125; catch (SQLException e) &#123; e.printStackTrace(); this.free(conn, pstmt, rs); &#125; return rs; &#125; /** * 更新【update】 无参更新 * * @param sql * @return int */ public int executeUpdate(String sql) &#123; this.connect(); Statement stmt = null; int rs = 0; try &#123; stmt = conn.createStatement(); rs = stmt.executeUpdate(sql); &#125; catch (SQLException e) &#123; e.printStackTrace(); this.free(conn, stmt, null); &#125; return rs; &#125; /** * 更新【update】 有参更新 * * @param sql * @param obj * @return int */ public int executeUpdate(String sql, Object... obj) &#123; this.connect(); PreparedStatement pstmt = null; int rs = 0; try &#123; pstmt = conn.prepareStatement(sql); for (int i = 0; i &lt; obj.length; i++) &#123; pstmt.setObject(i + 1, obj[i]); &#125; rs = pstmt.executeUpdate(); &#125; catch (SQLException e) &#123; e.printStackTrace(); this.free(conn, pstmt, null); &#125; return rs; &#125; /** * 将ResultSet内的数据输出至控制台 * @param rs * @throws SQLException */ public static void showResultSet(ResultSet rs) throws SQLException &#123; ResultSetMetaData rsmd = rs.getMetaData(); int columnCount = rsmd.getColumnCount(); // 输出列名 for (int i=1; i&lt;=columnCount; i++) &#123; System.out.print(rsmd.getColumnName(i)); System.out.print(&quot;(&quot; + rsmd.getColumnTypeName(i) + &quot;)&quot;); System.out.print(&quot; | &quot;); &#125; System.out.println(); // 输出数据 while (rs.next()) &#123; for (int i=1; i&lt;=columnCount; i++) &#123; System.out.print(rs.getString(i) + &quot; | &quot;); &#125; System.out.println(); &#125; &#125; /** * 判断记录是否存在 * * @param sql * @return Boolean */ public Boolean isExist(String sql) throws SQLException &#123; this.connect(); Statement stmt = null; Boolean isEx = false; ResultSet rs = null; try &#123; stmt = conn.createStatement(); rs = stmt.executeQuery(sql); rs.last(); isEx = rs.getRow()&gt;0; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; this.free(conn, stmt, rs); &#125; return isEx; &#125; /** * 判断记录是否存在 * @param sql * @return Boolean */ public Boolean isExist(String sql, Object... obj) throws SQLException &#123; this.connect(); PreparedStatement pstmt = null; Boolean isEx = false; ResultSet rs = null; try &#123; pstmt = conn.prepareStatement(sql); for (int i = 0; i &lt; obj.length; i++) &#123; pstmt.setObject(i + 1, obj[i]); &#125; rs = pstmt.executeQuery(); rs.last(); isEx = rs.getRow()&gt;0; &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; finally &#123; this.free(conn, pstmt, rs); &#125; return isEx; &#125; /** * 用来释放所有数据资源 * @param conn * @param stmt * @param rs * @throws SQLException */ public void free(Connection conn, Statement stmt, ResultSet rs) throws SQLException &#123; if (conn!=null) &#123; try &#123; conn.close(); &#125; catch(SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (stmt!=null) &#123; try &#123; stmt.close(); &#125; catch(SQLException e) &#123; e.printStackTrace(); &#125; &#125; if (rs!=null) &#123; try &#123; rs.close(); &#125; catch(SQLException e) &#123; e.printStackTrace(); &#125; &#125; &#125; public void close() &#123; try &#123; this.conn.close(); &#125; catch (SQLException e) &#123; e.printStackTrace(); &#125; &#125; public static void main(String args[]) throws SQLException &#123; DBhealper connector = DBhealper.getInstance(); connector.connSQL(&quot;root&quot;, &quot;1q2w3e&quot;, &quot;student&quot;, 3307); // connector.showTable(&quot;student&quot;); //ResultSet rs = connector.executeQuery(&quot;SELECT * FROM student WHERE st_name=? and st_Password=?&quot;, &quot;Jack&quot;, &quot;1234&quot;); //DBhealper.showResultSet(rs); Boolean isEx = connector.isExist(&quot;SELECT * FROM student WHERE st_name=? and st_Password=?&quot;, &quot;Jack&quot;, &quot;1234&quot;); System.out.println(isEx); connector.close(); &#125; &#125;","raw":null,"content":null,"categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"https://hareric.com/tags/Java/"},{"name":"数据库","slug":"数据库","permalink":"https://hareric.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"}]},{"title":"基于规则的分词算法(MM RMM算法)及python代码实现","slug":"基于规则的分词算法(MM RMM算法)及python代码实现","date":"2016-10-08T15:39:29.000Z","updated":"2021-05-23T13:21:37.192Z","comments":true,"path":"2016/10/08/基于规则的分词算法(MM RMM算法)及python代码实现/","link":"","permalink":"https://hareric.com/2016/10/08/%E5%9F%BA%E4%BA%8E%E8%A7%84%E5%88%99%E7%9A%84%E5%88%86%E8%AF%8D%E7%AE%97%E6%B3%95(MM%20RMM%E7%AE%97%E6%B3%95)%E5%8F%8Apython%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/","excerpt":"一、理论描述　　中文分词指的是将一个汉字序列切分成一个一个单独的词。现有的分词算法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。其中，基于字符串匹配的分词方法是按照一定的策略将待分析的汉字串与机器词典中的词条进行匹配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。按照扫描方向的不同，基于字符串匹配的分词方法可以分为正向最大匹配算法（Maximum Matching，下面简称MM）和逆向最大匹配算法（Reverse Maximum Matching，下面简称RMM）。　　本分词系统只是一个简单的演示程序，简单地根据语料库的信息，利用“正向最大匹配”、“逆向最大匹配算法”将一小段文字进行分词。","text":"一、理论描述 中文分词指的是将一个汉字序列切分成一个一个单独的词。现有的分词算法可分为三大类：基于字符串匹配的分词方法、基于理解的分词方法和基于统计的分词方法。其中，基于字符串匹配的分词方法是按照一定的策略将待分析的汉字串与机器词典中的词条进行匹配，若在词典中找到某个字符串，则匹配成功（识别出一个词）。按照扫描方向的不同，基于字符串匹配的分词方法可以分为正向最大匹配算法（Maximum Matching，下面简称MM）和逆向最大匹配算法（Reverse Maximum Matching，下面简称RMM）。 本分词系统只是一个简单的演示程序，简单地根据语料库的信息，利用“正向最大匹配”、“逆向最大匹配算法”将一小段文字进行分词。 二、算法描述➢ 正向最大匹配算法（MM）：按照人得而自然阅读顺序从左往右对一段话甚至文章进行词库匹配切分。设MaxLen为最大词长,D为分词词典（1）从待切分语料中按正向取长度为MaxLen的字符串str，令Len =MaxLen；（2）将str与D中的词语互相匹配；（3）if匹配成功，将指针向前移Len个汉字，并返回到（1）；（4）if 不成功： if（ Len&gt;1）： Len = Len-1； 从待切分语料中取长度为Len的字符串str，并返回（2）； else： 得到单个汉字，指针向前移一个汉字，并返回（1） ➢ 逆向最大匹配算法（RMM）：主要原理与正向最大匹配算法一致，只是切分方向相反，从文章的尾部开始匹配。 三、python 代码实现MM算法12345678910111213141516171819def mm_cut(self, sentence=&#x27;&#x27;, max_len=6): &quot;&quot;&quot; 使用正向最大匹配法划分词语 :param sentence: str 待划分句子 :param max_len: int 最大词长 默认为6 :return: str-list 已分词的字符串列表 &quot;&quot;&quot; sentence = sentence.decode(&#x27;utf-8&#x27;) cur = 0 # 表示分词的位置 sen_len = sentence.__len__() # 句子的长度 word_list = [] # 划分的结果 while cur &lt; sen_len: l = None for l in range(max_len, 0, -1): if sentence[cur: cur+l] in self.word_set: break word_list.append(sentence[cur: cur+l]) cur += l return word_list RMM算法123456789101112131415161718192021def rmm_cut(self, sentence=&#x27;&#x27;, max_len=6): &quot;&quot;&quot; 使用逆向最大匹配法划分词语 :param sentence: str 待划分句子 :param max_len: int 最大词长 默认为6 :return: str-list 已分词的字符串列表 &quot;&quot;&quot; sentence = sentence.decode(&#x27;utf-8&#x27;) sen_len = sentence.__len__() # 句子的长度 cur = sen_len # 表示分词的位置 word_list = [] # 划分的结果 while cur &gt; 0: l = None if max_len &gt; cur: max_len = cur for l in range(max_len, 0, -1): if sentence[cur-l: cur] in self.word_set: break word_list.insert(0, sentence[cur-l: cur]) cur -= l return word_list 附软件示意图 源码下载","raw":null,"content":null,"categories":[],"tags":[{"name":"NLP","slug":"NLP","permalink":"https://hareric.com/tags/NLP/"},{"name":"python","slug":"python","permalink":"https://hareric.com/tags/python/"}]},{"title":"FTP带用户名密码的访问路径","slug":"FTP带用户名密码的访问路径","date":"2016-09-25T07:02:03.000Z","updated":"2021-05-22T08:01:05.398Z","comments":true,"path":"2016/09/25/FTP带用户名密码的访问路径/","link":"","permalink":"https://hareric.com/2016/09/25/FTP%E5%B8%A6%E7%94%A8%E6%88%B7%E5%90%8D%E5%AF%86%E7%A0%81%E7%9A%84%E8%AE%BF%E9%97%AE%E8%B7%AF%E5%BE%84/","excerpt":"","text":"不带密码的ftp路径格式：ftp://url/目录/文件带密码的ftp路径格式：ftp://username:password@ip/目录/文件","raw":null,"content":null,"categories":[],"tags":[{"name":"其他","slug":"其他","permalink":"https://hareric.com/tags/%E5%85%B6%E4%BB%96/"}]},{"title":"基于SNN密度的聚类及python代码实现","slug":"基于SNN密度的聚类及python代码实现","date":"2016-09-01T14:19:26.000Z","updated":"2021-05-23T13:39:37.267Z","comments":true,"path":"2016/09/01/基于SNN密度的聚类及python代码实现/","link":"","permalink":"https://hareric.com/2016/09/01/%E5%9F%BA%E4%BA%8ESNN%E5%AF%86%E5%BA%A6%E7%9A%84%E8%81%9A%E7%B1%BB%E5%8F%8Apython%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/","excerpt":"　　在某些情况下，依赖于相似度和密度的标准方法的聚类技术不能产生理想的聚类效果。","text":"在某些情况下，依赖于相似度和密度的标准方法的聚类技术不能产生理想的聚类效果。 存在的问题1.传统的相似度在高维数据上的问题 传统的欧几里得密度在高维空间变得没有意义。特别在文本处理之中，以分词作为特征，数据的维度将会非常得高，文本与文本之间的相似度低并不罕见。然而许多文档都有着不同类的最近邻，虽然近邻之间相似度虽然高，然而却不是同一类文档。由此可以看出，传统的相似度成为了不可靠的指导。 2.密度不同的问题 传统的基于密度的聚类问题，虽然能够有效的发现不同形状的簇，然而由于在计算密度的时候是通过计算之间的欧式距离确定密度的，若存在密度差别较大的簇将无法很好地识别。 如下图，显示了一对具有不同密度点的二维簇。右边的簇尽管不太稠密，但形成了同样合法的簇。 若使用传统的dbscan聚类算法(sklearn开源包)，将获得以下聚类效果，其中黑色点表示噪声。 SNN相似度 为了解决上诉问题，提出了共享最近邻(Shared Nearest Neighbor, SNN)相似度。如字面意思，通过计算2个点之间共享的近邻个数，确定两点之间的相似度。算法流程12345找出所有点的k-最近邻if 两个点x和y不是在对方的k-最近邻中 then similarity(x, y) = 0else similarity(x, y) = 共享的近邻个数 SNN相似度只依赖于两个对象共享的最近邻的个数，而不是这些近邻之间相距多远。这样一来相似度关于点的密度自动进行缩放 SNN密度 由于SNN相似性度量反映了数据空间中点的局部结构，因此它对密度的变化和空间的维度相对不太敏感，所以可以用来做新的密度度量。定义如下核心点： 在给定邻域(Eps)内的点数超过某个阈值(MinPts)边界点： 不属于核心点，但在某个核心点的邻域内。噪声点： 既非核心点，也非边界点的噪声。 可以理解为，SNN密度度量一个点被类似的点包围的程度。 基于SNN密度的聚类 将上面定义的SNN密度与dbScan算法结合起来，就可以得出一种新的聚类算法算法流程12计算SNN相似度图以用户指定的参数Eps和MinPts，使用dbScan算法 以上面的数据集为例，使用该聚类算法得出以下结果。 具体python代码实现，使用了开源包sklearn的kd-tree以及dbScan算法。1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374import numpy as npfrom sklearn.neighbors import NearestNeighborsfrom itertools import combinationsimport timedef snn_sim_matrix(X, k=5): &quot;&quot;&quot; 利用sklearn包中的KDTree,计算节点的共享最近邻相似度(SNN)矩阵 :param X: array-like, shape = [samples_size, features_size] :param k: positive integer(default = 5), 计算snn相似度的阈值k :return: snn距离矩阵 &quot;&quot;&quot; try: X = np.array(X) except: raise ValueError(&quot;输入的数据集必须为矩阵&quot;) samples_size, features_size = X.shape # 数据集样本的个数和特征的维数 nbrs = NearestNeighbors(n_neighbors=k, algorithm=&#x27;kd_tree&#x27;).fit(X) knn_matrix = nbrs.kneighbors(X, return_distance=False) # 记录每个样本的k个最近邻对应的索引 sim_matrix = 0.5 + np.zeros((samples_size, samples_size)) # snn相似度矩阵 for i in range(samples_size): t = np.where(knn_matrix == i)[0] c = list(combinations(t, 2)) for j in c: if j[0] not in knn_matrix[j[1]]: continue sim_matrix[j[0]][j[1]] += 1 sim_matrix = 1 / sim_matrix # 将相似度矩阵转化为距离矩阵 sim_matrix = np.triu(sim_matrix) sim_matrix += sim_matrix.T - np.diag(sim_matrix.diagonal()) return sim_matrixif __name__ == &#x27;__main__&#x27;: from sklearn.cluster import DBSCAN from sklearn.datasets.samples_generator import make_blobs import matplotlib.pyplot as plt # 构建数据集 centers = [37, 4] centers_2 = [-37, 4] X, labels_true = make_blobs(n_samples=100, centers=centers, cluster_std=20) X_2, l_2 = make_blobs(n_samples=50, cluster_std=8, centers=centers_2) X = np.concatenate((X, X_2)) # 基于snn相似度的聚类 t1 = time.time() sim_matrix = snn_sim_matrix(X, k=8) t2 = time.time() print &quot;the time of creating sim matrix is %.5fs&quot; % (t2 - t1) t1 = time.time() db = DBSCAN(eps=0.5, min_samples=5, metric=&#x27;precomputed&#x27;).fit(sim_matrix) t2 = time.time() print &quot;the time of clustering is %.5fs&quot; % (t2 - t1) # 构图 core_samples_mask = np.zeros_like(db.labels_, dtype=bool) core_samples_mask[db.core_sample_indices_] = True labels = db.labels_ n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0) unique_labels = set(labels) colors = plt.cm.Spectral(np.linspace(0, 1, len(unique_labels))) for k, col in zip(unique_labels, colors): if k == -1: # Black used for noise. col = &#x27;k&#x27; class_member_mask = (labels == k) xy = X[class_member_mask &amp; core_samples_mask] plt.plot(xy[:, 0], xy[:, 1], &#x27;o&#x27;, markerfacecolor=col, markeredgecolor=&#x27;k&#x27;, markersize=10) xy = X[class_member_mask &amp; ~core_samples_mask] plt.plot(xy[:, 0], xy[:, 1], &#x27;o&#x27;, markerfacecolor=col, markeredgecolor=&#x27;k&#x27;, markersize=6) plt.title(&#x27;SNN&#x27;) plt.show()","raw":null,"content":null,"categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://hareric.com/tags/python/"},{"name":"聚类","slug":"聚类","permalink":"https://hareric.com/tags/%E8%81%9A%E7%B1%BB/"}]},{"title":"简易方法解决ubuntu下解压文件乱码","slug":"简易方法解决ubuntu下解压文件乱码","date":"2016-07-26T08:56:49.000Z","updated":"2021-05-22T08:01:05.401Z","comments":true,"path":"2016/07/26/简易方法解决ubuntu下解压文件乱码/","link":"","permalink":"https://hareric.com/2016/07/26/%E7%AE%80%E6%98%93%E6%96%B9%E6%B3%95%E8%A7%A3%E5%86%B3ubuntu%E4%B8%8B%E8%A7%A3%E5%8E%8B%E6%96%87%E4%BB%B6%E4%B9%B1%E7%A0%81/","excerpt":"","text":"在windows上压缩的文件，是以系统默认编码中文来压缩文件。由于zip文件中没有声明其编码，所以linux上的unzip一般以默认编码解压，中文文件名会出现乱码。可以使用终端zip文件进行解压并指定解压的编码unzip -O GBK xxx.zip解压后的文件的路径为当前终端所在的路径","raw":null,"content":null,"categories":[],"tags":[{"name":"Linux","slug":"Linux","permalink":"https://hareric.com/tags/Linux/"}]},{"title":"一趟聚类（One-pass Cluster）及python实现","slug":"一趟聚类(One-Pass Cluster)及python实现","date":"2016-07-06T09:17:56.000Z","updated":"2021-05-23T13:36:42.411Z","comments":true,"path":"2016/07/06/一趟聚类(One-Pass Cluster)及python实现/","link":"","permalink":"https://hareric.com/2016/07/06/%E4%B8%80%E8%B6%9F%E8%81%9A%E7%B1%BB(One-Pass%20Cluster)%E5%8F%8Apython%E5%AE%9E%E7%8E%B0/","excerpt":"一趟聚类简介一趟聚类算法是由蒋盛益教授提出的无监督聚类算法，该算法具有高效，简单的特点。数据集只需要遍历一遍即可完成聚类。算法对超球状分布的数据有良好的识别，对凸型数据分布识别较差。 一趟聚类可以在大规模数据，或者二次聚类中，或者聚类与其他算法结合的情况下，发挥其高效，简单的特点；","text":"一趟聚类简介一趟聚类算法是由蒋盛益教授提出的无监督聚类算法，该算法具有高效，简单的特点。数据集只需要遍历一遍即可完成聚类。算法对超球状分布的数据有良好的识别，对凸型数据分布识别较差。 一趟聚类可以在大规模数据，或者二次聚类中，或者聚类与其他算法结合的情况下，发挥其高效，简单的特点； 算法流程 初始时从数据集读入一个新的对象 以这个对象构建一个新的簇 若达到数据集末尾,则转6,否则读入一个新的对象;计算它与每个已有簇之间的距离，并选择与它距离最小的簇 若最小距离超过给定的阈值r,转2 否则将对象并入该簇，并更新簇心，转3 结束 距离公式在本算法中，我采用的是欧氏距离公式计算节点与簇心之间的距离欧氏距离公式在python代码中,使用numpy包可以轻易地实现123import numpy as npdef euclidian_distance(vec_a, vec_b): return np.sqrt(np.sum(np.square(np.array(vec_a) - vec_b))) 代码实现核心代码123456789101112131415161718192021def clustering(self): self.cluster_list.append(ClusterUnit()) # 初始新建一个簇 self.cluster_list[0].add_node(0, self.vectors[0]) # 将读入的第一个节点归于该簇 for index in range(len(self.vectors))[1:]: min_distance = euclidian_distance(vec_a=self.vectors[0], vec_b=self.cluster_list[0].centroid) # 与簇的质心的最小距离 min_cluster_index = 0 # 最小距离的簇的索引 for cluster_index, cluster in enumerate(self.cluster_list[1:]): # 寻找距离最小的簇，记录下距离和对应的簇的索引 distance = euclidian_distance(vec_a=self.vectors[index], vec_b=cluster.centroid) if distance &lt; min_distance: min_distance = distance min_cluster_index = cluster_index + 1 if min_distance &lt; self.threshold: # 最小距离小于阀值,则归于该簇 self.cluster_list[min_cluster_index].add_node(index, self.vectors[index]) else: # 否则新建一个簇 new_cluster = ClusterUnit() new_cluster.add_node(index, self.vectors[index]) self.cluster_list.append(new_cluster) del new_cluster 实验实验说明 现有中国天气的数据集,数据包括中国各个城市每年的最低和最高温以及该城市的x，y坐标。 现在分别使用nltk包中自带的k-mean聚类算法和上述的一趟聚类算法对中国城市的气温情况进行聚类。每个城市的属性只考虑2个属性分别为最高气温和最低气温。 最后使用matplotlib包对聚类结果进行构图。 其中k-means,初始设定的k的个数为5; 一趟聚类,阈值初始设置为9,最后聚类出10个簇。 实验结果运行时间针对该份数据集,通过多次运行计算运行时间求平均值得出：k-means: 0.00024sone-pass cluster: 0.00008s不难看出一趟聚类由于算法自身的简单,运行速度相比于k-means有显著的提升。 聚类效果不能只看运行时间，我们同样也要观察聚类的效果如何。现有2幅分别为k-means和一趟聚类的效果图。 k-means聚类效果图 一趟聚类效果图 中国年平均气温图 通过简单的目测对比可得,一趟聚类和k-means聚类的效果都能够有效地反映出中国气温的分布。综上所述，一趟聚类是一种较为有效的聚类算法，考虑到它的算法简单，该算法在处理大数据上有着显著的优越性。 完整代码","raw":null,"content":null,"categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://hareric.com/tags/python/"},{"name":"聚类","slug":"聚类","permalink":"https://hareric.com/tags/%E8%81%9A%E7%B1%BB/"}]},{"title":"Java细节知识点整理","slug":"Java细节知识点","date":"2016-06-27T08:53:48.000Z","updated":"2021-05-22T08:01:05.399Z","comments":true,"path":"2016/06/27/Java细节知识点/","link":"","permalink":"https://hareric.com/2016/06/27/Java%E7%BB%86%E8%8A%82%E7%9F%A5%E8%AF%86%E7%82%B9/","excerpt":"上学期Java期末考试前整理的一些不怎么会注意到的细节知识点，结果考试考的挺实在的，基本没有用到。不过既然整理了就发出来吧。","text":"上学期Java期末考试前整理的一些不怎么会注意到的细节知识点，结果考试考的挺实在的，基本没有用到。不过既然整理了就发出来吧。 数值数据类型整数 byte 8位带符号数 short 16位带符号数 int 32位带符号数 long 64位带符号数 浮点数 float 32位，标准IEEE754，单精度 double 64位，标准IEEE754，双精度 char和stringchar letter = &apos;a&apos;; String s = &quot;a&quot;; &apos;a&apos;是一个字符 &quot;A&quot;是字符串 特殊字符转义序列\\b 退格键 \\t Tab键 \\n 换行符号 \\f 换页 \\r 回车键 \\\\\\ 反斜杠 \\&apos; 单引号 \\&quot; 双引号 简易GUI界面123456789101112import javax.swing.JOptionPane;//接收字符串GUI界面String s = JOptionPane.showInputDialoge(&quot;Please enter an input&quot;,&quot;Title&quot;,JOptionPane.QUESTION_MESSAGE);//选择是否的GUI界面int a = JOptionPane.showConfirmDialog(null,&quot;Continue&quot;); /* JOptionPane.YES_OPTION = 0 JOptionPane.NO_OPTION = 1 JOptionPane.CANCEL_OPTION = 2 *///显示信息的GUI界面JOptionPane.showMessageDialog(null,&quot;The Message!&quot;); 字符串转化为数字12345int intValue = Integer.parseInt(intString);double doubleValue = Double.parseDouble(doubleString);long longValue = Long.parseLong(longString);byte byteValue = Byte.parseByte(byteString);short shortValue = Short.parseShort(shortString); 数字转化为字符串1String.valueOf();//各种类型的变量都可以用这个方法转化 switch用法12345678char ch = &#x27;a&#x27;;switch(ch)&#123; case &#x27;a&#x27;:System.out.print(ch); case &#x27;b&#x27;:System.out.print(ch); case &#x27;c&#x27;:System.out.print(ch); default:System.out.print(ch);&#125; 运行结果为：aaaa 三元运算符y = (x &gt; 0) ? 1 : -1 格式化输出常用的标识符 %b 布尔值 %c 字符 %d 十进制整数 %f 浮点数 %e 标准科学计数法 %s 字符串 指定宽度和精度 %5c 指定宽度为5 %6b 指定宽度为6 %10.2f 指定宽度为10 精度为2 浮点数来控制循环不要使用浮点数来控制循环，因为浮点数通常是用近似值表示的，无法得到像整数一样精确的值，比如以下循环是无限循环1234567double item = 1;double sum = 0;while(item != 0)&#123; sum += item; item -= 0.1;&#125; 输入和输出重定向在终端运行java程序时，可以通过文本文件完成输入和输出 java MyClass &lt; input.txt 通过txt输入数据 java MyClass &gt; output.txt 将运行结果输出到txt java MyClass &lt; input.txt &gt; output.txt 也可连用 for循环for( ; ; ) 等价 for( ;true; ) 等价 whlie(true) 最小化数值误差涉及浮点数的数值误差是不可避免的，如：计算0.01到1.0的数列之和1234double sum = 0;for(double i = 0.01;i &lt;= 1.0;i += 0.01) sum += i; System.out.println(sum);//结果为49.50000000000003，（精确结果应为50.50)在大数之前先加小数是减小误差的方法，因为例如，100000000.0+0.00000001的精确结果是100000000.0所以在本例中，以（0.01，0.02。。。0.99，1.0）的顺序求和，结果会更精确 蒙特卡罗模拟利用随机数和概率解决问题例如：求π（1）随机生成正方形内的一点（2）计算落入内接圆的概率，从而求得PI。12345678910int inNum = 0;int testNum = 10000000;//测试的次数for(int i=0;i&lt;testNum;i++)&#123; double x = Math.random(); double y = Math.random(); if(Math.sqrt( (Math.pow(x-0.5,2) + Math.pow(y-0.5,2)) ) &lt;0.5)//记录在圆内的个数 inNum++; &#125;System.out.println(&quot;PI = &quot; + inNum*4/(double)testNum); 方法的重载被重载的方法必须具有不同的参数列表。不能基于不同修饰符号或返回值类型来重载方法，但并不意味着不能修改修饰符号或返回值。例如下面这个例子。 1234public void m1(int x)&#123;&#125;private int m1(int x,int y)&#123;return 12;&#125;//public String m1(int x)&#123;return &quot;12&quot;;&#125; //error!Duplicate method m1(int) 有时调用重载的方法时，会有多个匹配，此时会产生歧义调用的错误 12345public static void main()&#123; double m = max(1,2); //error 无法识别与哪个方法匹配&#125;;static double max(int num1,double num2)&#123;&#125;;static double max(double num1,double num2)&#123;&#125;; 声明数组的方法123elementType[] arrayRefVar = new elementType[arraySize];elementType arrayRefVar[] = new elementType[arraySize];elementType arrayRefVar[] = &#123;,,,&#125;; 与类的实例化不同，分开写会报错，如12double[] myList;myList = &#123;1.9,2.9,3.4&#125;;//error char[]对于char[]类型的数组，可以用一条语句打印。如：12char[] city = &#123;&#x27;D&#x27;,&#x27;a&#x27;,&#x27;l&#x27;,&#x27;l&#x27;,&#x27;a&#x27;,&#x27;s&#x27;&#125;;System.out.print(city); for-each循环针对数组使用的循环 for(double u : myList){} 数组复制list2 = list1;该语句并不能将list1复制到list2，只会将数组list1的引用传递给list2复制数组的方法：1) 使用循环逐个复制数组的元素。2) 使用System类中的静态方法arraycopy(注意:该方法违背了Java的命名习惯) System.arraycopy(sourseArray, src_pos, targetArray, tar_pos, length); 3) 使用clone方法复制数组 匿名数组new int[]&#123;3,1,2,3,1&#125; 没有变量名显式地引用变量,这样的数组称为匿名数组 方法传递数组对于数组类型的参数,参数值是数组的引用,给方法传递的是这个引用.例:12345678public static void main(String arguments[])&#123; int[] as = &#123;1,2,3&#125;; m(as); System.out.println(as[0]); //输出为100&#125;public static void m(int[] array)&#123; array[0] = 100;&#125;array虽然是参数,却成功修改了as[0]的值 可变长参数列表12345public static void main()&#123; m(1.2); m(new double[]&#123;1,232,1.2&#125;);&#125;public static void m(double... numbers)&#123;&#125; numbers是可变长参数,所以既可以传double类型的数或者double类型的数组 Arrays类123456java.util.Arrays.sort(array);//Sort the whole arrayjava.util.Arrays.sort(array,1,3);//Sort part of arrayjava.util.Arrays.binarySearch(list,12);//利用二分法查找列表中某个元素的位置,列表必须排好序java.util.Arrays.fill(list1,5);//Fill 5 to the whole arrayjava.util.Arrays.fill(list1,1,3,8);//Fill 8 to a partial arrayjava.util.Arrays.equals(list1,list2);//判断列表1和列表2的内容是否相同 二维数组构建使用语法 new int[5][]创建数组时,必须指定第一个下标.语法new int[][]是错误的 基本类型变量和引用类型变量int double 等属于基本类型变量数组 各种类的实例化后的变量 属于引用类型变量","raw":null,"content":null,"categories":[],"tags":[{"name":"Java","slug":"Java","permalink":"https://hareric.com/tags/Java/"}]},{"title":"mac中XAMPP如何开启局域网访问","slug":"mac中XAMPP如何开启局域网访问","date":"2016-06-17T15:01:20.000Z","updated":"2021-05-22T08:01:05.400Z","comments":true,"path":"2016/06/17/mac中XAMPP如何开启局域网访问/","link":"","permalink":"https://hareric.com/2016/06/17/mac%E4%B8%ADXAMPP%E5%A6%82%E4%BD%95%E5%BC%80%E5%90%AF%E5%B1%80%E5%9F%9F%E7%BD%91%E8%AE%BF%E9%97%AE/","excerpt":"XAMPP作为apache服务器默认的时候是无法再局域网的其他机器上访问设置界面的。那么怎样启用局域网访问呢？","text":"XAMPP作为apache服务器默认的时候是无法再局域网的其他机器上访问设置界面的。那么怎样启用局域网访问呢？1、找到该文件在XAMPP的安装路径中找到该文件/Applications/XAMPP/xamppfiles/etc/extra/httpd-xampp.conf用sublime或者使用vim编辑器打开 sudo vim /Applications/XAMPP/xamppfiles/etc/extra/httpd-xampp.conf 2、修改访问权限找到以下代码123456789101112# since XAMPP 1.4.3&lt;Directory &quot;/Applications/XAMPP/xamppfiles/phpmyadmin&quot;&gt; AllowOverride AuthConfig Limit Require local ErrorDocument 403 /error/XAMPP_FORBIDDEN.html.var&lt;/Directory&gt;&lt;Directory &quot;/Applications/XAMPP/xamppfiles/phpsqliteadmin&quot;&gt; AllowOverride AuthConfig Limit Require local ErrorDocument 403 /error/XAMPP_FORBIDDEN.html.var&lt;/Directory&gt;修改为1234567891011121314151617&lt;Directory &quot;/Applications/XAMPP/xamppfiles/phpmyadmin&quot;&gt; AllowOverride AuthConfig Limit # Require local Require all granted Order allow,deny Allow from all ErrorDocument 403 /error/XAMPP_FORBIDDEN.html.var&lt;/Directory&gt;&lt;Directory &quot;/Applications/XAMPP/xamppfiles/phpsqliteadmin&quot;&gt; AllowOverride AuthConfig Limit # Require local Require all granted Order allow,deny Allow from all ErrorDocument 403 /error/XAMPP_FORBIDDEN.html.var&lt;/Directory&gt; 3、使用IP地址访问在终端使用ifconfig查询本机的ip地址，然后在局域网中输入IP地址便可以在局域网中访问自己的服务器了。","raw":null,"content":null,"categories":[],"tags":[{"name":"XAMPP","slug":"XAMPP","permalink":"https://hareric.com/tags/XAMPP/"}]},{"title":"花20分钟写的-大白话讲解如何给github上项目贡献代码","slug":"花20分钟写的-大白话讲解如何给github上项目贡献代码","date":"2016-06-05T13:03:44.000Z","updated":"2021-05-22T08:01:05.401Z","comments":true,"path":"2016/06/05/花20分钟写的-大白话讲解如何给github上项目贡献代码/","link":"","permalink":"https://hareric.com/2016/06/05/%E8%8A%B120%E5%88%86%E9%92%9F%E5%86%99%E7%9A%84-%E5%A4%A7%E7%99%BD%E8%AF%9D%E8%AE%B2%E8%A7%A3%E5%A6%82%E4%BD%95%E7%BB%99github%E4%B8%8A%E9%A1%B9%E7%9B%AE%E8%B4%A1%E7%8C%AE%E4%BB%A3%E7%A0%81/","excerpt":"转自 https://site.douban.com/196781/widget/notes/12161495/note/269163206/\n本文献给对git很迷茫的新手，注意是新手，但至少会点基本操作，有点基本概念的新手，我不会从怎么用github和git是什么开始讲的。如果作为新手你看书又看不进去，原理又太复杂，有没有直接了当告诉我们怎么给项目贡献代码，并和项目同步代码的大体流程。于是我快速写了这么个东西。一来整理下自己混乱的思路，二来想号召大家一起用git开发点东西，可是好几个同鞋只会寂寞的给自己push。","text":"转自 https://site.douban.com/196781/widget/notes/12161495/note/269163206/ 本文献给对git很迷茫的新手，注意是新手，但至少会点基本操作，有点基本概念的新手，我不会从怎么用github和git是什么开始讲的。如果作为新手你看书又看不进去，原理又太复杂，有没有直接了当告诉我们怎么给项目贡献代码，并和项目同步代码的大体流程。于是我快速写了这么个东西。一来整理下自己混乱的思路，二来想号召大家一起用git开发点东西，可是好几个同鞋只会寂寞的给自己push。我先说下 我之前对github操作的一些迷茫历程，然后之后是怎么解惑的。 最最开始，我以为git clone ［项目地址］，也就是把代码clone下来 然后修改，然后push到项目里就可以了。后来发现，这种情况只适合该项目属于你自己的情况，如果你git clone别人的项目代码，你想push都push不上去，因为 git push 不是需要你输入github帐号密码么。 然后 我就知道了 github上 不是有个fork么， 好，那么给别人的项目提交代码的方式就有了，先fork下别人的代码，于是你的github上就出现了同名的项目，这个项目就属于你自己了，你把这个自己的项目git clone到本地，修修改改，然后push到你自己的项目里，那么你如何把你对自己项目的改动，给发到之前fork的那个原项目呢，看见了没，在github上你的项目页面有个按钮，叫Pull request，对 你点它就会把你的修改发到对方的项目里，人还会收到邮件呢，由原项目主人决定是否接受你的修改。但是，这样问题就出来了，在你fork他的项目之后，如果他又更新了代码，你自己fork的项目怎么做到和原项目同步呢？我就想啊，是不是 我还得重新git clone原项目的代码，然后手动合并到我fork的项目里呢。。。梁老师说，你丫这太蛋疼了，肯定不是这么麻烦，我细想，也是啊，这不2么。。。 然后，从《Pro git》上看到一个知识点，我擦，github居然可以给项目添加合作者，也就是说，假如你，对，说你呢，戴眼镜那个，你想参与我的项目，你跟我说一声，我就把你添加到我的项目里作为合作者，这个选项在项目的设置里面有，只要我添加你了，你就可以git clone我的代码然后修修改改，然后直接push上来就行了，就不用fork那么麻烦了，如果你要更新服务器代码，只要git pull就行了，看 合作者这东西多方便，就像我们在公司用svn似的。 然后我就想啊，有了合作者还需要你丫fork这个功能干啥？仔细一想，你写个好项目，不能随便加合作者啊，加了个熊孩子把你代码改废了可咋整，这年头熊孩子很多，我自己不就是一个么。所以fork肯定还是需要，fork就是专门预防熊孩子的，这就是真相！那么前面说道到fork之后如何与原项目同步的问题还在啊，没有得到解决。 于是《Pro git》再次给了我一个解答，具体流程是你啊想给我的项目做贡献，你先git clone我的代码到本地，然后修修改改，然后你不是不能push到我的项目里么，你可以先在github页面上fork我的项目，有了你自己的项目地址（url）之后呢，你在本地操作git remote add [sort name] [your url]，意思就是添加第二个远程仓库地址，这个仓库的“昵称”就是你刚指定的[sort name]，然后，你之后push文件呢 就通过指定这个［sort name］来push到这个你自己的仓库里。等你觉得想要把你改的发给原项目同步，就在你的项目上点Pull request按钮.说下另一种情况，如果是，原项目发生了改动，你要想同步到本地，就直接从git fetch origin 从原项目的地址同步代码，然后再merge就好了。当然，如《Pro git》上所写，你可以通过新建分支的方式往自己的项目上push，这样同步的时候直接fetch就行了。这块如果我没写明白或者你想知道怎么新建分支的方式push到自己的项目里，可以直接参考《Pro git》的“公开的小型项目”一节，那我的贡献就是指点你如何从这本书里快速的找到你想要的。= =。","raw":null,"content":null,"categories":[],"tags":[{"name":"git","slug":"git","permalink":"https://hareric.com/tags/git/"}]},{"title":"招聘网数据库系统说明书","slug":"招聘网数据库系统说明书","date":"2016-06-03T14:52:11.000Z","updated":"2021-05-23T13:45:24.052Z","comments":true,"path":"2016/06/03/招聘网数据库系统说明书/","link":"","permalink":"https://hareric.com/2016/06/03/%E6%8B%9B%E8%81%98%E7%BD%91%E6%95%B0%E6%8D%AE%E5%BA%93%E7%B3%BB%E7%BB%9F%E8%AF%B4%E6%98%8E%E4%B9%A6/","excerpt":"一、需求分析1.1 背景　　随着互联网的快速发展，网民数量的不断增加，人们上网的途径也越来越方便，网络在现代人生活中的应用也越来越广泛。与互联网相结合的网络招聘在解决了部分大学生工作的同时，也大大减少了人力、物力的消耗以及大学毕业生奔波路途的时间和精力，其效果也将远远超过传统招聘所获得的效果。目前，网络招聘已发展成为深受招聘求职者喜欢的一种求职招聘环境。互联网给求职招聘者提供了丰富的资源，未他们创造出一种良好的求职招聘平台，具备现实中人才中介机构的功能。招聘求职网站为应聘者提供了方便、快捷的应聘途径，不仅信息更新快、信息数量多、而且选择余地大。对招聘单位来说，招聘网站不仅为他们开辟了招聘人才的新方式，而且使其工作流程更加方便、快捷、高效。使得招聘工作中的人员初选工作变得轻松简单。双方通过交互式的网上登陆和查询完成信息的交流。这种现代化招聘方式与传统招聘方式有很大的不同，它不需要时间和空间上保持绝对的一致，方便了双方时间的选择。　　网络招聘在节约招聘单位和求职者双方的费用上有很大的优势。求职者可以通过互联网对工作类型、地点、工作待遇等条件进行筛选之后快速得到自己想要的招聘信息并投递简历，大大减少了打印简历和证书、交通等当面的费用；对于招聘单位而言，在网上发布招聘信息不仅仅能够帮助扩大招聘信息的覆盖面及宣传度，还能帮助企业节约不少传统招聘所需的人力、财力资源，而且对信息的发布和修改也十分方便。招聘网就是这样一个跨时间和空间的信息互动过程。招聘单位与求职者的双方的积极互动将促进网络招聘的功能更加完善，而网络招聘也将减少企业招聘和求职者应聘过程中的盲目行为，大大提高招聘求职规模和招聘求职成功率。","text":"一、需求分析1.1 背景 随着互联网的快速发展，网民数量的不断增加，人们上网的途径也越来越方便，网络在现代人生活中的应用也越来越广泛。与互联网相结合的网络招聘在解决了部分大学生工作的同时，也大大减少了人力、物力的消耗以及大学毕业生奔波路途的时间和精力，其效果也将远远超过传统招聘所获得的效果。目前，网络招聘已发展成为深受招聘求职者喜欢的一种求职招聘环境。互联网给求职招聘者提供了丰富的资源，未他们创造出一种良好的求职招聘平台，具备现实中人才中介机构的功能。招聘求职网站为应聘者提供了方便、快捷的应聘途径，不仅信息更新快、信息数量多、而且选择余地大。对招聘单位来说，招聘网站不仅为他们开辟了招聘人才的新方式，而且使其工作流程更加方便、快捷、高效。使得招聘工作中的人员初选工作变得轻松简单。双方通过交互式的网上登陆和查询完成信息的交流。这种现代化招聘方式与传统招聘方式有很大的不同，它不需要时间和空间上保持绝对的一致，方便了双方时间的选择。 网络招聘在节约招聘单位和求职者双方的费用上有很大的优势。求职者可以通过互联网对工作类型、地点、工作待遇等条件进行筛选之后快速得到自己想要的招聘信息并投递简历，大大减少了打印简历和证书、交通等当面的费用；对于招聘单位而言，在网上发布招聘信息不仅仅能够帮助扩大招聘信息的覆盖面及宣传度，还能帮助企业节约不少传统招聘所需的人力、财力资源，而且对信息的发布和修改也十分方便。招聘网就是这样一个跨时间和空间的信息互动过程。招聘单位与求职者的双方的积极互动将促进网络招聘的功能更加完善，而网络招聘也将减少企业招聘和求职者应聘过程中的盲目行为，大大提高招聘求职规模和招聘求职成功率。 1.2功能需求网站主要用户角色分为应聘用户、企业用户个人用户： 用户注册 用户信息管理 创建简历 简历信息管理 投放简历企业用户： 用户注册 用户信息管理 发布招聘信息 招聘信息管理 获得相关简历的投放信息 管理简历的评判成绩 1.3数据需求1.3.1流程图 1.3.2数据需求表数据组名：用户信息与其他数据的约束关系：与简历存在一对多关系 数据项名 数据类型 数据长度 小数位数 约束 允许空值 备注 用户名 Varchar 20 - - 否 口令 Varchar 20 长度为6-16个字符 不允许空格 否 可用MD5算法加密 性别 Varchar 2 - - 否 邮件地址 Varchar 32 - unique 否 联系电话 Varchar 11 - unique 否 数据组名：企业信息与其他数据的约束关系：与招聘信息存在一对多关系 数据项名 数据类型 数据长度 小数位数 约束 允许空值 备注 企业名称 Varchar 20 - foreign key 否 公司地址 Varchar 128 - 否 公司成立时间 Varchar 20 - 否 邮件地址 Varchar 40 - unique 否 公司性质 Varchar 40 - 否 联系电话 Varchar 11 - unique 否 公司规模 Varchar 20 - 否 公司简介 Varchar 255 - 否 数据组名：简历信息与其他数据的约束关系：与用户存在一对一关系 数据项名 数据类型 数据长度 小数位数 约束 允许空值 备注 求职类别 Varchar 40 - 否 求职者姓名 Varchar 20 - 否 出生日期 Varchar 20 - 否 现居住地 Varchar 20 - 否 学历 Varchar 40 - 否 工作经验 Varchar 128 - 否 毕业院校 Varchar 40 - 是 主修专业 Varchar 20 - 是 邮件 Varchar 20 - unique 否 联系电话 Int 20 - unique 否 个人介绍 Varchar 128 - 否 期望工作 Varchar 40 - 否 数据组名：招聘信息与其他数据的约束关系：与企业存在多对一关系 数据项名 数据类型 数据长度 小数位数 约束 允许空值 备注 所发布岗位类别 Varchar 40 - 否 岗位名称 Varchar 2 - 否 岗位地址 Varchar 40 - 否 学历要求 Varchar 20 - 否 性别要求 Varchar 10 - 是 工作经验要求 Varchar 20 - 否 年龄要求 Int 20 - 否 岗位地址 Varchar 40 - 否 工资 Varchar 20 - 否 招聘人数 Int 10 - 否 联系电话 Varchar 11 - unique 否 邮箱 Varchar 20 - unique 否 岗位简介 Varchar 128 - 是 发布时间 Varchar 20 - 否 二、概念模型 三、逻辑模型 1. 用户信息（user） 属性名 说明 类型 长度 默认值 允许空值 约束 id 用户id,主码 Varchar 15 - 否 - mid 用户对应的网站管理信息 Varchar 20 - 否 外码，参照manage中的id name 姓名/联系人 Varchar 20 - 否 - sex 性别 Varchar 2 - 否 - email 邮件地址 Varchar 20 - 否 - tel 联系电话 Varchar 20 - 否 - 2. 公司信息（company） 属性名 说明 类型 长度 默认值 允许空值 约束 id 公司id，主码 Varchar 15 - 否 - mid 公司对应的网站管理信息 Varchar 20 - 否 外码，参照manage中的id name 公司名称 Varchar 20 - 否 - address 公司地址 Varchar 20 - 否 - establish 公司成立时间 Varchar 10 - 否 - email 邮件地址 Varchar 20 - 是 - property 公司性质 Varchar 20 - 否 - tel 联系电话 Varchar 20 - 否 - scale 公司规模 Varchar 10 - 否 - intro 公司简介 Varchar 300 - 否 - 3. 管理信息（manage） 属性名 说明 类型 长度 默认值 允许空值 约束 id 注册id，主码 Varchar 15 - 否 - password 密码 Varchar 20 - 否 - role 用户角色 Varchar 2 - 否 - 4. 简历（CV） 属性名 说明 类型 长度 默认值 允许空值 约束 id 简历id，主码 Varchar 15 - 否 - category 求职类型 Varchar 15 - 否 外码，参照job_category中的cate_name name 名字 Varchar 20 - 否 - sex 性别 Varchar 2 - 否 - birth 出生日期 Varchar 20 - 否 - address 居住地址 Varchar 20 - 否 - education 学历 Varchar 10 - 否 - work_exp 工作经验 Varchar 10 - 否 - school 毕业院校 Varchar 20 - 是 - major 主修专业 Varchar 20 - 是 - email 邮箱 Varchar 20 - 否 - tel 联系电话 Varchar 20 - 否 - intro 个人介绍 Varchar 300 - 否 - expected_job 期望工作 Varchar 20 - 否 - 5. 岗位（job_deliver） 属性名 说明 类型 长度 默认值 允许空值 约束 id 岗位id，主码 Varchar 15 - 否 - category 岗位类别 Varchar 20 - 否 外码，参照manage中的id job_name 岗位名称 Varchar 10 - 否 - sex 性别要求 Varchar 5 - 否 - education 学历要求 Varchar 10 - 否 - work_year 工作经验要求 Varchar 10 - 否 - age 年龄要求 Varchar 10 - 否 - position 岗位地址 Varchar 20 - 否 - salary 工资 Varchar 10 - 否 - num 招聘人数 Int 4 - 否 - contact 联系方式 Varchar 20 - 否 - description 岗位介绍 Varchar 300 - 是 - created_time 发布时间 Varchar 20 - 否 - 6. 岗位类别（job_category） 属性名 说明 类型 长度 默认值 允许空值 约束 cate_name 岗位类别 Varchar 10 - 否 - pre_cate 岗位的父类类别，若无父类，则为空 Varchar 20 - 否 - 7. 用户-简历（user-CV） 属性名 说明 类型 长度 默认值 允许空值 约束 id 用户-简历id，主码 Varchar 15 - 否 - user_id 对应的用户id Varchar 20 - 否 外码，参照user中的id cv_id 对应的简历id Varchar 20 - 否 外码，参照CV中的id 8. 公司-岗位（company-job） 属性名 说明 类型 长度 默认值 允许空值 约束 id 公司-岗位id，主码 Varchar 15 - 否 - company_id 对应的公司id Varchar 20 - 否 外码，参照company中的id job_deliver_id 对应的岗位id Varchar 20 - 否 外码，参照job_deliver中的id 9. 简历投递（job-wanted） 属性名 说明 类型 长度 默认值 允许空值 约束 id 用户-简历id，主码 Varchar 15 - 否 - job_deliver_id 对应的招聘岗位id Varchar 20 - 否 外码，参照job_deliver中的id cv_id 对应的简历id Varchar 20 - 否 外码，参照CV中的id result 投递状态 Varchar 10 - 否 - 四、建表代码1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495969798991001011021031041051061071081091101111121131141151161171.建立数据库：CREATE DATABASE jobnetwork;USE jobnetwork;2.建立管理信息表：CREATE TABLE manage ( ID varchar(12) NOT NULL, /* 注册者管理信息的id */ role varchar(20) NOT NULL, /* 0代表应聘者，1代表公司 */ password varchar(20) NOT NULL, /* 注册者的登录密码 */ PRIMARY KEY (ID))3.建立应聘者信息表：CREATE TABLE applicant ( ID varchar(11) NOT NULL, /* 应聘者的id */ name varchar(20) NOT NULL, /* 应聘者名字 */ sex varchar(10) NOT NULL, /* 应聘者性别 */ email varchar(20) NOT NULL, /* 应聘者邮箱 */ telephone varchar(20) NOT NULL, /* 应聘者手机号码 */ mid_id varchar(12) NOT NULL, /* 应聘者对应的注册管理信息的id */ PRIMARY KEY (ID), FOREIGN KEY (mid_id) REFERENCES manage (ID))4.建立公司信息表：CREATE TABLE company ( ID varchar(11) NOT NULL, /* 公司的id */ name varchar(20) NOT NULL, /* 公司名字 */ address varchar(30) NOT NULL, /* 公司地址 */ establish varchar(20) NOT NULL, /* 公司建立时间 */ email varchar(20) NOT NULL, /* 公司联系邮箱 */ property varchar(20) NOT NULL, /* 公司性质 */ telephone varchar(20) NOT NULL, /* 公司联系电话 */ scale varchar(20) NOT NULL, /* 公司规模 */ intro varchar(300) NOT NULL, /* 公司简介 */ mid_id varchar(12) NOT NULL, /* 公司对应的注册管理信息的id */ PRIMARY KEY (ID), FOREIGN KEY (mid_id) REFERENCES manage (ID))5.建立岗位类别表：CREATE TABLE jobcategory ( cate_name varchar(20) NOT NULL, /* 岗位类别 */ pre_cate varchar(20) NOT NULL, /* 岗位对应的父类别，若无则为空 */ PRIMARY KEY (cate_name))6.建立简历表：CREATE TABLE cv ( ID varchar(11) NOT NULL, /* 简历的id */ name varchar(20) NOT NULL, /* 应聘者名字 */ sex varchar(10) NOT NULL, /* 应聘者性别 */ birth varchar(20) NOT NULL, /* 应聘者出生日期 */ create_time varchar(20) NOT NULL, /* 简历创建日期 */ location varchar(50) NOT NULL, /* 应聘者先居住地 */ education varchar(20) NOT NULL, /* 应聘者学历 */ work_exp varchar(100) NOT NULL, /* 应聘者工作经历 */ school varchar(20) NOT NULL, /* 应聘者毕业院校 */ major varchar(20) NOT NULL, /* 应聘者专业 */ email varchar(20) NOT NULL, /* 应聘者邮箱 */ telephone varchar(20) NOT NULL, /* 应聘者手机号码 */ intro varchar(300) NOT NULL, /* 应聘者简介 */ expected_job varchar(20) NOT NULL, /* 应聘者期望应聘岗位 */ category_id varchar(20) NOT NULL, /* 应聘者应聘岗位类别 */ PRIMARY KEY (ID), FOREIGN KEY (category_id) REFERENCES jobcategory (cate_name))7.建立发布岗位表：CREATE TABLE jobdeliver ( ID varchar(11) NOT NULL, /* 发布的岗位id */ job_name varchar(20) NOT NULL, /* 岗位名字 */ position varchar(40) NOT NULL, /* 岗位工作地点 */ numb int NOT NULL, /* 岗位招聘人数 */ sex varchar(10) NOT NULL, /* 性别要求 */ work_year varchar(5) NOT NULL, /* 工作经历要求 */ age varchar(10) NOT NULL, /* 年龄要求 */ education varchar(20) NOT NULL, /* 学历要求 */ salary varchar(20) NOT NULL, /* 工资 */ intro varchar(300) NOT NULL, /* 岗位介绍 */ contact varchar(20) NOT NULL, /* 联系方式 */ created_time varchar(20) NOT NULL, /* 创建时间 */ category_id varchar(20) NOT NULL, /* 岗位所属类别 */ PRIMARY KEY (ID), FOREIGN KEY (category_id) REFERENCES jobcategory (cate_name))8.建立公司-岗位表：CREATE TABLE companyjob ( ID varchar(11) NOT NULL, /* 公司-岗位的id */ company_id varchar(11) NOT NULL, /* 公司的id */ job_deliver_id varchar(11) NOT NULL,/* 已发布岗位的id */ PRIMARY KEY (ID), FOREIGN KEY (job_deliver_id) REFERENCES jobdeliver (ID), FOREIGN KEY (company_id) REFERENCES company (ID))9.建立应聘者-简历表：CREATE TABLE usercv ( ID varchar(11) NOT NULL, /* 用户-简历的id */ cv_id varchar(11) NOT NULL, /* 简历的id */ applicant_id varchar(11) NOT NULL, /* 应聘者的id */ PRIMARY KEY (ID), FOREIGN KEY (applicant_id) REFERENCES applicant (ID), FOREIGN KEY (cv_id) REFERENCES cv (ID))10.建立岗位-建立投递表：CREATE TABLE jobwanted ( ID varchar(11) NOT NULL, /* 简历-岗位 投递的id */ result varchar(10) NOT NULL, /* 投递结果 */ cv_id varchar(11) NOT NULL, /* 简历的id */ job_deliver_id varchar(11) NOT NULL,/* 发布岗位的id */ PRIMARY KEY (ID), FOREIGN KEY (job_deliver_id) REFERENCES jobdeliver (ID), FOREIGN KEY (cv_id) REFERENCES cv (ID)) 五、招聘网数据库查询系统说明5.1系统介绍1)操作系统 系统搭建在Ubuntu系统上。Ubuntu是一免费开源操作系统，是基于Linux的发行版本，具有开放性和高性价比等显著特点。同时Ubuntu具有庞大的社区力量支持，可以方便地从社区获得帮助。系统选用的是Ubuntu14.04版本，该版本代号为“Trusty Tahr”（值得信赖的塔尔羊），是一个长期支持版本，有很好的性能和交互性。 2）开发语言 系统主要使用python，python是一种完全面向对象、解释型计算机程序设计语言，语法简洁清晰，拥有丰富和强大的库，易读且易维护，提供API，能方便进行维护和管理。 3）数据库管理系统系统选用SQLite数据库，SQLite是遵守ACID的关系数据库管理系统，它包含在一个相对小的C程式库中。与许多其它数据库管理系统不同，SQLite不是一个客户端/服务器结构的数据库引擎，而是被集成在用户程序中。 4）开发框架 系统选用Django框架，Django是一个开放源代码的Web应用框架，由Python写成。采用了MVC的软件设计模式。它强调代码复用,多个组件可以很方便的以“插件”形式服务于整个框架，有许多功能强大的第三方插件，具有很强的可扩展性。 5.2界面展示5.2.1登陆界面 输入账号和密码，查询数据库并与密码匹配成功后可登陆 若查询失败则无法登陆 5.2.2普通用户登录 获得用户详细信息，和对应的简历列表每份简历的详细信息 5.2.3企业用户登录 获得企业用户详细信息，和对应的发布的招聘信息列表每条招聘的详细招聘信息以及对应该招聘投放的简历 5.2.4数据库后台管理用来管理数据库的后台，实现增删查改。 5.3 系统使用说明系统运行需求：系统环境：ubuntu14.04/OS X程序语言：python 2.7.+框架: django 1.8.0+ 系统下载:使用github下载：git clone https://github.com/Hareric/dbBigHW.git直接下载：https://github.com/Hareric/dbBigHW/archive/master.zip 系统运行：进入该系统根目录，终端下执行python manage.py runserver开启本地服务器浏览器访问: 127.0.0.1:8000/home 进入查询页面浏览器访问: 127.0.0.1:8000/admin 进入数据库后台管理页面管理员帐号: admin 密码: admin","raw":null,"content":null,"categories":[],"tags":[{"name":"数据库","slug":"数据库","permalink":"https://hareric.com/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"Django","slug":"Django","permalink":"https://hareric.com/tags/Django/"},{"name":"文档","slug":"文档","permalink":"https://hareric.com/tags/%E6%96%87%E6%A1%A3/"}]},{"title":"scikit-learn的安装和基本使用教程","slug":"scikit-learn的安装和基本使用教程","date":"2016-05-22T04:11:40.000Z","updated":"2021-05-22T08:01:05.400Z","comments":true,"path":"2016/05/22/scikit-learn的安装和基本使用教程/","link":"","permalink":"https://hareric.com/2016/05/22/scikit-learn%E7%9A%84%E5%AE%89%E8%A3%85%E5%92%8C%E5%9F%BA%E6%9C%AC%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/","excerpt":"引言　　scikit-learn是Python的一个开源机器学习模块，它建立在NumPy，SciPy和matplotlib模块之上能够为用户提供各种机器学习算法接口，可以让用户简单、高效地进行数据挖掘和数据分析。","text":"引言 scikit-learn是Python的一个开源机器学习模块，它建立在NumPy，SciPy和matplotlib模块之上能够为用户提供各种机器学习算法接口，可以让用户简单、高效地进行数据挖掘和数据分析。 scikit-learn安装python 中安装许多模板库之前都有依赖关系,安装 scikit-learn 之前需要以下先决条件: Python(&gt;= 2.6 or &gt;= 3.3) NumPy (&gt;= 1.6.1) SciPy (&gt;= 0.9) 如无意外,下面用 pip 的安装方法可以顺利完成~~安装 numpy sudo pip install numpy 安装 scipy需要先安装 matplotlib ipython ipython-notebook pandas sympy sudo apt-get install python-matplotlib ipython ipython-notebook sudo apt-get install python-pandas python-sympy python-nose sudo pip install scipy 安装 scikit-learn sudo pip install -U scikit-learn 测试在 terminal 里面输入 pip list 这个会列出 pip 安装的所有东西,如果里面有 sklearn 这一项,应该就是大功告成了!或者尝试着将几个模板库导入进来 import numpy import scipy import sklearn 加载数据(Data Loading) 本文所使用的数据集为‘今日头条’近期两篇热门新闻“牛！川大学霸寝室5人获16份名校通知书”、“张超凡的最后14天：山西15岁休学少年是如何殒命网吧的”分别500条评论，共1000条评论。 去除停用词后得到了词库大小为3992的词库。因此构建了1000×3992的特征矩阵，以及长度为1000的对应评论所属类别列表 具体爬虫和特征矩阵构建代码 class_result_save.npy下载 feature_matrix_save.npy下载123import numpy as npfeature_matrix = np.load(&#x27;dataSet/feature_matrix_save.npy&#x27;)class_list = np.load(&#x27;dataSet/class_result_save.npy&#x27;) 数据归一化(Data Normalization) 大多数机器学习算法中的梯度方法对于数据的缩放和尺度都是很敏感的，在开始跑算法之前，我们应该进行归一化或者标准化的过程，这使得特征数据缩放到0-1范围中。scikit-learn提供了归一化的方法：1234567from sklearn import preprocessing# 归一化（Normalization）normalized_X = preprocessing.normalize(feature_matrix)print normalized_X# 标准化（Standardization）standardized_X = preprocessing.scale(feature_matrix)print standardized_X 特征选择(Feature Selection) 在解决一个实际问题的过程中，选择合适的特征或者构建特征的能力特别重要。这成为特征选择或者特征工程。特征选择时一个很需要创造力的过程，更多的依赖于直觉和专业知识，并且有很多现成的算法来进行特征的选择。下面的树算法(Tree algorithms)计算特征的信息量：12345from sklearn.ensemble import ExtraTreesClassifiermodel = ExtraTreesClassifier()print feature_matrix.shape # 原特征矩阵规模feature_matrix = model.fit(feature_matrix, class_list).transform(feature_matrix)print feature_matrix.shape # 特征选择后 特征矩阵的规模 特征提取(Feature Extraction) 用TFIDF算法来计算特征词的权重值是表示当一个词在这篇文档中出现的频率越高，同时在其他文档中出现的次数越少，则表明该词对于表示这篇文档的区分能力越强，所以其权重值就应该越大。123from sklearn.feature_extraction.text import TfidfTransformertfidf_transformer = TfidfTransformer()feature_matrix = tfidf_transformer.fit_transform(feature_matrix).toarray() 朴素贝叶斯(Naive Bayes) 朴素贝叶斯是一个很著名的机器学习算法，主要是根据训练样本的特征来计算各个类别的概率，在多分类问题上用的比较多。12345678910111213from sklearn import metricsfrom sklearn.naive_bayes import GaussianNB# 构建朴素贝叶斯模型model = GaussianNB()model.fit(feature_matrix, class_list)print model# 使用测试集进行测试(此处将训练集做测试集)expected = class_listpredicted = model.predict(feature_matrix)# 输出测试效果print metrics.classification_report(expected, predicted)print metrics.confusion_matrix(expected, predicted) k近邻(k-Nearest Neighbours) k近邻算法常常被用作是分类算法一部分，比如可以用它来评估特征，在特征选择上我们可以用到它。12345678910111213from sklearn import metricsfrom sklearn.neighbors import KNeighborsClassifier# 构建knn模型model = KNeighborsClassifier()model.fit(feature_matrix, class_list)print model# 使用测试集进行测试(此处将训练集做测试集)expected = class_listpredicted = model.predict(feature_matrix)# 输出测试效果print metrics.classification_report(expected, predicted)print metrics.confusion_matrix(expected, predicted) 决策树(Decision Tree) 分类与回归树(Classification and Regression Trees ,CART)算法常用于特征含有类别信息的分类或者回归问题，这种方法非常适用于多分类情况。12345678910111213from sklearn import metricsfrom sklearn.tree import DecisionTreeClassifier# 构建决策数模型model = DecisionTreeClassifier()model.fit(feature_matrix, class_list)print model# 使用测试集进行测试(此处将训练集做测试集)expected = class_listpredicted = model.predict(feature_matrix)# 输出测试效果print metrics.classification_report(expected, predicted)print metrics.confusion_matrix(expected, predicted)","raw":null,"content":null,"categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://hareric.com/tags/python/"},{"name":"机器学习","slug":"机器学习","permalink":"https://hareric.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"KNN算法实现之KD树","slug":"KNN算法实现之KD树","date":"2016-05-21T17:53:05.000Z","updated":"2021-05-22T08:01:05.399Z","comments":true,"path":"2016/05/22/KNN算法实现之KD树/","link":"","permalink":"https://hareric.com/2016/05/22/KNN%E7%AE%97%E6%B3%95%E5%AE%9E%E7%8E%B0%E4%B9%8BKD%E6%A0%91/","excerpt":"一、背景　　了解了KNN算法后，存在一个问题，由于KNN算法需要将所有数据集读入内存，并需要通过计算未知点与所有训练点的距离，筛选出距离最小的k个点。因此在庞大的训练集面前，计算复杂度和空间复杂度都相当得高，KNN算法就显得非常的无力。","text":"一、背景 了解了KNN算法后，存在一个问题，由于KNN算法需要将所有数据集读入内存，并需要通过计算未知点与所有训练点的距离，筛选出距离最小的k个点。因此在庞大的训练集面前，计算复杂度和空间复杂度都相当得高，KNN算法就显得非常的无力。 二、原理 将训练集构建出二叉树，利用回溯算法寻找与样点最近的k个训练点，避免与所有训练集计算距离的需求，大大减少了计算复杂度。 三、算法流程1、KD树的构建 Kd-树：是对数据点在k维空间{二维(x，y)，三维(x，y，z)，k维(x，y，z..)}中划分的一种数据结构，主要应用于多维空间关键数据的搜索(如：范围搜索和最近邻搜索)。本质上说，Kd-树就是一种平衡二叉树。 假设有6个二维数据点&#123;(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)&#125;，数据点位于二维空间内，根据这些数据点构造一个kd树。如下图： 步骤 ①6个数据点在x，y维度上的数据方差分别为39，28.63，所以在x轴上方差更大，故先选x轴为坐标轴来切分矩形； ②根据x维上的值将数据排序：&#123;(2,3),(4,7),(5,4),(7,2),(8,1),(9,6)&#125;6个数据的中值为7，所以以数据点(7,2)为节点，通过(7,2)并垂直于x轴的直线来把平面切分成两个矩形； ③确定左子空间和右子空间。x=7将整个空间分为两部分：x&lt;=7的部分为左子空间，包含3个节点=&#123;(2,3),(5,4),(4,7)&#125;；另一部分为右子空间，包含2个节点=&#123;(8,1)，(9,6)&#125;； kd树的构建是一个递归过程，我们对左子空间和右子空间内的数据重复上述过程将空间和数据集进一步细分，如此往复直到空间中任意两个子区域没有实例存在时，停止划分。 最后可以获得二叉树,如下图： 2、k-d树上的最邻近查找算法 目的是检索在k-d树中与查询点距离最近的数据点。 步骤： 假设查询点A。通过二叉搜索，顺着搜索路径很快就能找到最邻近的近似点，也就是叶子节点B。 而找到的叶子节点B并不一定就是最邻近的，最邻近肯定距离查询点更近，应该位于以查询点为圆心且通过叶子节点的圆域内。 为了找到真正的最近邻，还需要进行相关的回溯操作。也就是说，算法沿搜索路径反向查找是否有距离查询点更近的数据点。 实例：以查询(2.1,3.1)为例： 1.二叉树搜索 此时搜索路径中的节点为&#123;(7,2)，(5,4)，(2,3)&#125;，以(2,3)作为当前最近邻点，计算其到查询点(2.1,3.1)的距离为0.1414 2.回溯查找 在得到(2,3)为查询点的最近点之后，回溯到其父节点(5,4)，并判断在该父节点的其他子节点空间中是否有距离查询点更近的数据点。 以(2.1,3.1)为圆心，以0.1414为半径画圆，如下图所示。发现该圆并不和超平面y = 4交割，因此不用进入(5,4)节点右子空间中(图中绿色区域)去搜索； 最后，再回溯到(7,2)，以(2.1,3.1)为圆心，以0.1414为半径的圆更不会与x = 7超平面交割，因此不用进入(7,2)右子空间(图中红色区域)进行查找。至此，搜索路径中的节点已经全部回溯完，结束整个搜索，返回最近邻点(2,3)，最近距离为0.1414。","raw":null,"content":null,"categories":[],"tags":[{"name":"算法","slug":"算法","permalink":"https://hareric.com/tags/%E7%AE%97%E6%B3%95/"},{"name":"机器学习","slug":"机器学习","permalink":"https://hareric.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"决策树之特征选择---信息增益","slug":"决策树之特征选择---信息增益","date":"2016-05-18T18:33:51.000Z","updated":"2021-05-22T08:01:05.401Z","comments":true,"path":"2016/05/19/决策树之特征选择---信息增益/","link":"","permalink":"https://hareric.com/2016/05/19/%E5%86%B3%E7%AD%96%E6%A0%91%E4%B9%8B%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9---%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A/","excerpt":"　　基于信息增益的特征选取是一种广泛使用在决策树(decision tree)分类算法中用到的特征选取。该特征选择的方法是通过计算每个特征值划分数据集获得信息增益，通过比较信息增益的大小选取合适的特征值。","text":"基于信息增益的特征选取是一种广泛使用在决策树(decision tree)分类算法中用到的特征选取。该特征选择的方法是通过计算每个特征值划分数据集获得信息增益，通过比较信息增益的大小选取合适的特征值。 一、定义1.1 熵 信息的期望值，可理解为数据集的无序度，熵的值越大，表示数据越无序，公式如下： 其中H表示该数据集的熵值， pi表示类别i的概率， 若所有数据集只有一个类别，那么pi=1，H=0。因此H=0为熵的最小值，表示该数据集完全有序。 1.2 信息增益 熵的减少或者是数据无序度的减少。 二、流程1、计算原始数据的信息熵H12、选取一个特征，根据特征值对数据进行分类，再对每个类别分别计算信息熵，按比例求和，得出这种划分方式的信息熵H23、计算信息增益： infoGain = H1 - H24、根据2，3计算所有特征属性对应的信息增益，保留信息增益较大的特征属性。 三、实例海洋生物数据 被分类项\\特征 不浮出水面是否可以生存 是否有脚蹼 属于鱼类 1 是 是 是 2 是 是 是 3 是 否 否 4 否 是 否 5 否 是 否 3.1 原始数据信息熵p(是鱼类) = p1 =0.4p(非鱼类) = p2 =0.6通过信息熵公式可得原始数据信息熵 H1 = 0.97095 3.2 根据特征分类计算信息熵选择’不服出水面是否可以生存’作为分析的特征属性可将数据集分为[1,2,3]与[4,5]，分别占0.6和0.4。[1,2,3]可计算该类数据信息熵为 h1=0.918295834054[4,5] 可计算该类数据信息熵为 h2=0计算划分后的信息熵 H2 = 0.6 * h1 + 0.4 * h2 = 0.550977500433 3.3 计算信息增益infoGain_0 = H1-H2 = 0.419973094022 3.4 特征选择同理可得对特征’是否有脚蹼’该特征计算信息增益 infoGain_1 = 0.170950594455比较可得，’不服出水面是否可以生存’所得的信息增益更大，因此在该实例中，该特征是最好用于划分数据集的特征 四、代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576# coding=utf-8import numpy as npfrom math import logdata_feature_matrix = np.array([[1, 1], [1, 1], [1, 0], [0, 1], [0, 1]]) # 特征矩阵category = [&#x27;yes&#x27;, &#x27;yes&#x27;, &#x27;no&#x27;, &#x27;no&#x27;, &#x27;no&#x27;] # 5个对象分别所属的类别def calc_shannon_ent(category_list): &quot;&quot;&quot; :param category_list: 类别列表 :return: 该类别列表的熵值 &quot;&quot;&quot; label_count = &#123;&#125; # 统计数据集中每个类别的个数 num = len(category_list) # 数据集个数 for i in range(num): try: label_count[category_list[i]] += 1 except KeyError: label_count[category_list[i]] = 1 shannon_ent = 0. for k in label_count: prob = float(label_count[k]) / num shannon_ent -= prob * log(prob, 2) # 计算信息熵 return shannon_entdef split_data(feature_matrix, category_list, feature_index, value): &quot;&quot;&quot; 筛选出指定特征值所对应的类别列表 :param category_list: 类别列表 :param feature_matrix: 特征矩阵 :param feature_index: 指定特征索引 :param value: 指定特征属性的特征值 :return: 符合指定特征属性的特征值的类别列表 &quot;&quot;&quot; # feature_matrix = np.array(feature_matrix) ret_index = np.where(feature_matrix[:, feature_index] == value)[0] # 获取符合指定特征值的索引 ret_category_list = [category_list[i] for i in ret_index] # 根据索引取得指定的所属类别，构建为列表 return ret_category_listdef choose_best_feature(feature_matrix, category_list): &quot;&quot;&quot; 根据信息增益获取最优特征 :param feature_matrix: 特征矩阵 :param category_list: 类别列表 :return: 最优特征对应的索引 &quot;&quot;&quot; feature_num = len(feature_matrix[0]) # 特征个数 data_num = len(category_list) # 数据集的个数 base_shannon_ent = calc_shannon_ent(category_list=category_list) # 原始数据的信息熵 best_info_gain = 0 # 最优信息增益 best_feature_index = -1 # 最优特征对应的索引 for f in range(feature_num): uni_value_list = set(feature_matrix[:, f]) # 该特征属性所包含的特征值 new_shannon_ent = 0. for value in uni_value_list: sub_cate_list = split_data(feature_matrix=feature_matrix, category_list=category_list, feature_index=f, value=value) prob = float(len(sub_cate_list)) / data_num new_shannon_ent += prob * calc_shannon_ent(sub_cate_list) info_gain = base_shannon_ent - new_shannon_ent # 信息增益 print &#x27;初始信息熵为：&#x27;, base_shannon_ent, &#x27;按照特征%i分类后的信息熵为：&#x27; % f, new_shannon_ent, &#x27;信息增益为：&#x27;, info_gain if info_gain &gt; best_info_gain: best_info_gain = info_gain best_feature_index = f return best_feature_indexif __name__ == &#x27;__main__&#x27;: best_feature = choose_best_feature(data_feature_matrix, category) print &#x27;最好用于划分数据集的特征为：&#x27;, best_feature","raw":null,"content":null,"categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://hareric.com/tags/python/"},{"name":"机器学习","slug":"机器学习","permalink":"https://hareric.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"},{"name":"特征选择","slug":"特征选择","permalink":"https://hareric.com/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"}]},{"title":"k-近邻算法(K Nearest Neighbor)","slug":"k-近邻算法(K Nearest Neighbor)","date":"2016-04-28T20:01:05.000Z","updated":"2021-05-22T08:01:05.400Z","comments":true,"path":"2016/04/29/k-近邻算法(K Nearest Neighbor)/","link":"","permalink":"https://hareric.com/2016/04/29/k-%E8%BF%91%E9%82%BB%E7%AE%97%E6%B3%95(K%20Nearest%20Neighbor)/","excerpt":"工作原理　　存在一份训练样本集，并且每个样本都有属于自己的标签，即我们知道每个样本集中所属于的类别。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后提取样本集中与之最相近的k个样本。观察并统计这k个样本的标签，选择数量最大的标签作为这个新数据的标签。","text":"工作原理 存在一份训练样本集，并且每个样本都有属于自己的标签，即我们知道每个样本集中所属于的类别。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后提取样本集中与之最相近的k个样本。观察并统计这k个样本的标签，选择数量最大的标签作为这个新数据的标签。 用以下这幅图可以很好的解释kNN算法： 不同形状的点，为不同标签的点。其中绿色点为未知标签的数据点。现在要对绿色点进行预测。由图不难得出： 如果k=3，那么离绿色点最近的有2个红色三角形和1个蓝色的正方形，这3个点投票，于是绿色的这个待分类点属于红色的三角形。 如果k=5，那么离绿色点最近的有2个红色三角形和3个蓝色的正方形，这5个点投票，于是绿色的这个待分类点属于蓝色的正方形。 kNN算法实施伪代码对未知属性的数据集中的每个点执行以下操作 计算已知类型类别数据集中的点与当前点之间的距离 按照距离递增次序排序 选取与当前点距离最小的k个点 确定前k个点所在类别的出现频率 返回前k个点出现频率最高的类别作为当前点的预测分类 欧式距离(计算两点之间的距离公式)计算点x与点y之间欧式距离 python代码实现123456789101112131415161718192021222324252627282930313233343536373839import numpy as npimport operator# 训练集data_set = np.array([[1., 1.1], [1.0, 1.0], [0., 0.], [0, 0.1]])labels = [&#x27;A&#x27;, &#x27;A&#x27;, &#x27;B&#x27;, &#x27;B&#x27;]def classify_knn(in_vector, training_data, training_label, k): &quot;&quot;&quot; :param in_vector: 待分类向量 :param training_data: 训练集向量 :param training_label: 训练集标签 :param k: 选择最近邻居的数目 :return: 分类器对 in_vector 分类的类别 &quot;&quot;&quot; data_size = training_data.shape[0] # .shape[0] 返回二维数组的行数 diff_mat = np.tile(in_vector, (data_size, 1)) - data_set # np.tile(array, (3, 2)) 对 array 进行 3×2 扩展为二维数组 sq_diff_mat = diff_mat ** 2 sq_distances = sq_diff_mat.sum(axis=1) # .sum(axis=1) 矩阵以列求和 # distances = sq_distances ** 0.5 # 主要是通过比较求最近点,所以没有必要求平方根 distances_sorted_index = sq_distances.argsort() # .argsort() 对array进行排序 返回排序后对应的索引 class_count_dict = &#123;&#125; # 用于统计类别的个数 for i in range(k): label = training_label[distances_sorted_index[i]] try: class_count_dict[label] += 1 except KeyError: class_count_dict[label] = 1 class_count_dict = sorted(class_count_dict.iteritems(), key=operator.itemgetter(1), reverse=True) # 根据字典的value值对字典进行逆序排序 return class_count_dict[0][0]if __name__ == &#x27;__main__&#x27;: vector = [0, 0] # 待分类数据集 print classify_knn(in_vector=vector, training_data=data_set, training_label=labels, k=3) 算法评价 优点：精度高、对异常值不敏感、无数据输入假定 缺点：计算复杂度高、空间复杂度高 使用数据范围：数据型和标称型 适用：kNN方法通常用于一个更复杂分类算法的一部分。例如，我们可以用它的估计值做为一个对象的特征。有时候，一个简单的kNN算法在良好选择的特征上会有很出色的表现。","raw":null,"content":null,"categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://hareric.com/tags/python/"},{"name":"机器学习","slug":"机器学习","permalink":"https://hareric.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"朴素贝叶斯分类器（Naive Bayes classifier)","slug":"朴素贝叶斯分类器","date":"2016-04-23T10:54:28.000Z","updated":"2021-05-22T08:01:05.401Z","comments":true,"path":"2016/04/23/朴素贝叶斯分类器/","link":"","permalink":"https://hareric.com/2016/04/23/%E6%9C%B4%E7%B4%A0%E8%B4%9D%E5%8F%B6%E6%96%AF%E5%88%86%E7%B1%BB%E5%99%A8/","excerpt":"　　朴素贝叶斯是一种构建分类器的简单方法。该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。它不是训练这种分类器的单一算法，而是一系列基于相同原理的算法：所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关。举个例子，如果一种水果其具有红，圆，直径大概3英寸等特征，该水果可以被判定为是苹果。尽管这些特征相互依赖或者有些特征由其他特征决定，然而朴素贝叶斯分类器认为这些属性在判定该水果是否为苹果的概率分布上独立的。","text":"朴素贝叶斯是一种构建分类器的简单方法。该分类器模型会给问题实例分配用特征值表示的类标签，类标签取自有限集合。它不是训练这种分类器的单一算法，而是一系列基于相同原理的算法：所有朴素贝叶斯分类器都假定样本每个特征与其他特征都不相关。举个例子，如果一种水果其具有红，圆，直径大概3英寸等特征，该水果可以被判定为是苹果。尽管这些特征相互依赖或者有些特征由其他特征决定，然而朴素贝叶斯分类器认为这些属性在判定该水果是否为苹果的概率分布上独立的。 一、朴素贝叶斯分类的基本原理 给定的待分类项的特征属性，计算该项在各个类别出现的概率，取最大的概率类别作为预测项。 二、贝叶斯定理 根据条件概率的定义。在事件B发生的条件下事件A发生的概率是: 同样地，在事件A发生的条件下事件B发生的概率: 整理与合并这两个方程式，我们可以得到: 这个引理有时称作概率乘法规则。上式两边同除以P(A)，若P(A)是非零的，我们可以得到贝叶斯定理: 三、贝叶斯定理的变式应用 现有一个待分类项x，拥有n个特征，归于类别c的概率可表示为： 使用贝叶斯定理可变式为: 由于在计算其他类别的概率时，均会除以，并不影响最后结果的判断预测，因此可省略，即: 变式后: 例： 现给出10个被分类项的特征和已确定的类别，用来训练该分类器。 被分类项\\特征 F1 F2 F3 F4 F5 F6 类别 U1 1 0 1 0 1 0 a U2 1 1 0 1 0 0 a U3 1 0 0 1 1 1 b U4 0 0 1 0 1 1 a U5 1 0 0 1 1 0 c U6 1 1 1 1 1 0 b U7 1 1 1 0 1 1 a U8 1 0 1 0 1 0 c U9 0 1 0 0 1 0 c U10 0 1 1 0 1 0 a 由上示表格可计算出 P(F1|c)=0.66 ; P(F5|a)=0.8 ; P(F3|b)=0.5 ; P(c)=0.3…现给出 U11 项的特征，进行预测推断该项应属于a,b,c中的哪一类 被分类项\\特征 F1 F2 F3 F4 F5 F6 类别 U11 0 0 1 0 1 0 未知 由表格可得知：被分类项 U11 拥有特征F3和F5 预测分到类别a的概率: 预测分到类别b的概率: 预测分到类别c的概率: 因此可预测该项应分至类别a 四、贝叶斯分类运用于文本分类中实例:对邮件进行垃圾邮件和正常邮件的分类基本思路：1.将邮件中的文本的每一个词都作为特征，是否存在这个词可视为该邮件是否存在这个特征。2.首先给出了已人工划分好的邮件作为训练集，创建关于邮件的完整词库。3.对每条邮件都创建关于词库的向量，邮件若存在某词语则为1，不存在则为0.4.创建如下图的向量表。 ability ad after basket battle behind … zero zebu zest zoom 是否为垃圾邮件 Email01 1 0 1 0 1 1 0 1 1 1 1 Email02 0 0 0 0 1 0 1 0 1 1 0 Email03 1 1 1 1 0 1 1 1 0 1 0 Email04 0 0 0 0 1 0 0 0 0 0 1 Email05 1 1 0 1 1 1 1 0 0 1 1 Email06 0 0 1 0 0 0 0 0 1 0 0 Email07 0 1 0 1 1 1 1 0 0 0 1 Email08 0 1 1 1 0 0 0 1 1 1 1 Email09 1 1 1 1 1 1 1 0 0 1 0 Email10 0 0 0 0 0 0 0 1 0 0 1 … Email n 0 0 1 0 1 0 0 1 0 0 1 5、待分类的邮件则可以利用上图表使用贝叶斯定理，计算出归于垃圾邮件和正常邮件的概率，并进行预测。 代码实例１．首先读入数据集 spam 文件夹中的垃圾邮件和 ham 文件夹中的正常邮件，并对它进行解码、分词处理和小写处理，最后合并成一个列表。 12345678910111213141516171819202122def loadDataSet(): #导入垃圾邮件和正常邮件作为训练集 test_list = [] for i in range(21)[1:]: file1 = open(r&#x27;spam/%s.txt&#x27;%i) text = file1.read() code = chardet.detect(text)[&#x27;encoding&#x27;] text = text.decode(code).lower() words = nltk.word_tokenize(text) test_list.append(words) file1.close() for i in range(21)[1:]: file1 = open(r&#x27;ham/%s.txt&#x27;%i) text = file1.read() code = chardet.detect(text)[&#x27;encoding&#x27;] text = text.decode(code).lower() words = nltk.word_tokenize(text) test_list.append(words) file1.close() classVec = [1 for i in range(20)] classVec.extend([0 for j in range(20)])#1 代表垃圾邮件 0代表普通邮件 return test_list,classVec 2、利用set类型操作，对列表进行处理，删除重复单词，最后合并成一个词库12345def createVocabList(dataSet):#创建词库 vocabSet = set([]) for i in dataSet: vocabSet = vocabSet | set(i) #取并集，消除重复集 return list(vocabSet) 3、对单条邮件创建向量 123456789def createVector(unit,vocabList): vector = [0]*len(vocabList) for i in unit: if i in vocabList: vector[vocabList.index(i)] = 1 else: print &quot;the word %s is not in my vocabList&quot;%i continue return vector 4、利用已分好类型的邮件数对贝叶斯分类器进行训练。训练结束后能够得到：正常邮件中和垃圾邮件词库中每个词出现的概率列表（p1Vect,p0Vect），以及垃圾邮件和正常邮件的概率（p1,p0） 12345678910111213141516171819202122232425def trainNBO(train_matrix,train_bool): train_num = len(train_matrix) words_num = len(train_matrix[0]) sum_1 = [0 for i in range(words_num)] sum_0 = [0 for i in range(words_num)] _1_num = 0 #是垃圾邮件的邮件数 _0_num = 0 #非垃圾邮件的邮件数 for i in range(train_num): #将训练矩阵向量进行相加 if train_bool[i]==1: for j in range(words_num): sum_1[j] += train_matrix[i][j] _1_num = _1_num + 1 if train_bool[i]==0: for j in range(words_num): sum_0[j] += train_matrix[i][j] _0_num = _0_num + 1 print &quot;正常邮件数：&quot;,_0_num,&quot; 垃圾邮件数：&quot;,_1_num p1Vect = [(float(sum_1[j])/_1_num) for j in range(words_num)] p0Vect = [(float(sum_0[j])/_0_num) for j in range(words_num)] p1 = float(_1_num)/train_num p0 = float(_0_num)/train_num return p1Vect,p0Vect,p1,p0 5、将已转化成向量的邮件，进行类别推测。 12345678910111213def classing(p1Vect,p0Vect,P1,P0,unitVector): p1 = 1. p0 = 1. words_num = len(unitVector) for i in range(words_num): if unitVector[i]==1: p1 *= p1_vect[i] p0 *= p0_vect[i] p1 *= P1 p0 *= P0 if p1&gt;p0: return 1 else:return 0 数据集和源代码下载 资料参考：1、 维基百科2、《机器学习实战》","raw":null,"content":null,"categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://hareric.com/tags/python/"},{"name":"机器学习","slug":"机器学习","permalink":"https://hareric.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"python学习笔记----第7章类和类型","slug":"python学习笔记----第7章类和类型","date":"2016-04-23T10:54:23.000Z","updated":"2021-05-22T08:01:05.400Z","comments":true,"path":"2016/04/23/python学习笔记----第7章类和类型/","link":"","permalink":"https://hareric.com/2016/04/23/python%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0----%E7%AC%AC7%E7%AB%A0%E7%B1%BB%E5%92%8C%E7%B1%BB%E5%9E%8B/","excerpt":"与其它编程语言相比，Python的类机制添加了最小的新语法和语义。它是C++和Modula-3中的类机制的混合。Python的类提供了面向对象编程的所有的标准特性，类继承机制允许有多个基类，一个子类可以重写基类中的任何方法，一个方法可以调用基类里面的同名方法。对象可以包含任意数量和种类的数据。就像模块那样，类参与Python的动态天性，在运行时被创建，创建后可以被进一步修改。","text":"与其它编程语言相比，Python的类机制添加了最小的新语法和语义。它是C++和Modula-3中的类机制的混合。Python的类提供了面向对象编程的所有的标准特性，类继承机制允许有多个基类，一个子类可以重写基类中的任何方法，一个方法可以调用基类里面的同名方法。对象可以包含任意数量和种类的数据。就像模块那样，类参与Python的动态天性，在运行时被创建，创建后可以被进一步修改。 创建简单类123456789101112131415161718192021222324252627class Employee: &#x27;所有员工的基类&#x27; empCount = 0 #开头没有self的数据域,且写在方法外类似于静态变量private # 构造方法，实例化后第一时间自动调用 def __init__(self, name, salary): #注意：1、双下划线 2、是init不是int self.name = name #开头有self的数据可以让类的其他方法使用 self.salary = salary Employee.empCount += 1 def displayCount(self): print &quot;Total Employee %d&quot; % Employee.empCount def displayEmployee(self): print &quot;Name : &quot;, self.name, &quot;, Salary: &quot;, self.salary#创建实例对象 &quot;创建 Employee 类的第一个对象&quot;emp1 = Employee(&quot;Zara&quot;, 2000)&quot;创建 Employee 类的第二个对象&quot;emp2 = Employee(&quot;Manni&quot;, 5000)#访问属性emp1.displayEmployee()emp2.displayEmployee()print &quot;Total Employee %d&quot; % Employee.empCount 输出结果Name : Zara ,Salary: 2000Name : Manni ,Salary: 5000Total Employee 2 重绑定empCount新的empCount值被写到emp1得特性中，屏蔽了类范围内的变量 emp1.empCount2emp2.empCount2emp1.empCount = “two”emp1.empCount“two”emp2.empCount2 类方法和静态方法@classmethod我们要写一个只在类中运行而不在实例中运行的方法. 如果我们想让方法不在实例中运行，可以这么做:123456789def iget_no_of_instance(ins_obj): return ins_obj.__class__.no_instclass Kls(object): no_inst = 0 def __init__(self): Kls.no_inst = Kls.no_inst + 1ik1 = Kls()ik2 = Kls()print iget_no_of_instance(ik1)输出：2 在Python2.2以后可以使用@classmethod装饰器来创建类方法.1234567891011class Kls(object): no_inst = 0 def __init__(self): Kls.no_inst = Kls.no_inst + 1 @classmethod def get_no_of_instance(cls_obj): return cls_obj.no_instik1 = Kls()ik2 = Kls()print ik1.get_no_of_instance()print Kls.get_no_of_instance()输出:22这样的好处是: 不管这个方式是从实例调用还是从类调用，它都用第一个参数把类传递过来.","raw":null,"content":null,"categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://hareric.com/tags/python/"}]},{"title":"使用MySQLdb对mysql数据库的进行读写","slug":"使用MySQLdb对mysql数据库的进行读写","date":"2016-04-23T10:47:10.000Z","updated":"2021-05-22T08:01:05.401Z","comments":true,"path":"2016/04/23/使用MySQLdb对mysql数据库的进行读写/","link":"","permalink":"https://hareric.com/2016/04/23/%E4%BD%BF%E7%94%A8MySQLdb%E5%AF%B9mysql%E6%95%B0%E6%8D%AE%E5%BA%93%E7%9A%84%E8%BF%9B%E8%A1%8C%E8%AF%BB%E5%86%99/","excerpt":"MySQL是一个小型关系型数据库管理系统，开发者为瑞典MySQLAB公司。在2008年1月16号被Sun公司收购。目前MySQL被广泛地应用在Internet上的中小型网站中。由于其体积小、速度快、总体拥有成本低，尤其是开放源码这一特点，许多中小型网站为了降低网站总体拥有成本而选择了MySQL作为网站数据库。","text":"MySQL是一个小型关系型数据库管理系统，开发者为瑞典MySQLAB公司。在2008年1月16号被Sun公司收购。目前MySQL被广泛地应用在Internet上的中小型网站中。由于其体积小、速度快、总体拥有成本低，尤其是开放源码这一特点，许多中小型网站为了降低网站总体拥有成本而选择了MySQL作为网站数据库。 创建数据库表如果数据库连接存在我们可以使用execute()方法来为数据库创建表，如下所示创建表EMPLOYEE：1234567891011121314151617181920212223import MySQLdb# 打开数据库连接db = MySQLdb.connect(&quot;localhost&quot;,&quot;testuser&quot;,&quot;test123&quot;,&quot;TESTDB&quot; )# 使用cursor()方法获取操作游标 cursor = db.cursor()# 如果数据表已经存在使用 execute() 方法删除表。cursor.execute(&quot;DROP TABLE IF EXISTS EMPLOYEE&quot;)# 创建数据表SQL语句sql = &quot;&quot;&quot;CREATE TABLE EMPLOYEE ( FIRST_NAME CHAR(20) NOT NULL, LAST_NAME CHAR(20), AGE INT, SEX CHAR(1), INCOME FLOAT )&quot;&quot;&quot;cursor.execute(sql)# 关闭数据库连接db.close() 数据库插入操作以下实例使用执行 SQL INSERT 语句向表 EMPLOYEE 插入记录：1234567891011121314151617181920212223import MySQLdb# 打开数据库连接db = MySQLdb.connect(&quot;localhost&quot;,&quot;testuser&quot;,&quot;test123&quot;,&quot;TESTDB&quot; )# 使用cursor()方法获取操作游标 cursor = db.cursor()# SQL 插入语句sql = &quot;&quot;&quot;INSERT INTO EMPLOYEE(FIRST_NAME, LAST_NAME, AGE, SEX, INCOME) VALUES (&#x27;Mac&#x27;, &#x27;Mohan&#x27;, 20, &#x27;M&#x27;, 2000)&quot;&quot;&quot;try: # 执行sql语句 cursor.execute(sql) # 提交到数据库执行 db.commit()except: # Rollback in case there is any error db.rollback()# 关闭数据库连接db.close() 以上例子也可以写成如下形式：123456789101112131415161718192021222324import MySQLdb# 打开数据库连接db = MySQLdb.connect(&quot;localhost&quot;,&quot;testuser&quot;,&quot;test123&quot;,&quot;TESTDB&quot; )# 使用cursor()方法获取操作游标 cursor = db.cursor()# SQL 插入语句sql = &quot;INSERT INTO EMPLOYEE(FIRST_NAME, \\ LAST_NAME, AGE, SEX, INCOME) \\ VALUES (&#x27;%s&#x27;, &#x27;%s&#x27;, &#x27;%d&#x27;, &#x27;%c&#x27;, &#x27;%d&#x27; )&quot; % \\ (&#x27;Mac&#x27;, &#x27;Mohan&#x27;, 20, &#x27;M&#x27;, 2000)try: # 执行sql语句 cursor.execute(sql) # 提交到数据库执行 db.commit()except: # 发生错误时回滚 db.rollback()# 关闭数据库连接db.close()","raw":null,"content":null,"categories":[],"tags":[{"name":"python","slug":"python","permalink":"https://hareric.com/tags/python/"},{"name":"Mysql","slug":"Mysql","permalink":"https://hareric.com/tags/Mysql/"}]},{"title":"MySQL命令行--导入导出数据库","slug":"MySQL命令行--导入导出数据库","date":"2016-04-23T03:11:40.000Z","updated":"2021-05-22T08:01:05.400Z","comments":true,"path":"2016/04/23/MySQL命令行--导入导出数据库/","link":"","permalink":"https://hareric.com/2016/04/23/MySQL%E5%91%BD%E4%BB%A4%E8%A1%8C--%E5%AF%BC%E5%85%A5%E5%AF%BC%E5%87%BA%E6%95%B0%E6%8D%AE%E5%BA%93/","excerpt":"MySQL命令行导出数据库：mysqldump -u 用户名 -p 数据库名 &gt; 导出的地址/导出的文件名如我输入的命令行:mysqldump -u root -p swpu &gt; ‘/Users/frand/Desktop/swpu.sql’,输入后会让你输入进入MySQL的密码,输入密码即可看到swpu.sql出现在桌面上。","text":"MySQL命令行导出数据库：mysqldump -u 用户名 -p 数据库名 &gt; 导出的地址/导出的文件名如我输入的命令行:mysqldump -u root -p swpu &gt; ‘/Users/frand/Desktop/swpu.sql’,输入后会让你输入进入MySQL的密码,输入密码即可看到swpu.sql出现在桌面上。 MySQL命令行导出一个表：mysqldump -u 用户名 -p 数据库名 表名&gt; 导出的地址/导出的文件名如我输入的命令行：mysqldump -u root -p swpu users&gt; ‘/Users/frand/Desktop/swpu_users.sql’,输入后会让你输入进入MySQL的密码,输入密码即可看到swpu_users.sql出现在桌面上。 MySQL命令行导入一个数据库1，先进入数据库，如我输入的命令行:mysql -u root -p (输入同样后会让你输入MySQL的密码)2，在MySQL-Front中新建你要建的数据库，这时是空数据库，如新建一个名为news的目标数据库3，输入：use 目标数据库名,如我输入的命令行:use news;4，导入文件：source 导入的文件名,如我输入的命令行：source /Users/frand/Desktop/swpu.sqls; MySQL命令行导入一个数据表1，先进入数据库，如我输入的命令行:mysql -u root -p (输入同样后会让你输入MySQL的密码)2，在MySQL-Front中新建你要建的数据库，这时是空数据库，如新建一个名为news的目标数据库3，输入：use 目标数据库名,如我输入的命令行:use news;4，导入文件：source 导入的文件名,如我输入的命令行：source /Users/frand/Desktop/swpu_falcatys.sqls;MySQL备份和还原,都是利用mysqldump、mysql和source命令来完成的。","raw":null,"content":null,"categories":[],"tags":[{"name":"Mysql","slug":"Mysql","permalink":"https://hareric.com/tags/Mysql/"}]},{"title":"Hello World","slug":"hello-world","date":"2016-04-22T11:09:48.000Z","updated":"2021-05-22T08:01:05.400Z","comments":true,"path":"2016/04/22/hello-world/","link":"","permalink":"https://hareric.com/2016/04/22/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","raw":null,"content":null,"categories":[],"tags":[]}]}